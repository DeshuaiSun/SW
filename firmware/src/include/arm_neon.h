// https://github.com/sifive/neon2rvv/commit/b0ba7d037b5c02b62da32e37e9b726d29d1d2889
// VLEN=64, aarch=64, indent='\t', only='.*', validate=False
/*
Copyright (c) 2015 - 2021 SiFive, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to use this
Software with RISC-V based SiFive products only and not with any other
processors and platforms, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE LICENSED SOFTWARE IS PROVIDED TO LICENSEE “AS IS” WITHOUT ANY SUPPORT
SERVICES OR WARRANTIES OF ANY KIND, INCLUDING, BUT WITHOUT LIMITATION, SIFIVE
DOES NOT WARRANT TO LICENSEE THAT THE LICENSED SOFTWARE WILL OPERATE ERROR FREE
OR UNINTERRUPTED, NOR THAT IT WILL MEET YOUR REQUIREMENTS. SIFIVE SHALL NOT HAVE
ANY DUTY OR OBLIGATION TO DEFEND OR INDEMNIFY LICENSEE OR TO HOLD IT HARMLESS
FOR ANY REASON RELATED TO THE LICENSED SOFTWARE, OR OTHERWISE BE LIABLE TO
LICENSEE OR ANY THIRD PARTY FOR ANY INCIDENTAL, INDIRECT, SPECIAL, EXEMPLARY,
PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOST PROFITS) ARISING OUT OF OR
RELATED TO THIS AGREEMENT.
*/
#ifndef NEON2RVV_H
#define NEON2RVV_H
#include <assert.h>
#include <fenv.h>
#include <stdint.h>
#ifdef NEON2RVV_DEBUG
#include <stdio.h>
#define NEON2RVV_DEBUG_FUNCTION printf("call %s(NEON2RVV)\n", __func__)
#else
#define NEON2RVV_DEBUG_FUNCTION
#endif
#ifdef __riscv_v
#include <riscv_vector.h>
#endif
#ifdef __riscv_zfh
typedef __fp16 float16_t;
typedef float16_t float16x4_t __attribute__ ((vector_size(8)));
typedef struct{float16x4_t val[2];} float16x4x2_t;
typedef struct{float16x4_t val[3];} float16x4x3_t;
typedef struct{float16x4_t val[4];} float16x4x4_t;
typedef float16_t float16x8_t __attribute__ ((vector_size(16)));
typedef struct{float16x8_t val[2];} float16x8x2_t;
typedef struct{float16x8_t val[3];} float16x8x3_t;
typedef struct{float16x8_t val[4];} float16x8x4_t;
#endif
#if 32 <= __riscv_flen
typedef float float32_t;
typedef float32_t float32x2_t __attribute__ ((vector_size(8)));
typedef struct{float32x2_t val[2];} float32x2x2_t;
typedef struct{float32x2_t val[3];} float32x2x3_t;
typedef struct{float32x2_t val[4];} float32x2x4_t;
typedef float32_t float32x4_t __attribute__ ((vector_size(16)));
typedef struct{float32x4_t val[2];} float32x4x2_t;
typedef struct{float32x4_t val[3];} float32x4x3_t;
typedef struct{float32x4_t val[4];} float32x4x4_t;
#endif
#if 64 <= __riscv_flen
typedef double float64_t;
typedef float64_t float64x1_t __attribute__ ((vector_size(8)));
typedef struct{float64x1_t val[2];} float64x1x2_t;
typedef struct{float64x1_t val[3];} float64x1x3_t;
typedef struct{float64x1_t val[4];} float64x1x4_t;
typedef float64_t float64x2_t __attribute__ ((vector_size(16)));
typedef struct{float64x2_t val[2];} float64x2x2_t;
typedef struct{float64x2_t val[3];} float64x2x3_t;
typedef struct{float64x2_t val[4];} float64x2x4_t;
#endif
typedef int8_t int8x8_t __attribute__ ((vector_size(8)));
typedef struct{int8x8_t val[2];} int8x8x2_t;
typedef struct{int8x8_t val[3];} int8x8x3_t;
typedef struct{int8x8_t val[4];} int8x8x4_t;
typedef int8_t int8x16_t __attribute__ ((vector_size(16)));
typedef struct{int8x16_t val[2];} int8x16x2_t;
typedef struct{int8x16_t val[3];} int8x16x3_t;
typedef struct{int8x16_t val[4];} int8x16x4_t;
typedef int16_t int16x4_t __attribute__ ((vector_size(8)));
typedef struct{int16x4_t val[2];} int16x4x2_t;
typedef struct{int16x4_t val[3];} int16x4x3_t;
typedef struct{int16x4_t val[4];} int16x4x4_t;
typedef int16_t int16x8_t __attribute__ ((vector_size(16)));
typedef struct{int16x8_t val[2];} int16x8x2_t;
typedef struct{int16x8_t val[3];} int16x8x3_t;
typedef struct{int16x8_t val[4];} int16x8x4_t;
typedef int32_t int32x2_t __attribute__ ((vector_size(8)));
typedef struct{int32x2_t val[2];} int32x2x2_t;
typedef struct{int32x2_t val[3];} int32x2x3_t;
typedef struct{int32x2_t val[4];} int32x2x4_t;
typedef int32_t int32x4_t __attribute__ ((vector_size(16)));
typedef struct{int32x4_t val[2];} int32x4x2_t;
typedef struct{int32x4_t val[3];} int32x4x3_t;
typedef struct{int32x4_t val[4];} int32x4x4_t;
typedef int64_t int64x1_t __attribute__ ((vector_size(8)));
typedef struct{int64x1_t val[2];} int64x1x2_t;
typedef struct{int64x1_t val[3];} int64x1x3_t;
typedef struct{int64x1_t val[4];} int64x1x4_t;
typedef int64_t int64x2_t __attribute__ ((vector_size(16)));
typedef struct{int64x2_t val[2];} int64x2x2_t;
typedef struct{int64x2_t val[3];} int64x2x3_t;
typedef struct{int64x2_t val[4];} int64x2x4_t;
typedef uint8_t uint8x8_t __attribute__ ((vector_size(8)));
typedef struct{uint8x8_t val[2];} uint8x8x2_t;
typedef struct{uint8x8_t val[3];} uint8x8x3_t;
typedef struct{uint8x8_t val[4];} uint8x8x4_t;
typedef uint8_t uint8x16_t __attribute__ ((vector_size(16)));
typedef struct{uint8x16_t val[2];} uint8x16x2_t;
typedef struct{uint8x16_t val[3];} uint8x16x3_t;
typedef struct{uint8x16_t val[4];} uint8x16x4_t;
typedef uint16_t uint16x4_t __attribute__ ((vector_size(8)));
typedef struct{uint16x4_t val[2];} uint16x4x2_t;
typedef struct{uint16x4_t val[3];} uint16x4x3_t;
typedef struct{uint16x4_t val[4];} uint16x4x4_t;
typedef uint16_t uint16x8_t __attribute__ ((vector_size(16)));
typedef struct{uint16x8_t val[2];} uint16x8x2_t;
typedef struct{uint16x8_t val[3];} uint16x8x3_t;
typedef struct{uint16x8_t val[4];} uint16x8x4_t;
typedef uint32_t uint32x2_t __attribute__ ((vector_size(8)));
typedef struct{uint32x2_t val[2];} uint32x2x2_t;
typedef struct{uint32x2_t val[3];} uint32x2x3_t;
typedef struct{uint32x2_t val[4];} uint32x2x4_t;
typedef uint32_t uint32x4_t __attribute__ ((vector_size(16)));
typedef struct{uint32x4_t val[2];} uint32x4x2_t;
typedef struct{uint32x4_t val[3];} uint32x4x3_t;
typedef struct{uint32x4_t val[4];} uint32x4x4_t;
typedef uint64_t uint64x1_t __attribute__ ((vector_size(8)));
typedef struct{uint64x1_t val[2];} uint64x1x2_t;
typedef struct{uint64x1_t val[3];} uint64x1x3_t;
typedef struct{uint64x1_t val[4];} uint64x1x4_t;
typedef uint64_t uint64x2_t __attribute__ ((vector_size(16)));
typedef struct{uint64x2_t val[2];} uint64x2x2_t;
typedef struct{uint64x2_t val[3];} uint64x2x3_t;
typedef struct{uint64x2_t val[4];} uint64x2x4_t;
#ifndef FE_TONEARESTFROMZERO
#define FE_TONEARESTFROMZERO FE_TONEAREST
#endif

#ifdef __clang__
#define NEON2RVV_NOT_IMPLEMENT(message) __attribute__((diagnose_if(1, "NEON2RVV does not implement " message, "error")))
#else
#define NEON2RVV_NOT_IMPLEMENT(message) __attribute__((error("NEON2RVV does not implement " message)))
#endif

#ifdef __riscv_v
static inline int __saturation_occurred(void)
{
	return vegetxsat();
}

static inline void __set_saturation_occurred(int sat)
{
	vesetxsat(sat);
}
#endif

static inline void __ignore_saturation(void){}
NEON2RVV_NOT_IMPLEMENT("__crc32b") uint32_t __crc32b(uint32_t a, uint8_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32cb") uint32_t __crc32cb(uint32_t a, uint8_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32cd") uint32_t __crc32cd(uint32_t a, uint64_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32ch") uint32_t __crc32ch(uint32_t a, uint16_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32cw") uint32_t __crc32cw(uint32_t a, uint32_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32d") uint32_t __crc32d(uint32_t a, uint64_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32h") uint32_t __crc32h(uint32_t a, uint16_t b);
NEON2RVV_NOT_IMPLEMENT("__crc32w") uint32_t __crc32w(uint32_t a, uint32_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_s16") int32x4_t vabal_high_s16(int32x4_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_s32") int64x2_t vabal_high_s32(int64x2_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_s8") int16x8_t vabal_high_s8(int16x8_t a, int8x16_t b, int8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_u16") uint32x4_t vabal_high_u16(uint32x4_t a, uint16x8_t b, uint16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_u32") uint64x2_t vabal_high_u32(uint64x2_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabal_high_u8") uint16x8_t vabal_high_u8(uint16x8_t a, uint8x16_t b, uint8x16_t c);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vabal_s16(int32x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	vint32m2_t temp_3 = vwsub_vv(vmax(temp_1, temp_2, 4), vmin(temp_1, temp_2, 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vadd(temp_0, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vabal_s32(int64x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	vint64m2_t temp_3 = vwsub_vv(vmax(temp_1, temp_2, 2), vmin(temp_1, temp_2, 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vadd(temp_0, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vabal_s8(int16x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	vint16m2_t temp_3 = vwsub_vv(vmax(temp_1, temp_2, 8), vmin(temp_1, temp_2, 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vadd(temp_0, temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vabal_u16(uint32x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	vuint32m2_t temp_3 = vwsubu_vv(vmaxu(temp_1, temp_2, 4), vminu(temp_1, temp_2, 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vadd(temp_0, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vabal_u32(uint64x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	vuint64m2_t temp_3 = vwsubu_vv(vmaxu(temp_1, temp_2, 2), vminu(temp_1, temp_2, 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vadd(temp_0, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vabal_u8(uint16x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	vuint16m2_t temp_3 = vwsubu_vv(vmaxu(temp_1, temp_2, 8), vminu(temp_1, temp_2, 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vadd(temp_0, temp_3, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_s16") int16x8_t vabaq_s16(int16x8_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_s32") int32x4_t vabaq_s32(int32x4_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_s8") int8x16_t vabaq_s8(int8x16_t a, int8x16_t b, int8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_u16") uint16x8_t vabaq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_u32") uint32x4_t vabaq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabaq_u8") uint8x16_t vabaq_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_s16") int16x4_t vaba_s16(int16x4_t a, int16x4_t b, int16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_s32") int32x2_t vaba_s32(int32x2_t a, int32x2_t b, int32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_s8") int8x8_t vaba_s8(int8x8_t a, int8x8_t b, int8x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_u16") uint16x4_t vaba_u16(uint16x4_t a, uint16x4_t b, uint16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_u32") uint32x2_t vaba_u32(uint32x2_t a, uint32x2_t b, uint32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaba_u8") uint8x8_t vaba_u8(uint8x8_t a, uint8x8_t b, uint8x8_t c);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vabdd_f64") float64_t vabdd_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vabd_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfabs(vfsub(temp_0, temp_1, 4), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vabd_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfabs(vfsub(temp_0, temp_1, 2), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vabd_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfabs(vfsub(temp_0, temp_1, 1), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vabdh_f16") float16_t vabdh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_s16") int32x4_t vabdl_high_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_s32") int64x2_t vabdl_high_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_s8") int16x8_t vabdl_high_s8(int8x16_t a, int8x16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_u16") uint32x4_t vabdl_high_u16(uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_u32") uint64x2_t vabdl_high_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vabdl_high_u8") uint16x8_t vabdl_high_u8(uint8x16_t a, uint8x16_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vabdl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwsub_vv(vmax(temp_0, temp_1, 4), vmin(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vabdl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwsub_vv(vmax(temp_0, temp_1, 2), vmin(temp_0, temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vabdl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwsub_vv(vmax(temp_0, temp_1, 8), vmin(temp_0, temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vabdl_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwsubu_vv(vmaxu(temp_0, temp_1, 4), vminu(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vabdl_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwsubu_vv(vmaxu(temp_0, temp_1, 2), vminu(temp_0, temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vabdl_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwsubu_vv(vmaxu(temp_0, temp_1, 8), vminu(temp_0, temp_1, 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vabdq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfabs(vfsub(temp_0, temp_1, 8), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vabdq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfabs(vfsub(temp_0, temp_1, 4), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vabdq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfabs(vfsub(temp_0, temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vabdq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsub(vmax(temp_0, temp_1, 8), vmin(temp_0, temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vabdq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsub(vmax(temp_0, temp_1, 4), vmin(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vabdq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vsub(vmax(temp_0, temp_1, 16), vmin(temp_0, temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vabdq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsub(vmaxu(temp_0, temp_1, 8), vminu(temp_0, temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vabdq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsub(vmaxu(temp_0, temp_1, 4), vminu(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vabdq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vsub(vmaxu(temp_0, temp_1, 16), vminu(temp_0, temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vabd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vsub(vmax(temp_0, temp_1, 4), vmin(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vabd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vsub(vmax(temp_0, temp_1, 2), vmin(temp_0, temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vabd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vsub(vmax(temp_0, temp_1, 8), vmin(temp_0, temp_1, 8), 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vabds_f32") float32_t vabds_f32(float32_t a, float32_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vabd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vsub(vmaxu(temp_0, temp_1, 4), vminu(temp_0, temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vabd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vsub(vmaxu(temp_0, temp_1, 2), vminu(temp_0, temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vabd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vsub(vmaxu(temp_0, temp_1, 8), vminu(temp_0, temp_1, 8), 8));
}
#endif
__attribute__((always_inline)) inline int64_t vabsd_s64(int64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	int64_t temp_0;
	if ((a < 0))
	{
		temp_0 = (((uint64_t)(a)) * -1);
	}
	else
	{
		temp_0 = a;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vabs_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfabs(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vabs_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfabs(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vabs_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfabs(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vabsh_f16(float16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	float16_t temp_0;
	if ((a < 0))
	{
		temp_0 = (a * -1);
	}
	else
	{
		temp_0 = a;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vabsq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfabs(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vabsq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfabs(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vabsq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfabs(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vabsq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmul(vmslt(temp_0, 0, 8), temp_0, temp_0, -1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vabsq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmul(vmslt(temp_0, 0, 4), temp_0, temp_0, -1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vabsq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmul(vmslt(temp_0, 0, 2), temp_0, temp_0, -1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vabsq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmul(vmslt(temp_0, 0, 16), temp_0, temp_0, -1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vabs_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmul(vmslt(temp_0, 0, 4), temp_0, temp_0, -1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vabs_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmul(vmslt(temp_0, 0, 2), temp_0, temp_0, -1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vabs_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmul(vmslt(temp_0, 0, 1), temp_0, temp_0, -1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vabs_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmul(vmslt(temp_0, 0, 8), temp_0, temp_0, -1, 8));
}
#endif
__attribute__((always_inline)) inline int64_t vaddd_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (((uint64_t)(a)) + ((uint64_t)(b)));
}
__attribute__((always_inline)) inline uint64_t vaddd_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a + b);
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vadd_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfadd(temp_0, temp_1, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vadd_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfadd(temp_0, temp_1, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vadd_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfadd(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vaddh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a + b);
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_s16") int8x16_t vaddhn_high_s16(int8x8_t r, int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_s32") int16x8_t vaddhn_high_s32(int16x4_t r, int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_s64") int32x4_t vaddhn_high_s64(int32x2_t r, int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_u16") uint8x16_t vaddhn_high_u16(uint8x8_t r, uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_u32") uint16x8_t vaddhn_high_u32(uint16x4_t r, uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaddhn_high_u64") uint32x4_t vaddhn_high_u64(uint32x2_t r, uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vaddhn_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = vadd(temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vnsra(temp_2, 8, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vaddhn_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = vadd(temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnsra(temp_2, 16, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vaddhn_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	vint64m2_t temp_2 = vadd(temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnsra(temp_2, 32, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vaddhn_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = vadd(temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vnsrl(temp_2, 8, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vaddhn_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = vadd(temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnsrl(temp_2, 16, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vaddhn_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	vuint64m2_t temp_2 = vadd(temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnsrl(temp_2, 32, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vaddl_high_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vlmul_trunc_v_i32m4_i32m2(vwadd_vv(vslidedown(vundefined_i16m2(), temp_0, 4, 4), vslidedown(vundefined_i16m2(), temp_1, 4, 4), 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vaddl_high_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vlmul_trunc_v_i64m4_i64m2(vwadd_vv(vslidedown(vundefined_i32m2(), temp_0, 2, 2), vslidedown(vundefined_i32m2(), temp_1, 2, 2), 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vaddl_high_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vlmul_trunc_v_i16m4_i16m2(vwadd_vv(vslidedown(vundefined_i8m2(), temp_0, 8, 8), vslidedown(vundefined_i8m2(), temp_1, 8, 8), 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vaddl_high_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vlmul_trunc_v_u32m4_u32m2(vwaddu_vv(vslidedown(vundefined_u16m2(), temp_0, 4, 4), vslidedown(vundefined_u16m2(), temp_1, 4, 4), 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vaddl_high_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vlmul_trunc_v_u64m4_u64m2(vwaddu_vv(vslidedown(vundefined_u32m2(), temp_0, 2, 2), vslidedown(vundefined_u32m2(), temp_1, 2, 2), 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vaddl_high_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vlmul_trunc_v_u16m4_u16m2(vwaddu_vv(vslidedown(vundefined_u8m2(), temp_0, 8, 8), vslidedown(vundefined_u8m2(), temp_1, 8, 8), 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vaddl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwadd_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vaddl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwadd_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vaddl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwadd_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vaddl_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwaddu_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vaddl_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwaddu_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vaddl_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwaddu_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vaddlvq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint32m1_t temp_1 = vmv_v_x_i32m1(0, 1);
	return vmv_x(vwredsum(vundefined_i32m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64_t vaddlvq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint64m1_t temp_1 = vmv_v_x_i64m1(0, 1);
	return vmv_x(vwredsum(vundefined_i64m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vaddlvq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint16m1_t temp_1 = vmv_v_x_i16m1(0, 1);
	return vmv_x(vwredsum(vundefined_i16m1(), temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vaddlvq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint32m1_t temp_1 = vmv_v_x_u32m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u32m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64_t vaddlvq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint64m1_t temp_1 = vmv_v_x_u64m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u64m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vaddlvq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint16m1_t temp_1 = vmv_v_x_u16m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u16m1(), temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vaddlv_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint32m1_t temp_1 = vmv_v_x_i32m1(0, 1);
	return vmv_x(vwredsum(vundefined_i32m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64_t vaddlv_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint64m1_t temp_1 = vmv_v_x_i64m1(0, 1);
	return vmv_x(vwredsum(vundefined_i64m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vaddlv_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint16m1_t temp_1 = vmv_v_x_i16m1(0, 1);
	return vmv_x(vwredsum(vundefined_i16m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vaddlv_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint32m1_t temp_1 = vmv_v_x_u32m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u32m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64_t vaddlv_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint64m1_t temp_1 = vmv_v_x_u64m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u64m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vaddlv_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint16m1_t temp_1 = vmv_v_x_u16m1(0, 1);
	return vmv_x(vwredsumu(vundefined_u16m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vaddq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfadd(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vaddq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfadd(temp_0, temp_1, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vaddq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vaddq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vadd(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vaddq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vaddq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vaddq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vadd(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vaddq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vadd(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vaddq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vaddq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vaddq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vadd(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vadd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vadd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vadd_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vadd(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vadd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vadd(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vadd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vadd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vadd_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vadd(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vadd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vadd(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vaddv_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	float32_t temp_2;
	if ((vfirst(vmsne(vreinterpret_v_f32m1_u32m1(temp_0), (-2147483647L - 1), 2), 2) != -1))
	{
		vfloat32m1_t temp_1 = vfmv_v_f_f32m1(0, 1);
		temp_2 = vfmv_f(vfredosum(vundefined_f32m1(), temp_0, temp_1, 2));
	}
	else
	{
		temp_2 = -0.0;
	}
	return temp_2;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vaddvq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	float32_t temp_2;
	if ((vfirst(vmsne(vreinterpret_v_f32m2_u32m2(temp_0), (-2147483647L - 1), 4), 4) != -1))
	{
		vfloat32m1_t temp_1 = vfmv_v_f_f32m1(0, 1);
		temp_2 = vfmv_f(vfredosum(vundefined_f32m1(), temp_0, temp_1, 4));
	}
	else
	{
		temp_2 = -0.0;
	}
	return temp_2;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64_t vaddvq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	float64_t temp_2;
	if ((vfirst(vmsne(vreinterpret_v_f64m2_u64m2(temp_0), (-9223372036854775807LL - 1), 2), 2) != -1))
	{
		vfloat64m1_t temp_1 = vfmv_v_f_f64m1(0, 1);
		temp_2 = vfmv_f(vfredosum(vundefined_f64m1(), temp_0, temp_1, 2));
	}
	else
	{
		temp_2 = -0.0;
	}
	return temp_2;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vaddvq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m1_t temp_1 = vmv_v_x_i16m1(0, 1);
	return vmv_x(vredsum(vundefined_i16m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vaddvq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m1_t temp_1 = vmv_v_x_i32m1(0, 1);
	return vmv_x(vredsum(vundefined_i32m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64_t vaddvq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m1_t temp_1 = vmv_v_x_i64m1(0, 1);
	return vmv_x(vredsum(vundefined_i64m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vaddvq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m1_t temp_1 = vmv_v_x_i8m1(0, 1);
	return vmv_x(vredsum(vundefined_i8m1(), temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vaddvq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m1_t temp_1 = vmv_v_x_u16m1(0, 1);
	return vmv_x(vredsum(vundefined_u16m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vaddvq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m1_t temp_1 = vmv_v_x_u32m1(0, 1);
	return vmv_x(vredsum(vundefined_u32m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64_t vaddvq_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m1_t temp_1 = vmv_v_x_u64m1(0, 1);
	return vmv_x(vredsum(vundefined_u64m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vaddvq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m1_t temp_1 = vmv_v_x_u8m1(0, 1);
	return vmv_x(vredsum(vundefined_u8m1(), temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vaddv_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = vmv_v_x_i16m1(0, 1);
	return vmv_x(vredsum(vundefined_i16m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vaddv_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = vmv_v_x_i32m1(0, 1);
	return vmv_x(vredsum(vundefined_i32m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vaddv_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = vmv_v_x_i8m1(0, 1);
	return vmv_x(vredsum(vundefined_i8m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vaddv_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = vmv_v_x_u16m1(0, 1);
	return vmv_x(vredsum(vundefined_u16m1(), temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vaddv_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = vmv_v_x_u32m1(0, 1);
	return vmv_x(vredsum(vundefined_u32m1(), temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vaddv_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = vmv_v_x_u8m1(0, 1);
	return vmv_x(vredsum(vundefined_u8m1(), temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vaddw_high_s16(int32x4_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m1_t temp_2 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwadd_wv(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vaddw_high_s32(int64x2_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m1_t temp_2 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwadd_wv(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vaddw_high_s8(int16x8_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m1_t temp_2 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwadd_wv(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vaddw_high_u16(uint32x4_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m1_t temp_2 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwaddu_wv(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vaddw_high_u32(uint64x2_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m1_t temp_2 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwaddu_wv(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vaddw_high_u8(uint16x8_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m1_t temp_2 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwaddu_wv(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vaddw_s16(int32x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwadd_wv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vaddw_s32(int64x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwadd_wv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vaddw_s8(int16x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwadd_wv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vaddw_u16(uint32x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwaddu_wv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vaddw_u32(uint64x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwaddu_wv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vaddw_u8(uint16x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwaddu_wv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaesdq_u8") uint8x16_t vaesdq_u8(uint8x16_t data, uint8x16_t key);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaeseq_u8") uint8x16_t vaeseq_u8(uint8x16_t data, uint8x16_t key);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaesimcq_u8") uint8x16_t vaesimcq_u8(uint8x16_t data);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vaesmcq_u8") uint8x16_t vaesmcq_u8(uint8x16_t data);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vandq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vand(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vandq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vand(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vandq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vand(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vandq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vand(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vandq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vand(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vandq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vand(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vandq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vand(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vandq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vand(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vand_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vand(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vand_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vand(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vand_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vand(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vand_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vand(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vand_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vand(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vand_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vand(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vand_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vand(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vand_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vand(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_s16") int16x8_t vbcaxq_s16(int16x8_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_s32") int32x4_t vbcaxq_s32(int32x4_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_s64") int64x2_t vbcaxq_s64(int64x2_t a, int64x2_t b, int64x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_s8") int8x16_t vbcaxq_s8(int8x16_t a, int8x16_t b, int8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_u16") uint16x8_t vbcaxq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_u32") uint32x4_t vbcaxq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_u64") uint64x2_t vbcaxq_u64(uint64x2_t a, uint64x2_t b, uint64x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vbcaxq_u8") uint8x16_t vbcaxq_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vbicq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vand(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vbicq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vand(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vbicq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vand(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vbicq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vand(temp_0, vnot(temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vbicq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vand(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vbicq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vand(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vbicq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vand(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vbicq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vand(temp_0, vnot(temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vbic_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vand(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vbic_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vand(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vbic_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vand(temp_0, vnot(temp_1, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vbic_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vand(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vbic_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vand(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vbic_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vand(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vbic_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vand(temp_0, vnot(temp_1, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vbic_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vand(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vbsl_f16(uint16x4_t a, float16x4_t b, float16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1(c);
	vuint16m1_t temp_3 = vreinterpret_v_f16m1_u16m1(temp_1);
	vuint16m1_t temp_4 = vreinterpret_v_f16m1_u16m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vreinterpret_v_u16m1_f16m1(vxor(vand(temp_0, temp_3, 4), vand(vnot(temp_0, 4), temp_4, 4), 4)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vbsl_f32(uint32x2_t a, float32x2_t b, float32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(c);
	vuint32m1_t temp_3 = vreinterpret_v_f32m1_u32m1(temp_1);
	vuint32m1_t temp_4 = vreinterpret_v_f32m1_u32m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vreinterpret_v_u32m1_f32m1(vxor(vand(temp_0, temp_3, 2), vand(vnot(temp_0, 2), temp_4, 2), 2)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vbsl_f64(uint64x1_t a, float64x1_t b, float64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(c);
	vuint64m1_t temp_3 = vreinterpret_v_f64m1_u64m1(temp_1);
	vuint64m1_t temp_4 = vreinterpret_v_f64m1_u64m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vreinterpret_v_u64m1_f64m1(vxor(vand(temp_0, temp_3, 1), vand(vnot(temp_0, 1), temp_4, 1), 1)));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vbslq_f16(uint16x8_t a, float16x8_t b, float16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2(c);
	vuint16m2_t temp_3 = vreinterpret_v_f16m2_u16m2(temp_1);
	vuint16m2_t temp_4 = vreinterpret_v_f16m2_u16m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vreinterpret_v_u16m2_f16m2(vxor(vand(temp_0, temp_3, 8), vand(vnot(temp_0, 8), temp_4, 8), 8)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vbslq_f32(uint32x4_t a, float32x4_t b, float32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(c);
	vuint32m2_t temp_3 = vreinterpret_v_f32m2_u32m2(temp_1);
	vuint32m2_t temp_4 = vreinterpret_v_f32m2_u32m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vreinterpret_v_u32m2_f32m2(vxor(vand(temp_0, temp_3, 4), vand(vnot(temp_0, 4), temp_4, 4), 4)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vbslq_f64(uint64x2_t a, float64x2_t b, float64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(c);
	vuint64m2_t temp_3 = vreinterpret_v_f64m2_u64m2(temp_1);
	vuint64m2_t temp_4 = vreinterpret_v_f64m2_u64m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vreinterpret_v_u64m2_f64m2(vxor(vand(temp_0, temp_3, 2), vand(vnot(temp_0, 2), temp_4, 2), 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vbslq_s16(uint16x8_t a, int16x8_t b, int16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(c);
	vuint16m2_t temp_3 = vreinterpret_v_i16m2_u16m2(temp_1);
	vuint16m2_t temp_4 = vreinterpret_v_i16m2_u16m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vreinterpret_v_u16m2_i16m2(vxor(vand(temp_0, temp_3, 8), vand(vnot(temp_0, 8), temp_4, 8), 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vbslq_s32(uint32x4_t a, int32x4_t b, int32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(c);
	vuint32m2_t temp_3 = vreinterpret_v_i32m2_u32m2(temp_1);
	vuint32m2_t temp_4 = vreinterpret_v_i32m2_u32m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vreinterpret_v_u32m2_i32m2(vxor(vand(temp_0, temp_3, 4), vand(vnot(temp_0, 4), temp_4, 4), 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vbslq_s64(uint64x2_t a, int64x2_t b, int64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2(c);
	vuint64m2_t temp_3 = vreinterpret_v_i64m2_u64m2(temp_1);
	vuint64m2_t temp_4 = vreinterpret_v_i64m2_u64m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vreinterpret_v_u64m2_i64m2(vxor(vand(temp_0, temp_3, 2), vand(vnot(temp_0, 2), temp_4, 2), 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vbslq_s8(uint8x16_t a, int8x16_t b, int8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2(c);
	vuint8m2_t temp_3 = vreinterpret_v_i8m2_u8m2(temp_1);
	vuint8m2_t temp_4 = vreinterpret_v_i8m2_u8m2(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vreinterpret_v_u8m2_i8m2(vxor(vand(temp_0, temp_3, 16), vand(vnot(temp_0, 16), temp_4, 16), 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vbslq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vxor(vand(temp_0, temp_1, 8), vand(vnot(temp_0, 8), temp_2, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vbslq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vxor(vand(temp_0, temp_1, 4), vand(vnot(temp_0, 4), temp_2, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vbslq_u64(uint64x2_t a, uint64x2_t b, uint64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vxor(vand(temp_0, temp_1, 2), vand(vnot(temp_0, 2), temp_2, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vbslq_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vxor(vand(temp_0, temp_1, 16), vand(vnot(temp_0, 16), temp_2, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vbsl_s16(uint16x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	vuint16m1_t temp_3 = vreinterpret_v_i16m1_u16m1(temp_1);
	vuint16m1_t temp_4 = vreinterpret_v_i16m1_u16m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vreinterpret_v_u16m1_i16m1(vxor(vand(temp_0, temp_3, 4), vand(vnot(temp_0, 4), temp_4, 4), 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vbsl_s32(uint32x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	vuint32m1_t temp_3 = vreinterpret_v_i32m1_u32m1(temp_1);
	vuint32m1_t temp_4 = vreinterpret_v_i32m1_u32m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vreinterpret_v_u32m1_i32m1(vxor(vand(temp_0, temp_3, 2), vand(vnot(temp_0, 2), temp_4, 2), 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vbsl_s64(uint64x1_t a, int64x1_t b, int64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1(c);
	vuint64m1_t temp_3 = vreinterpret_v_i64m1_u64m1(temp_1);
	vuint64m1_t temp_4 = vreinterpret_v_i64m1_u64m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vreinterpret_v_u64m1_i64m1(vxor(vand(temp_0, temp_3, 1), vand(vnot(temp_0, 1), temp_4, 1), 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vbsl_s8(uint8x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	vuint8m1_t temp_3 = vreinterpret_v_i8m1_u8m1(temp_1);
	vuint8m1_t temp_4 = vreinterpret_v_i8m1_u8m1(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vreinterpret_v_u8m1_i8m1(vxor(vand(temp_0, temp_3, 8), vand(vnot(temp_0, 8), temp_4, 8), 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vbsl_u16(uint16x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vxor(vand(temp_0, temp_1, 4), vand(vnot(temp_0, 4), temp_2, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vbsl_u32(uint32x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vxor(vand(temp_0, temp_1, 2), vand(vnot(temp_0, 2), temp_2, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vbsl_u64(uint64x1_t a, uint64x1_t b, uint64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vxor(vand(temp_0, temp_1, 1), vand(vnot(temp_0, 1), temp_2, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vbsl_u8(uint8x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vxor(vand(temp_0, temp_1, 8), vand(vnot(temp_0, 8), temp_2, 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot270_f16") float16x8_t vcaddq_rot270_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot270_f32") float32x4_t vcaddq_rot270_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot270_f64") float64x2_t vcaddq_rot270_f64(float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot90_f16") float16x8_t vcaddq_rot90_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot90_f32") float32x4_t vcaddq_rot90_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaddq_rot90_f64") float64x2_t vcaddq_rot90_f64(float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcadd_rot270_f16") float16x4_t vcadd_rot270_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcadd_rot270_f32") float32x2_t vcadd_rot270_f32(float32x2_t a, float32x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcadd_rot90_f16") float16x4_t vcadd_rot90_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcadd_rot90_f32") float32x2_t vcadd_rot90_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcaged_f64") uint64_t vcaged_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcage_f16") uint16x4_t vcage_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcage_f32") uint32x2_t vcage_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcage_f64") uint64x1_t vcage_f64(float64x1_t a, float64x1_t b);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcageh_f16") uint16_t vcageh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcageq_f16") uint16x8_t vcageq_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcageq_f32") uint32x4_t vcageq_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcageq_f64") uint64x2_t vcageq_f64(float64x2_t a, float64x2_t b);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcages_f32") uint32_t vcages_f32(float32_t a, float32_t b);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcagtd_f64") uint64_t vcagtd_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcagt_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfgt(vfabs(temp_0, 4), vfabs(temp_1, 4), 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcagt_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfgt(vfabs(temp_0, 2), vfabs(temp_1, 2), 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcagt_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfgt(vfabs(temp_0, 1), vfabs(temp_1, 1), 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcagth_f16") uint16_t vcagth_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcagtq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfgt(vfabs(temp_0, 8), vfabs(temp_1, 8), 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcagtq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfgt(vfabs(temp_0, 4), vfabs(temp_1, 4), 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcagtq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfgt(vfabs(temp_0, 2), vfabs(temp_1, 2), 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcagts_f32") uint32_t vcagts_f32(float32_t a, float32_t b);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcaled_f64") uint64_t vcaled_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcale_f16") uint16x4_t vcale_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcale_f32") uint32x2_t vcale_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcale_f64") uint64x1_t vcale_f64(float64x1_t a, float64x1_t b);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcaleh_f16") uint16_t vcaleh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcaleq_f16") uint16x8_t vcaleq_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaleq_f32") uint32x4_t vcaleq_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcaleq_f64") uint64x2_t vcaleq_f64(float64x2_t a, float64x2_t b);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcales_f32") uint32_t vcales_f32(float32_t a, float32_t b);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcaltd_f64") uint64_t vcaltd_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcalt_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmflt(vfabs(temp_0, 4), vfabs(temp_1, 4), 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcalt_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmflt(vfabs(temp_0, 2), vfabs(temp_1, 2), 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcalt_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmflt(vfabs(temp_0, 1), vfabs(temp_1, 1), 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcalth_f16") uint16_t vcalth_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcaltq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmflt(vfabs(temp_0, 8), vfabs(temp_1, 8), 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcaltq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmflt(vfabs(temp_0, 4), vfabs(temp_1, 4), 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcaltq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmflt(vfabs(temp_0, 2), vfabs(temp_1, 2), 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcalts_f32") uint32_t vcalts_f32(float32_t a, float32_t b);
#endif
#if (64 <= __riscv_flen)
__attribute__((always_inline)) inline uint64_t vceqd_f64(float64_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a != b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#endif
__attribute__((always_inline)) inline uint64_t vceqd_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a != b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
__attribute__((always_inline)) inline uint64_t vceqd_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a != b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vceq_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfeq(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceq_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfeq(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceq_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfeq(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16_t vceqh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint16_t temp_0;
	if ((a != b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 65535U;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vceqq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfeq(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfeq(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfeq(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vceqq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmseq(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmseq(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmseq(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vceqq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmseq(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vceqq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmseq(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmseq(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmseq(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vceqq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmseq(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vceq_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmseq(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceq_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmseq(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceq_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmseq(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vceq_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmseq(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
__attribute__((always_inline)) inline uint32_t vceqs_f32(float32_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint32_t temp_0;
	if ((a != b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 4294967295UL;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vceq_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmseq(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceq_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmseq(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceq_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmseq(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vceq_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmseq(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vceqzd_f64") uint64_t vceqzd_f64(float64_t a);
#endif
NEON2RVV_NOT_IMPLEMENT("vceqzd_s64") uint64_t vceqzd_s64(int64_t a);
NEON2RVV_NOT_IMPLEMENT("vceqzd_u64") uint64_t vceqzd_u64(uint64_t a);
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vceqz_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfeq(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceqz_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfeq(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceqz_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfeq(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vceqzh_f16") uint16_t vceqzh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vceqzq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfeq(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqzq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfeq(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqzq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfeq(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vceqzq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmseq(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqzq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmseq(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqzq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmseq(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vceqzq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmseq(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vceqzq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmseq(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vceqzq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmseq(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vceqzq_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmseq(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vceqzq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmseq(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vceqz_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmseq(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceqz_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmseq(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceqz_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmseq(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vceqz_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmseq(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vceqzs_f32") uint32_t vceqzs_f32(float32_t a);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vceqz_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmseq(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vceqz_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmseq(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vceqz_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmseq(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vceqz_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmseq(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
__attribute__((always_inline)) inline uint64_t vcged_f64(float64_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#endif
__attribute__((always_inline)) inline uint64_t vcged_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
__attribute__((always_inline)) inline uint64_t vcged_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcge_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfge(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcge_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfge(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcge_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfge(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16_t vcgeh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint16_t temp_0;
	if ((a < b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 65535U;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcgeq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfge(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgeq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfge(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgeq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfge(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgeq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsge(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgeq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsge(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgeq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsge(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgeq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsge(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgeq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgeu(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgeq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgeu(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgeq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgeu(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgeq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgeu(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcge_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsge(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcge_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsge(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcge_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsge(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcge_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsge(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
__attribute__((always_inline)) inline uint32_t vcges_f32(float32_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint32_t temp_0;
	if ((a < b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 4294967295UL;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcge_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgeu(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcge_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgeu(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcge_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgeu(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcge_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgeu(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcgezd_f64") uint64_t vcgezd_f64(float64_t a);
#endif
NEON2RVV_NOT_IMPLEMENT("vcgezd_s64") uint64_t vcgezd_s64(int64_t a);
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcgez_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfge(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgez_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfge(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgez_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfge(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcgezh_f16") uint16_t vcgezh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcgezq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfge(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgezq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfge(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgezq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfge(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgezq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsge(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgezq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsge(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgezq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsge(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgezq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsge(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcgez_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsge(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgez_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsge(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgez_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsge(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcgez_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsge(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcgezs_f32") uint32_t vcgezs_f32(float32_t a);
#endif
#if (64 <= __riscv_flen)
__attribute__((always_inline)) inline uint64_t vcgtd_f64(float64_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
__attribute__((always_inline)) inline uint64_t vcgtd_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
__attribute__((always_inline)) inline uint64_t vcgtd_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcgt_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfgt(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgt_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfgt(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgt_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfgt(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16_t vcgth_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint16_t temp_0;
	if ((a > b))
	{
		temp_0 = 65535U;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcgtq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfgt(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgtq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfgt(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgtq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfgt(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgtq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgt(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgtq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgt(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgtq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgt(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgtq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgt(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgtq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgtu(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgtq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgtu(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgtq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgtu(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgtq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcgt_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgt(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgt_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgt(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgt_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgt(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcgt_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgt(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
__attribute__((always_inline)) inline uint32_t vcgts_f32(float32_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint32_t temp_0;
	if ((a > b))
	{
		temp_0 = 4294967295UL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcgt_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgtu(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgt_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgtu(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgt_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgtu(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcgt_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcgtzd_f64") uint64_t vcgtzd_f64(float64_t a);
#endif
NEON2RVV_NOT_IMPLEMENT("vcgtzd_s64") uint64_t vcgtzd_s64(int64_t a);
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcgtz_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfgt(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgtz_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfgt(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgtz_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfgt(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcgtzh_f16") uint16_t vcgtzh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcgtzq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfgt(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgtzq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfgt(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgtzq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfgt(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcgtzq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgt(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcgtzq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgt(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcgtzq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgt(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcgtzq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgt(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcgtz_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgt(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcgtz_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgt(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcgtz_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgt(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcgtz_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgt(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcgtzs_f32") uint32_t vcgtzs_f32(float32_t a);
#endif
#if (64 <= __riscv_flen)
__attribute__((always_inline)) inline uint64_t vcled_f64(float64_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#endif
__attribute__((always_inline)) inline uint64_t vcled_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
__attribute__((always_inline)) inline uint64_t vcled_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a > b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 18446744073709551615ULL;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcle_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfle(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcle_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfle(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcle_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfle(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16_t vcleh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint16_t temp_0;
	if ((a > b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 65535U;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcleq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfle(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcleq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfle(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcleq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfle(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcleq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsle(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcleq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsle(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcleq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsle(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcleq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsle(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcleq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsleu(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcleq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsleu(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcleq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsleu(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcleq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsleu(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcle_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsle(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcle_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsle(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcle_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsle(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcle_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsle(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
__attribute__((always_inline)) inline uint32_t vcles_f32(float32_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint32_t temp_0;
	if ((a > b))
	{
		temp_0 = 0;
	}
	else
	{
		temp_0 = 4294967295UL;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcle_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsleu(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcle_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsleu(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcle_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsleu(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcle_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsleu(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vclezd_f64") uint64_t vclezd_f64(float64_t a);
#endif
NEON2RVV_NOT_IMPLEMENT("vclezd_s64") uint64_t vclezd_s64(int64_t a);
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vclez_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfle(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vclez_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfle(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vclez_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfle(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vclezh_f16") uint16_t vclezh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vclezq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfle(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vclezq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfle(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vclezq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfle(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vclezq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsle(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vclezq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsle(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vclezq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsle(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vclezq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsle(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vclez_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsle(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vclez_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsle(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vclez_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsle(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vclez_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsle(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vclezs_f32") uint32_t vclezs_f32(float32_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_s16") int16x8_t vclsq_s16(int16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_s32") int32x4_t vclsq_s32(int32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_s8") int8x16_t vclsq_s8(int8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_u16") int16x8_t vclsq_u16(uint16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_u32") int32x4_t vclsq_u32(uint32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclsq_u8") int8x16_t vclsq_u8(uint8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_s16") int16x4_t vcls_s16(int16x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_s32") int32x2_t vcls_s32(int32x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_s8") int8x8_t vcls_s8(int8x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_u16") int16x4_t vcls_u16(uint16x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_u32") int32x2_t vcls_u32(uint32x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcls_u8") int8x8_t vcls_u8(uint8x8_t a);
#endif
#if (64 <= __riscv_flen)
__attribute__((always_inline)) inline uint64_t vcltd_f64(float64_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
__attribute__((always_inline)) inline uint64_t vcltd_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
__attribute__((always_inline)) inline uint64_t vcltd_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint64_t temp_0;
	if ((a < b))
	{
		temp_0 = 18446744073709551615ULL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vclt_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmflt(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vclt_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmflt(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vclt_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmflt(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16_t vclth_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint16_t temp_0;
	if ((a < b))
	{
		temp_0 = 65535U;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcltq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmflt(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcltq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmflt(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcltq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmflt(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcltq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmslt(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcltq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmslt(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcltq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmslt(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcltq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmslt(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcltq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsltu(temp_0, temp_1, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcltq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsltu(temp_0, temp_1, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcltq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsltu(temp_0, temp_1, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcltq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsltu(temp_0, temp_1, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vclt_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmslt(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vclt_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmslt(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vclt_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmslt(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vclt_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmslt(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
__attribute__((always_inline)) inline uint32_t vclts_f32(float32_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	uint32_t temp_0;
	if ((a < b))
	{
		temp_0 = 4294967295UL;
	}
	else
	{
		temp_0 = 0;
	}
	return temp_0;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vclt_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsltu(temp_0, temp_1, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vclt_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsltu(temp_0, temp_1, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vclt_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsltu(temp_0, temp_1, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vclt_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsltu(temp_0, temp_1, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcltzd_f64") uint64_t vcltzd_f64(float64_t a);
#endif
NEON2RVV_NOT_IMPLEMENT("vcltzd_s64") uint64_t vcltzd_s64(int64_t a);
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcltz_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmflt(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcltz_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmflt(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcltz_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmflt(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcltzh_f16") uint16_t vcltzh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcltzq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmflt(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcltzq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmflt(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcltzq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmflt(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcltzq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmslt(temp_0, 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcltzq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmslt(temp_0, 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcltzq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmslt(temp_0, 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcltzq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmslt(temp_0, 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcltz_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmslt(temp_0, 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcltz_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmslt(temp_0, 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcltz_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmslt(temp_0, 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcltz_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmslt(temp_0, 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcltzs_f32") uint32_t vcltzs_f32(float32_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_s16") int16x8_t vclzq_s16(int16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_s32") int32x4_t vclzq_s32(int32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_s8") int8x16_t vclzq_s8(int8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_u16") uint16x8_t vclzq_u16(uint16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_u32") uint32x4_t vclzq_u32(uint32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclzq_u8") uint8x16_t vclzq_u8(uint8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_s16") int16x4_t vclz_s16(int16x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_s32") int32x2_t vclz_s32(int32x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_s8") int8x8_t vclz_s8(int8x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_u16") uint16x4_t vclz_u16(uint16x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_u32") uint32x2_t vclz_u32(uint32x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vclz_u8") uint8x8_t vclz_u8(uint8x8_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_f16") float16x4_t vcmla_f16(float16x4_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_f32") float32x2_t vcmla_f32(float32x2_t r, float32x2_t a, float32x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_lane_f16") float16x4_t vcmla_lane_f16(float16x4_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_lane_f32") float32x2_t vcmla_lane_f32(float32x2_t r, float32x2_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_laneq_f16") float16x4_t vcmla_laneq_f16(float16x4_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_laneq_f32") float32x2_t vcmla_laneq_f32(float32x2_t r, float32x2_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_f16") float16x8_t vcmlaq_f16(float16x8_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_f32") float32x4_t vcmlaq_f32(float32x4_t r, float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_f64") float64x2_t vcmlaq_f64(float64x2_t r, float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_lane_f16") float16x8_t vcmlaq_lane_f16(float16x8_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_lane_f32") float32x4_t vcmlaq_lane_f32(float32x4_t r, float32x4_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_laneq_f16") float16x8_t vcmlaq_laneq_f16(float16x8_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_laneq_f32") float32x4_t vcmlaq_laneq_f32(float32x4_t r, float32x4_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_f16") float16x8_t vcmlaq_rot180_f16(float16x8_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_f32") float32x4_t vcmlaq_rot180_f32(float32x4_t r, float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_f64") float64x2_t vcmlaq_rot180_f64(float64x2_t r, float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_lane_f16") float16x8_t vcmlaq_rot180_lane_f16(float16x8_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_lane_f32") float32x4_t vcmlaq_rot180_lane_f32(float32x4_t r, float32x4_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_laneq_f16") float16x8_t vcmlaq_rot180_laneq_f16(float16x8_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot180_laneq_f32") float32x4_t vcmlaq_rot180_laneq_f32(float32x4_t r, float32x4_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_f16") float16x8_t vcmlaq_rot270_f16(float16x8_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_f32") float32x4_t vcmlaq_rot270_f32(float32x4_t r, float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_f64") float64x2_t vcmlaq_rot270_f64(float64x2_t r, float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_lane_f16") float16x8_t vcmlaq_rot270_lane_f16(float16x8_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_lane_f32") float32x4_t vcmlaq_rot270_lane_f32(float32x4_t r, float32x4_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_laneq_f16") float16x8_t vcmlaq_rot270_laneq_f16(float16x8_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot270_laneq_f32") float32x4_t vcmlaq_rot270_laneq_f32(float32x4_t r, float32x4_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_f16") float16x8_t vcmlaq_rot90_f16(float16x8_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_f32") float32x4_t vcmlaq_rot90_f32(float32x4_t r, float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_f64") float64x2_t vcmlaq_rot90_f64(float64x2_t r, float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_lane_f16") float16x8_t vcmlaq_rot90_lane_f16(float16x8_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_lane_f32") float32x4_t vcmlaq_rot90_lane_f32(float32x4_t r, float32x4_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_laneq_f16") float16x8_t vcmlaq_rot90_laneq_f16(float16x8_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmlaq_rot90_laneq_f32") float32x4_t vcmlaq_rot90_laneq_f32(float32x4_t r, float32x4_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_f16") float16x4_t vcmla_rot180_f16(float16x4_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_f32") float32x2_t vcmla_rot180_f32(float32x2_t r, float32x2_t a, float32x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_lane_f16") float16x4_t vcmla_rot180_lane_f16(float16x4_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_lane_f32") float32x2_t vcmla_rot180_lane_f32(float32x2_t r, float32x2_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_laneq_f16") float16x4_t vcmla_rot180_laneq_f16(float16x4_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot180_laneq_f32") float32x2_t vcmla_rot180_laneq_f32(float32x2_t r, float32x2_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_f16") float16x4_t vcmla_rot270_f16(float16x4_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_f32") float32x2_t vcmla_rot270_f32(float32x2_t r, float32x2_t a, float32x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_lane_f16") float16x4_t vcmla_rot270_lane_f16(float16x4_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_lane_f32") float32x2_t vcmla_rot270_lane_f32(float32x2_t r, float32x2_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_laneq_f16") float16x4_t vcmla_rot270_laneq_f16(float16x4_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot270_laneq_f32") float32x2_t vcmla_rot270_laneq_f32(float32x2_t r, float32x2_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_f16") float16x4_t vcmla_rot90_f16(float16x4_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_f32") float32x2_t vcmla_rot90_f32(float32x2_t r, float32x2_t a, float32x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_lane_f16") float16x4_t vcmla_rot90_lane_f16(float16x4_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_lane_f32") float32x2_t vcmla_rot90_lane_f32(float32x2_t r, float32x2_t a, float32x2_t b, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_laneq_f16") float16x4_t vcmla_rot90_laneq_f16(float16x4_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcmla_rot90_laneq_f32") float32x2_t vcmla_rot90_laneq_f32(float32x2_t r, float32x2_t a, float32x4_t b, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vcntq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vuint8m2_t temp_1 = vand(vreinterpret_v_i8m2_u8m2(temp_0), 1, 16);
	for (int temp_2 = 1; (temp_2 != 8); (++(temp_2)))
	{
		temp_1 = vadd(temp_1, vand(vsrl(vreinterpret_v_i8m2_u8m2(temp_0), temp_2, 16), 1, 16), 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcntq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = vand(temp_0, 1, 16);
	for (int temp_2 = 1; (temp_2 != 8); (++(temp_2)))
	{
		temp_1 = vadd(temp_1, vand(vsrl(temp_0, temp_2, 16), 1, 16), 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vcnt_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vuint8m1_t temp_1 = vand(vreinterpret_v_i8m1_u8m1(temp_0), 1, 8);
	for (int temp_2 = 1; (temp_2 != 8); (++(temp_2)))
	{
		temp_1 = vadd(temp_1, vand(vsrl(vreinterpret_v_i8m1_u8m1(temp_0), temp_2, 8), 1, 8), 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcnt_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = vand(temp_0, 1, 8);
	for (int temp_2 = 1; (temp_2 != 8); (++(temp_2)))
	{
		temp_1 = vadd(temp_1, vand(vsrl(temp_0, temp_2, 8), 1, 8), 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vcombine_f16(float16x4_t low, float16x4_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = vlmul_ext_v_f16m1_f16m2(__builtin_rvv_vcast_from_fixed_64_f16m1(low));
	vfloat16m2_t temp_1 = vlmul_ext_v_f16m1_f16m2(__builtin_rvv_vcast_from_fixed_64_f16m1(high));
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vslideup(temp_0, temp_1, 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vcombine_f32(float32x2_t low, float32x2_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = vlmul_ext_v_f32m1_f32m2(__builtin_rvv_vcast_from_fixed_64_f32m1(low));
	vfloat32m2_t temp_1 = vlmul_ext_v_f32m1_f32m2(__builtin_rvv_vcast_from_fixed_64_f32m1(high));
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vslideup(temp_0, temp_1, 2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcombine_f64(float64x1_t low, float64x1_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = vlmul_ext_v_f64m1_f64m2(__builtin_rvv_vcast_from_fixed_64_f64m1(low));
	vfloat64m2_t temp_1 = vlmul_ext_v_f64m1_f64m2(__builtin_rvv_vcast_from_fixed_64_f64m1(high));
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vslideup(temp_0, temp_1, 1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vcombine_s16(int16x4_t low, int16x4_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = vlmul_ext_v_i16m1_i16m2(__builtin_rvv_vcast_from_fixed_64_i16m1(low));
	vint16m2_t temp_1 = vlmul_ext_v_i16m1_i16m2(__builtin_rvv_vcast_from_fixed_64_i16m1(high));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vslideup(temp_0, temp_1, 4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcombine_s32(int32x2_t low, int32x2_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = vlmul_ext_v_i32m1_i32m2(__builtin_rvv_vcast_from_fixed_64_i32m1(low));
	vint32m2_t temp_1 = vlmul_ext_v_i32m1_i32m2(__builtin_rvv_vcast_from_fixed_64_i32m1(high));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vslideup(temp_0, temp_1, 2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcombine_s64(int64x1_t low, int64x1_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = vlmul_ext_v_i64m1_i64m2(__builtin_rvv_vcast_from_fixed_64_i64m1(low));
	vint64m2_t temp_1 = vlmul_ext_v_i64m1_i64m2(__builtin_rvv_vcast_from_fixed_64_i64m1(high));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vslideup(temp_0, temp_1, 1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vcombine_s8(int8x8_t low, int8x8_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(low));
	vint8m2_t temp_1 = vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(high));
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vslideup(temp_0, temp_1, 8, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcombine_u16(uint16x4_t low, uint16x4_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = vlmul_ext_v_u16m1_u16m2(__builtin_rvv_vcast_from_fixed_64_u16m1(low));
	vuint16m2_t temp_1 = vlmul_ext_v_u16m1_u16m2(__builtin_rvv_vcast_from_fixed_64_u16m1(high));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(temp_0, temp_1, 4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcombine_u32(uint32x2_t low, uint32x2_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = vlmul_ext_v_u32m1_u32m2(__builtin_rvv_vcast_from_fixed_64_u32m1(low));
	vuint32m2_t temp_1 = vlmul_ext_v_u32m1_u32m2(__builtin_rvv_vcast_from_fixed_64_u32m1(high));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(temp_0, temp_1, 2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcombine_u64(uint64x1_t low, uint64x1_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = vlmul_ext_v_u64m1_u64m2(__builtin_rvv_vcast_from_fixed_64_u64m1(low));
	vuint64m2_t temp_1 = vlmul_ext_v_u64m1_u64m2(__builtin_rvv_vcast_from_fixed_64_u64m1(high));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vslideup(temp_0, temp_1, 1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcombine_u8(uint8x8_t low, uint8x8_t high)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(low));
	vuint8m2_t temp_1 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(high));
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(temp_0, temp_1, 8, 16));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcopy_lane_f32(float32x2_t a, const int lane1, float32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vcopy_lane_f64(float64x1_t a, const int lane1, float64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcopy_laneq_f32(float32x2_t a, const int lane1, float32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vcopy_laneq_f64(float64x1_t a, const int lane1, float64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vcopy_laneq_s16(int16x4_t a, const int lane1, int16x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcopy_laneq_s32(int32x2_t a, const int lane1, int32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcopy_laneq_s64(int64x1_t a, const int lane1, int64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vcopy_laneq_s8(int8x8_t a, const int lane1, int8x16_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcopy_laneq_u16(uint16x4_t a, const int lane1, uint16x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcopy_laneq_u32(uint32x2_t a, const int lane1, uint32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcopy_laneq_u64(uint64x1_t a, const int lane1, uint64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcopy_laneq_u8(uint8x8_t a, const int lane1, uint8x16_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vcopy_lane_s16(int16x4_t a, const int lane1, int16x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcopy_lane_s32(int32x2_t a, const int lane1, int32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcopy_lane_s64(int64x1_t a, const int lane1, int64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vcopy_lane_s8(int8x8_t a, const int lane1, int8x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcopy_lane_u16(uint16x4_t a, const int lane1, uint16x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcopy_lane_u32(uint32x2_t a, const int lane1, uint32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcopy_lane_u64(uint64x1_t a, const int lane1, uint64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcopy_lane_u8(uint8x8_t a, const int lane1, uint8x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vcopyq_lane_f32(float32x4_t a, const int lane1, float32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcopyq_lane_f64(float64x2_t a, const int lane1, float64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vcopyq_laneq_f32(float32x4_t a, const int lane1, float32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcopyq_laneq_f64(float64x2_t a, const int lane1, float64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vcopyq_laneq_s16(int16x8_t a, const int lane1, int16x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcopyq_laneq_s32(int32x4_t a, const int lane1, int32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcopyq_laneq_s64(int64x2_t a, const int lane1, int64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vcopyq_laneq_s8(int8x16_t a, const int lane1, int8x16_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcopyq_laneq_u16(uint16x8_t a, const int lane1, uint16x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcopyq_laneq_u32(uint32x4_t a, const int lane1, uint32x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcopyq_laneq_u64(uint64x2_t a, const int lane1, uint64x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcopyq_laneq_u8(uint8x16_t a, const int lane1, uint8x16_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vcopyq_lane_s16(int16x8_t a, const int lane1, int16x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcopyq_lane_s32(int32x4_t a, const int lane1, int32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcopyq_lane_s64(int64x2_t a, const int lane1, int64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vcopyq_lane_s8(int8x16_t a, const int lane1, int8x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vcopyq_lane_u16(uint16x8_t a, const int lane1, uint16x4_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcopyq_lane_u32(uint32x4_t a, const int lane1, uint32x2_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcopyq_lane_u64(uint64x2_t a, const int lane1, uint64x1_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vcopyq_lane_u8(uint8x16_t a, const int lane1, uint8x8_t b, const int lane2)
{
	NEON2RVV_DEBUG_FUNCTION;
	a[lane1] = b[lane2];
	return a;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vcreate_f16(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1(((const float16_t *)((&a))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcreate_f32(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1(((const float32_t *)((&a))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vcreate_f64(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1(((const float64_t *)((&a))), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vcreate_s16(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1(((const int16_t *)((&a))), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcreate_s32(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1(((const int32_t *)((&a))), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcreate_s64(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1(((const int64_t *)((&a))), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vcreate_s8(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1(((const int8_t *)((&a))), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vcreate_u16(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1(((const uint16_t *)((&a))), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcreate_u32(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1(((const uint32_t *)((&a))), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcreate_u64(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1(((const uint64_t *)((&a))), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vcreate_u8(uint64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1(((const uint8_t *)((&a))), 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtad_s64_f64") int64_t vcvtad_s64_f64(float64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtad_u64_f64") uint64_t vcvtad_u64_f64(float64_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_s16_f16") int16_t vcvtah_s16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_s32_f16") int32_t vcvtah_s32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_s64_f16") int64_t vcvtah_s64_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_u16_f16") uint16_t vcvtah_u16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_u32_f16") uint32_t vcvtah_u32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtah_u64_f16") uint64_t vcvtah_u64_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vcvtaq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcvtaq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcvtaq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcvtaq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vfcvt_xu(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcvtaq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcvtaq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint64m2_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vcvta_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcvta_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcvta_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtas_s32_f32") int32_t vcvtas_s32_f32(float32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtas_u32_f32") uint32_t vcvtas_u32_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcvta_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcvta_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcvta_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vuint64m1_t temp_3 = vfcvt_xu(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_f64_s64") float64_t vcvtd_f64_s64(int64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_f64_u64") float64_t vcvtd_f64_u64(uint64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_n_f64_s64") float64_t vcvtd_n_f64_s64(int64_t a, const int n);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_n_f64_u64") float64_t vcvtd_n_f64_u64(uint64_t a, const int n);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_n_s64_f64") int64_t vcvtd_n_s64_f64(float64_t a, const int n);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_n_u64_f64") uint64_t vcvtd_n_u64_f64(float64_t a, const int n);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_s64_f64") int64_t vcvtd_s64_f64(float64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtd_u64_f64") uint64_t vcvtd_u64_f64(float64_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vcvt_f16_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfncvt_f(temp_0, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vcvt_f16_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfcvt_f(temp_0, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vcvt_f16_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfcvt_f(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float32x4_t vcvt_f32_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfwcvt_f(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcvt_f32_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfncvt_f(temp_0, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcvt_f32_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfcvt_f(temp_0, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vcvt_f32_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfcvt_f(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcvt_f64_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfwcvt_f(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vcvt_f64_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfcvt_f(temp_0, 1));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vcvt_f64_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfcvt_f(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_s16") float16_t vcvth_f16_s16(int16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_s32") float16_t vcvth_f16_s32(int32_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_s64") float16_t vcvth_f16_s64(int64_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_u16") float16_t vcvth_f16_u16(uint16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_u32") float16_t vcvth_f16_u32(uint32_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_f16_u64") float16_t vcvth_f16_u64(uint64_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_high_f16_f32") float16x8_t vcvt_high_f16_f32(float16x4_t r, float32x4_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_high_f32_f16") float32x4_t vcvt_high_f32_f16(float16x8_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_high_f32_f64") float32x4_t vcvt_high_f32_f64(float32x2_t r, float64x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_high_f64_f32") float64x2_t vcvt_high_f64_f32(float32x4_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_s16") float16_t vcvth_n_f16_s16(int16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_s32") float16_t vcvth_n_f16_s32(int32_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_s64") float16_t vcvth_n_f16_s64(int64_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_u16") float16_t vcvth_n_f16_u16(uint16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_u32") float16_t vcvth_n_f16_u32(uint32_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_f16_u64") float16_t vcvth_n_f16_u64(uint64_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_s16_f16") int16_t vcvth_n_s16_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_s32_f16") int32_t vcvth_n_s32_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_s64_f16") int64_t vcvth_n_s64_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_u16_f16") uint16_t vcvth_n_u16_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_u32_f16") uint32_t vcvth_n_u32_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_n_u64_f16") uint64_t vcvth_n_u64_f16(float16_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_s16_f16") int16_t vcvth_s16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_s32_f16") int32_t vcvth_s32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_s64_f16") int64_t vcvth_s64_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_u16_f16") uint16_t vcvth_u16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_u32_f16") uint32_t vcvth_u32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvth_u64_f16") uint64_t vcvth_u64_f16(float16_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtmd_s64_f64") int64_t vcvtmd_s64_f64(float64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtmd_u64_f64") uint64_t vcvtmd_u64_f64(float64_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_s16_f16") int16_t vcvtmh_s16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_s32_f16") int32_t vcvtmh_s32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_s64_f16") int64_t vcvtmh_s64_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_u16_f16") uint16_t vcvtmh_u16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_u32_f16") uint32_t vcvtmh_u32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtmh_u64_f16") uint64_t vcvtmh_u64_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vcvtmq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcvtmq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcvtmq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcvtmq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vfcvt_xu(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcvtmq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcvtmq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint64m2_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vcvtm_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcvtm_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcvtm_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtms_s32_f32") int32_t vcvtms_s32_f32(float32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtms_u32_f32") uint32_t vcvtms_u32_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcvtm_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcvtm_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcvtm_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vuint64m1_t temp_3 = vfcvt_xu(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtnd_s64_f64") int64_t vcvtnd_s64_f64(float64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtnd_u64_f64") uint64_t vcvtnd_u64_f64(float64_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f16_s16") float16x4_t vcvt_n_f16_s16(int16x4_t a, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f16_u16") float16x4_t vcvt_n_f16_u16(uint16x4_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f32_s32") float32x2_t vcvt_n_f32_s32(int32x2_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f32_u32") float32x2_t vcvt_n_f32_u32(uint32x2_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f64_s64") float64x1_t vcvt_n_f64_s64(int64x1_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_f64_u64") float64x1_t vcvt_n_f64_u64(uint64x1_t a, const int n);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_s16_f16") int16_t vcvtnh_s16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_s32_f16") int32_t vcvtnh_s32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_s64_f16") int64_t vcvtnh_s64_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_u16_f16") uint16_t vcvtnh_u16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_u32_f16") uint32_t vcvtnh_u32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtnh_u64_f16") uint64_t vcvtnh_u64_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vcvtnq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcvtnq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcvtnq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcvtnq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vfcvt_xu(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcvtnq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcvtnq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint64m2_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vcvtn_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_s16_f16") int16x4_t vcvt_n_s16_f16(float16x4_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcvtn_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_s32_f32") int32x2_t vcvt_n_s32_f32(float32x2_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcvtn_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_s64_f64") int64x1_t vcvt_n_s64_f64(float64x1_t a, const int n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtns_s32_f32") int32_t vcvtns_s32_f32(float32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtns_u32_f32") uint32_t vcvtns_u32_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcvtn_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_u16_f16") uint16x4_t vcvt_n_u16_f16(float16x4_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcvtn_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_u32_f32") uint32x2_t vcvt_n_u32_f32(float32x2_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcvtn_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vuint64m1_t temp_3 = vfcvt_xu(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvt_n_u64_f64") uint64x1_t vcvt_n_u64_f64(float64x1_t a, const int n);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtpd_s64_f64") int64_t vcvtpd_s64_f64(float64_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtpd_u64_f64") uint64_t vcvtpd_u64_f64(float64_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_s16_f16") int16_t vcvtph_s16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_s32_f16") int32_t vcvtph_s32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_s64_f16") int64_t vcvtph_s64_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_u16_f16") uint16_t vcvtph_u16_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_u32_f16") uint32_t vcvtph_u32_f16(float16_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtph_u64_f16") uint64_t vcvtph_u64_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vcvtpq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcvtpq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcvtpq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcvtpq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vfcvt_xu(temp_0, 8);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfne(temp_0, temp_0, 8), temp_3, 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcvtpq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcvtpq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint64m2_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vcvtp_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcvtp_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcvtp_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtps_s32_f32") int32_t vcvtps_s32_f32(float32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtps_u32_f32") uint32_t vcvtps_u32_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcvtp_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vfcvt_xu(temp_0, 4);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfne(temp_0, temp_0, 4), temp_3, 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcvtp_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vfcvt_xu(temp_0, 2);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfne(temp_0, temp_0, 2), temp_3, 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcvtp_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vuint64m1_t temp_3 = vfcvt_xu(temp_0, 1);
	fesetround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfne(temp_0, temp_0, 1), temp_3, 0, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vcvtq_f16_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfcvt_f(temp_0, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vcvtq_f16_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfcvt_f(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vcvtq_f32_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfcvt_f(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vcvtq_f32_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfcvt_f(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcvtq_f64_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfcvt_f(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vcvtq_f64_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfcvt_f(temp_0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f16_s16") float16x8_t vcvtq_n_f16_s16(int16x8_t a, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f16_u16") float16x8_t vcvtq_n_f16_u16(uint16x8_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f32_s32") float32x4_t vcvtq_n_f32_s32(int32x4_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f32_u32") float32x4_t vcvtq_n_f32_u32(uint32x4_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f64_s64") float64x2_t vcvtq_n_f64_s64(int64x2_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_f64_u64") float64x2_t vcvtq_n_f64_u64(uint64x2_t a, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_s16_f16") int16x8_t vcvtq_n_s16_f16(float16x8_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_s32_f32") int32x4_t vcvtq_n_s32_f32(float32x4_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_s64_f64") int64x2_t vcvtq_n_s64_f64(float64x2_t a, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_u16_f16") uint16x8_t vcvtq_n_u16_f16(float16x8_t a, const int n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_u32_f32") uint32x4_t vcvtq_n_u32_f32(float32x4_t a, const int n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtq_n_u64_f64") uint64x2_t vcvtq_n_u64_f64(float64x2_t a, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vcvtq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmfne(temp_0, temp_0, 8), vfcvt_rtz_x(temp_0, 8), 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vcvtq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmfne(temp_0, temp_0, 4), vfcvt_rtz_x(temp_0, 4), 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vcvtq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmfne(temp_0, temp_0, 2), vfcvt_rtz_x(temp_0, 2), 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vcvtq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmfne(temp_0, temp_0, 8), vfcvt_rtz_xu(temp_0, 8), 0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vcvtq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmfne(temp_0, temp_0, 4), vfcvt_rtz_xu(temp_0, 4), 0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vcvtq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmfne(temp_0, temp_0, 2), vfcvt_rtz_xu(temp_0, 2), 0, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vcvt_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmfne(temp_0, temp_0, 4), vfcvt_rtz_x(temp_0, 4), 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vcvt_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmfne(temp_0, temp_0, 2), vfcvt_rtz_x(temp_0, 2), 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vcvt_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmfne(temp_0, temp_0, 1), vfcvt_rtz_x(temp_0, 1), 0, 1));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_f32_s32") float32_t vcvts_f32_s32(int32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_f32_u32") float32_t vcvts_f32_u32(uint32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_n_f32_s32") float32_t vcvts_n_f32_s32(int32_t a, const int n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_n_f32_u32") float32_t vcvts_n_f32_u32(uint32_t a, const int n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_n_s32_f32") int32_t vcvts_n_s32_f32(float32_t a, const int n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_n_u32_f32") uint32_t vcvts_n_u32_f32(float32_t a, const int n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_s32_f32") int32_t vcvts_s32_f32(float32_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvts_u32_f32") uint32_t vcvts_u32_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vcvt_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmfne(temp_0, temp_0, 4), vfcvt_rtz_xu(temp_0, 4), 0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vcvt_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmfne(temp_0, temp_0, 2), vfcvt_rtz_xu(temp_0, 2), 0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vcvt_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmfne(temp_0, temp_0, 1), vfcvt_rtz_xu(temp_0, 1), 0, 1));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vcvtxd_f32_f64") float32_t vcvtxd_f32_f64(float64_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtx_f32_f64") float32x2_t vcvtx_f32_f64(float64x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vcvtx_high_f32_f64") float32x4_t vcvtx_high_f32_f64(float32x2_t r, float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vdiv_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfdiv(temp_0, temp_1, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vdiv_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfdiv(temp_0, temp_1, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vdiv_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfdiv(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vdivh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a / b);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vdivq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfdiv(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vdivq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfdiv(temp_0, temp_1, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vdivq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfdiv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vdot_laneq_s32(int32x2_t r, int8x8_t a, int8x16_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(r);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	int16_t temp_3[8];
	vint16m2_t temp_2 = vwcvt_x(temp_1, 8);
	vse16(temp_3, temp_2, 8);
	vint16mf2_t temp_4;
	vint16mf2_t temp_5;
	vint16mf2_t temp_6;
	vint16mf2_t temp_7;
	vlseg4e16_v_i16mf2((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, b[((lane * 4) + 0)], temp_4, 2), b[((lane * 4) + 1)], temp_5, 2), b[((lane * 4) + 2)], temp_6, 2), b[((lane * 4) + 3)], temp_7, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vdot_laneq_u32(uint32x2_t r, uint8x8_t a, uint8x16_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(r);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	uint16_t temp_3[8];
	vuint16m2_t temp_2 = vwcvtu_x(temp_1, 8);
	vse16(temp_3, temp_2, 8);
	vuint16mf2_t temp_4;
	vuint16mf2_t temp_5;
	vuint16mf2_t temp_6;
	vuint16mf2_t temp_7;
	vlseg4e16_v_u16mf2((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, b[((lane * 4) + 0)], temp_4, 2), b[((lane * 4) + 1)], temp_5, 2), b[((lane * 4) + 2)], temp_6, 2), b[((lane * 4) + 3)], temp_7, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vdot_lane_s32(int32x2_t r, int8x8_t a, int8x8_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(r);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	int16_t temp_3[8];
	vint16m2_t temp_2 = vwcvt_x(temp_1, 8);
	vse16(temp_3, temp_2, 8);
	vint16mf2_t temp_4;
	vint16mf2_t temp_5;
	vint16mf2_t temp_6;
	vint16mf2_t temp_7;
	vlseg4e16_v_i16mf2((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, b[((lane * 4) + 0)], temp_4, 2), b[((lane * 4) + 1)], temp_5, 2), b[((lane * 4) + 2)], temp_6, 2), b[((lane * 4) + 3)], temp_7, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vdot_lane_u32(uint32x2_t r, uint8x8_t a, uint8x8_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(r);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	uint16_t temp_3[8];
	vuint16m2_t temp_2 = vwcvtu_x(temp_1, 8);
	vse16(temp_3, temp_2, 8);
	vuint16mf2_t temp_4;
	vuint16mf2_t temp_5;
	vuint16mf2_t temp_6;
	vuint16mf2_t temp_7;
	vlseg4e16_v_u16mf2((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, b[((lane * 4) + 0)], temp_4, 2), b[((lane * 4) + 1)], temp_5, 2), b[((lane * 4) + 2)], temp_6, 2), b[((lane * 4) + 3)], temp_7, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vdotq_laneq_s32(int32x4_t r, int8x16_t a, int8x16_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(r);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	int16_t temp_3[16];
	vint16m4_t temp_2 = vwcvt_x(temp_1, 16);
	vse16(temp_3, temp_2, 16);
	vint16m1_t temp_4;
	vint16m1_t temp_5;
	vint16m1_t temp_6;
	vint16m1_t temp_7;
	vlseg4e16_v_i16m1((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, b[((lane * 4) + 0)], temp_4, 4), b[((lane * 4) + 1)], temp_5, 4), b[((lane * 4) + 2)], temp_6, 4), b[((lane * 4) + 3)], temp_7, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vdotq_laneq_u32(uint32x4_t r, uint8x16_t a, uint8x16_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(r);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	uint16_t temp_3[16];
	vuint16m4_t temp_2 = vwcvtu_x(temp_1, 16);
	vse16(temp_3, temp_2, 16);
	vuint16m1_t temp_4;
	vuint16m1_t temp_5;
	vuint16m1_t temp_6;
	vuint16m1_t temp_7;
	vlseg4e16_v_u16m1((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, b[((lane * 4) + 0)], temp_4, 4), b[((lane * 4) + 1)], temp_5, 4), b[((lane * 4) + 2)], temp_6, 4), b[((lane * 4) + 3)], temp_7, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vdotq_lane_s32(int32x4_t r, int8x16_t a, int8x8_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(r);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	int16_t temp_3[16];
	vint16m4_t temp_2 = vwcvt_x(temp_1, 16);
	vse16(temp_3, temp_2, 16);
	vint16m1_t temp_4;
	vint16m1_t temp_5;
	vint16m1_t temp_6;
	vint16m1_t temp_7;
	vlseg4e16_v_i16m1((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, b[((lane * 4) + 0)], temp_4, 4), b[((lane * 4) + 1)], temp_5, 4), b[((lane * 4) + 2)], temp_6, 4), b[((lane * 4) + 3)], temp_7, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vdotq_lane_u32(uint32x4_t r, uint8x16_t a, uint8x8_t b, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(r);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	uint16_t temp_3[16];
	vuint16m4_t temp_2 = vwcvtu_x(temp_1, 16);
	vse16(temp_3, temp_2, 16);
	vuint16m1_t temp_4;
	vuint16m1_t temp_5;
	vuint16m1_t temp_6;
	vuint16m1_t temp_7;
	vlseg4e16_v_u16m1((&temp_4), (&temp_5), (&temp_6), (&temp_7), temp_3, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, b[((lane * 4) + 0)], temp_4, 4), b[((lane * 4) + 1)], temp_5, 4), b[((lane * 4) + 2)], temp_6, 4), b[((lane * 4) + 3)], temp_7, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vdotq_s32(int32x4_t r, int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(r);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int16_t temp_5[16];
	int16_t temp_6[16];
	vint16m4_t temp_3 = vwcvt_x(temp_1, 16);
	vint16m4_t temp_4 = vwcvt_x(temp_2, 16);
	vse16(temp_5, temp_3, 16);
	vse16(temp_6, temp_4, 16);
	vint16m1_t temp_7;
	vint16m1_t temp_8;
	vint16m1_t temp_9;
	vint16m1_t temp_10;
	vlseg4e16_v_i16m1((&temp_7), (&temp_8), (&temp_9), (&temp_10), temp_5, 4);
	vint16m1_t temp_11;
	vint16m1_t temp_12;
	vint16m1_t temp_13;
	vint16m1_t temp_14;
	vlseg4e16_v_i16m1((&temp_11), (&temp_12), (&temp_13), (&temp_14), temp_6, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, temp_7, temp_11, 4), temp_8, temp_12, 4), temp_9, temp_13, 4), temp_10, temp_14, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vdotq_u32(uint32x4_t r, uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(r);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint16_t temp_5[16];
	uint16_t temp_6[16];
	vuint16m4_t temp_3 = vwcvtu_x(temp_1, 16);
	vuint16m4_t temp_4 = vwcvtu_x(temp_2, 16);
	vse16(temp_5, temp_3, 16);
	vse16(temp_6, temp_4, 16);
	vuint16m1_t temp_7;
	vuint16m1_t temp_8;
	vuint16m1_t temp_9;
	vuint16m1_t temp_10;
	vlseg4e16_v_u16m1((&temp_7), (&temp_8), (&temp_9), (&temp_10), temp_5, 4);
	vuint16m1_t temp_11;
	vuint16m1_t temp_12;
	vuint16m1_t temp_13;
	vuint16m1_t temp_14;
	vlseg4e16_v_u16m1((&temp_11), (&temp_12), (&temp_13), (&temp_14), temp_6, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, temp_7, temp_11, 4), temp_8, temp_12, 4), temp_9, temp_13, 4), temp_10, temp_14, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vdot_s32(int32x2_t r, int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(r);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int16_t temp_5[8];
	int16_t temp_6[8];
	vint16m2_t temp_3 = vwcvt_x(temp_1, 8);
	vint16m2_t temp_4 = vwcvt_x(temp_2, 8);
	vse16(temp_5, temp_3, 8);
	vse16(temp_6, temp_4, 8);
	vint16mf2_t temp_7;
	vint16mf2_t temp_8;
	vint16mf2_t temp_9;
	vint16mf2_t temp_10;
	vlseg4e16_v_i16mf2((&temp_7), (&temp_8), (&temp_9), (&temp_10), temp_5, 2);
	vint16mf2_t temp_11;
	vint16mf2_t temp_12;
	vint16mf2_t temp_13;
	vint16mf2_t temp_14;
	vlseg4e16_v_i16mf2((&temp_11), (&temp_12), (&temp_13), (&temp_14), temp_6, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vwmacc(vwmacc(vwmacc(vwmacc(temp_0, temp_7, temp_11, 2), temp_8, temp_12, 2), temp_9, temp_13, 2), temp_10, temp_14, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vdot_u32(uint32x2_t r, uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(r);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint16_t temp_5[8];
	uint16_t temp_6[8];
	vuint16m2_t temp_3 = vwcvtu_x(temp_1, 8);
	vuint16m2_t temp_4 = vwcvtu_x(temp_2, 8);
	vse16(temp_5, temp_3, 8);
	vse16(temp_6, temp_4, 8);
	vuint16mf2_t temp_7;
	vuint16mf2_t temp_8;
	vuint16mf2_t temp_9;
	vuint16mf2_t temp_10;
	vlseg4e16_v_u16mf2((&temp_7), (&temp_8), (&temp_9), (&temp_10), temp_5, 2);
	vuint16mf2_t temp_11;
	vuint16mf2_t temp_12;
	vuint16mf2_t temp_13;
	vuint16mf2_t temp_14;
	vlseg4e16_v_u16mf2((&temp_11), (&temp_12), (&temp_13), (&temp_14), temp_6, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vwmaccu(vwmaccu(vwmaccu(vwmaccu(temp_0, temp_7, temp_11, 2), temp_8, temp_12, 2), temp_9, temp_13, 2), temp_10, temp_14, 2));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupb_laneq_s8") int8_t vdupb_laneq_s8(int8x16_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupb_laneq_u8") uint8_t vdupb_laneq_u8(uint8x16_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupb_lane_s8") int8_t vdupb_lane_s8(int8x8_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupb_lane_u8") uint8_t vdupb_lane_u8(uint8x8_t vec, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_lane_f64") float64_t vdupd_lane_f64(float64x1_t vec, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_laneq_f64") float64_t vdupd_laneq_f64(float64x2_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_laneq_s64") int64_t vdupd_laneq_s64(int64x2_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_laneq_u64") uint64_t vdupd_laneq_u64(uint64x2_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_lane_s64") int64_t vdupd_lane_s64(int64x1_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdupd_lane_u64") uint64_t vdupd_lane_u64(uint64x1_t vec, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vduph_lane_f16") float16_t vduph_lane_f16(float16x4_t vec, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vduph_laneq_f16") float16_t vduph_laneq_f16(float16x8_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vduph_laneq_s16") int16_t vduph_laneq_s16(int16x8_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vduph_laneq_u16") uint16_t vduph_laneq_u16(uint16x8_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vduph_lane_s16") int16_t vduph_lane_s16(int16x4_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vduph_lane_u16") uint16_t vduph_lane_u16(uint16x4_t vec, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vdup_lane_f16(float16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vrgather(temp_0, lane, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vdup_lane_f32(float32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vrgather(temp_0, lane, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vdup_lane_f64(float64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vrgather(temp_0, lane, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vdup_laneq_f16(float16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vlmul_trunc_v_f16m2_f16m1(vrgather(temp_0, lane, 4)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vdup_laneq_f32(float32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vlmul_trunc_v_f32m2_f32m1(vrgather(temp_0, lane, 2)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vdup_laneq_f64(float64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vlmul_trunc_v_f64m2_f64m1(vrgather(temp_0, lane, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vdup_laneq_s16(int16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vlmul_trunc_v_i16m2_i16m1(vrgather(temp_0, lane, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vdup_laneq_s32(int32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vlmul_trunc_v_i32m2_i32m1(vrgather(temp_0, lane, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vdup_laneq_s64(int64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vlmul_trunc_v_i64m2_i64m1(vrgather(temp_0, lane, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vdup_laneq_s8(int8x16_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vrgather(temp_0, lane, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vdup_laneq_u16(uint16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vlmul_trunc_v_u16m2_u16m1(vrgather(temp_0, lane, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vdup_laneq_u32(uint32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vlmul_trunc_v_u32m2_u32m1(vrgather(temp_0, lane, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vdup_laneq_u64(uint64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vlmul_trunc_v_u64m2_u64m1(vrgather(temp_0, lane, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vdup_laneq_u8(uint8x16_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vrgather(temp_0, lane, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vdup_lane_s16(int16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vrgather(temp_0, lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vdup_lane_s32(int32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vrgather(temp_0, lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vdup_lane_s64(int64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vrgather(temp_0, lane, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vdup_lane_s8(int8x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vrgather(temp_0, lane, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vdup_lane_u16(uint16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vrgather(temp_0, lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vdup_lane_u32(uint32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vrgather(temp_0, lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vdup_lane_u64(uint64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vrgather(temp_0, lane, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vdup_lane_u8(uint8x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vrgather(temp_0, lane, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vdup_n_f16(float16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(value, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vdup_n_f32(float32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(value, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vdup_n_f64(float64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(value, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vdup_n_s16(int16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vdup_n_s32(int32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vdup_n_s64(int64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(value, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vdup_n_s8(int8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vdup_n_u16(uint16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vdup_n_u32(uint32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vdup_n_u64(uint64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(value, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vdup_n_u8(uint8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(value, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vdupq_lane_f16(float16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vrgather(vlmul_ext_v_f16m1_f16m2(temp_0), lane, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vdupq_lane_f32(float32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vrgather(vlmul_ext_v_f32m1_f32m2(temp_0), lane, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vdupq_lane_f64(float64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vrgather(vlmul_ext_v_f64m1_f64m2(temp_0), lane, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vdupq_laneq_f16(float16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vrgather(temp_0, lane, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vdupq_laneq_f32(float32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vrgather(temp_0, lane, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vdupq_laneq_f64(float64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vrgather(temp_0, lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vdupq_laneq_s16(int16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vrgather(temp_0, lane, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vdupq_laneq_s32(int32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vrgather(temp_0, lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vdupq_laneq_s64(int64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vrgather(temp_0, lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vdupq_laneq_s8(int8x16_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vrgather(temp_0, lane, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vdupq_laneq_u16(uint16x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vrgather(temp_0, lane, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vdupq_laneq_u32(uint32x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vrgather(temp_0, lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vdupq_laneq_u64(uint64x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vrgather(temp_0, lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vdupq_laneq_u8(uint8x16_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(vec);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vrgather(temp_0, lane, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vdupq_lane_s16(int16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vrgather(vlmul_ext_v_i16m1_i16m2(temp_0), lane, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vdupq_lane_s32(int32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vrgather(vlmul_ext_v_i32m1_i32m2(temp_0), lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vdupq_lane_s64(int64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vrgather(vlmul_ext_v_i64m1_i64m2(temp_0), lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vdupq_lane_s8(int8x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vrgather(vlmul_ext_v_i8m1_i8m2(temp_0), lane, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vdupq_lane_u16(uint16x4_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vrgather(vlmul_ext_v_u16m1_u16m2(temp_0), lane, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vdupq_lane_u32(uint32x2_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vrgather(vlmul_ext_v_u32m1_u32m2(temp_0), lane, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vdupq_lane_u64(uint64x1_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vrgather(vlmul_ext_v_u64m1_u64m2(temp_0), lane, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vdupq_lane_u8(uint8x8_t vec, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(vec);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vrgather(vlmul_ext_v_u8m1_u8m2(temp_0), lane, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vdupq_n_f16(float16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(value, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vdupq_n_f32(float32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(value, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vdupq_n_f64(float64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vdupq_n_s16(int16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vdupq_n_s32(int32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vdupq_n_s64(int64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vdupq_n_s8(int8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(value, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vdupq_n_u16(uint16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vdupq_n_u32(uint32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vdupq_n_u64(uint64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vdupq_n_u8(uint8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(value, 16));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_lane_f32") float32_t vdups_lane_f32(float32x2_t vec, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_laneq_f32") float32_t vdups_laneq_f32(float32x4_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_laneq_s32") int32_t vdups_laneq_s32(int32x4_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_laneq_u32") uint32_t vdups_laneq_u32(uint32x4_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_lane_s32") int32_t vdups_lane_s32(int32x2_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vdups_lane_u32") uint32_t vdups_lane_u32(uint32x2_t vec, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_s16") int16x8_t veor3q_s16(int16x8_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_s32") int32x4_t veor3q_s32(int32x4_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_s64") int64x2_t veor3q_s64(int64x2_t a, int64x2_t b, int64x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_s8") int8x16_t veor3q_s8(int8x16_t a, int8x16_t b, int8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_u16") uint16x8_t veor3q_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_u32") uint32x4_t veor3q_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_u64") uint64x2_t veor3q_u64(uint64x2_t a, uint64x2_t b, uint64x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("veor3q_u8") uint8x16_t veor3q_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t veorq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vxor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t veorq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vxor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t veorq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vxor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t veorq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vxor(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t veorq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vxor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t veorq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vxor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t veorq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vxor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t veorq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vxor(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t veor_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vxor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t veor_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vxor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t veor_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vxor(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t veor_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vxor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t veor_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vxor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t veor_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vxor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t veor_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vxor(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t veor_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vxor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vext_f16(float16x4_t a, float16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = vlmul_ext_v_f16m1_f16m2(__builtin_rvv_vcast_from_fixed_64_f16m1(a));
	vfloat16m2_t temp_1 = vlmul_ext_v_f16m1_f16m2(__builtin_rvv_vcast_from_fixed_64_f16m1(b));
	vfloat16m2_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vlmul_trunc_v_f16m2_f16m1(vslidedown(vundefined_f16m2(), temp_2, n, 4)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vext_f32(float32x2_t a, float32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = vlmul_ext_v_f32m1_f32m2(__builtin_rvv_vcast_from_fixed_64_f32m1(a));
	vfloat32m2_t temp_1 = vlmul_ext_v_f32m1_f32m2(__builtin_rvv_vcast_from_fixed_64_f32m1(b));
	vfloat32m2_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vlmul_trunc_v_f32m2_f32m1(vslidedown(vundefined_f32m2(), temp_2, n, 2)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vext_f64(float64x1_t a, float64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = vlmul_ext_v_f64m1_f64m2(__builtin_rvv_vcast_from_fixed_64_f64m1(a));
	vfloat64m2_t temp_1 = vlmul_ext_v_f64m1_f64m2(__builtin_rvv_vcast_from_fixed_64_f64m1(b));
	vfloat64m2_t temp_2 = vslideup(temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vlmul_trunc_v_f64m2_f64m1(vslidedown(vundefined_f64m2(), temp_2, n, 1)));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vextq_f16(float16x8_t a, float16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m4_t temp_0 = vlmul_ext_v_f16m2_f16m4(__builtin_rvv_vcast_from_fixed_64_f16m2(a));
	vfloat16m4_t temp_1 = vlmul_ext_v_f16m2_f16m4(__builtin_rvv_vcast_from_fixed_64_f16m2(b));
	vfloat16m4_t temp_2 = vslideup(temp_0, temp_1, 8, 16);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vlmul_trunc_v_f16m4_f16m2(vslidedown(vundefined_f16m4(), temp_2, n, 8)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vextq_f32(float32x4_t a, float32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m4_t temp_0 = vlmul_ext_v_f32m2_f32m4(__builtin_rvv_vcast_from_fixed_64_f32m2(a));
	vfloat32m4_t temp_1 = vlmul_ext_v_f32m2_f32m4(__builtin_rvv_vcast_from_fixed_64_f32m2(b));
	vfloat32m4_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vlmul_trunc_v_f32m4_f32m2(vslidedown(vundefined_f32m4(), temp_2, n, 4)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vextq_f64(float64x2_t a, float64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m4_t temp_0 = vlmul_ext_v_f64m2_f64m4(__builtin_rvv_vcast_from_fixed_64_f64m2(a));
	vfloat64m4_t temp_1 = vlmul_ext_v_f64m2_f64m4(__builtin_rvv_vcast_from_fixed_64_f64m2(b));
	vfloat64m4_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vlmul_trunc_v_f64m4_f64m2(vslidedown(vundefined_f64m4(), temp_2, n, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vextq_s16(int16x8_t a, int16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m4_t temp_0 = vlmul_ext_v_i16m2_i16m4(__builtin_rvv_vcast_from_fixed_64_i16m2(a));
	vint16m4_t temp_1 = vlmul_ext_v_i16m2_i16m4(__builtin_rvv_vcast_from_fixed_64_i16m2(b));
	vint16m4_t temp_2 = vslideup(temp_0, temp_1, 8, 16);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vlmul_trunc_v_i16m4_i16m2(vslidedown(vundefined_i16m4(), temp_2, n, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vextq_s32(int32x4_t a, int32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m4_t temp_0 = vlmul_ext_v_i32m2_i32m4(__builtin_rvv_vcast_from_fixed_64_i32m2(a));
	vint32m4_t temp_1 = vlmul_ext_v_i32m2_i32m4(__builtin_rvv_vcast_from_fixed_64_i32m2(b));
	vint32m4_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vlmul_trunc_v_i32m4_i32m2(vslidedown(vundefined_i32m4(), temp_2, n, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vextq_s64(int64x2_t a, int64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m4_t temp_0 = vlmul_ext_v_i64m2_i64m4(__builtin_rvv_vcast_from_fixed_64_i64m2(a));
	vint64m4_t temp_1 = vlmul_ext_v_i64m2_i64m4(__builtin_rvv_vcast_from_fixed_64_i64m2(b));
	vint64m4_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vlmul_trunc_v_i64m4_i64m2(vslidedown(vundefined_i64m4(), temp_2, n, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vextq_s8(int8x16_t a, int8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vlmul_ext_v_i8m2_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m2(a));
	vint8m4_t temp_1 = vlmul_ext_v_i8m2_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m2(b));
	vint8m4_t temp_2 = vslideup(temp_0, temp_1, 16, 32);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vlmul_trunc_v_i8m4_i8m2(vslidedown(vundefined_i8m4(), temp_2, n, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vextq_u16(uint16x8_t a, uint16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m4_t temp_0 = vlmul_ext_v_u16m2_u16m4(__builtin_rvv_vcast_from_fixed_64_u16m2(a));
	vuint16m4_t temp_1 = vlmul_ext_v_u16m2_u16m4(__builtin_rvv_vcast_from_fixed_64_u16m2(b));
	vuint16m4_t temp_2 = vslideup(temp_0, temp_1, 8, 16);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vlmul_trunc_v_u16m4_u16m2(vslidedown(vundefined_u16m4(), temp_2, n, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vextq_u32(uint32x4_t a, uint32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m4_t temp_0 = vlmul_ext_v_u32m2_u32m4(__builtin_rvv_vcast_from_fixed_64_u32m2(a));
	vuint32m4_t temp_1 = vlmul_ext_v_u32m2_u32m4(__builtin_rvv_vcast_from_fixed_64_u32m2(b));
	vuint32m4_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vlmul_trunc_v_u32m4_u32m2(vslidedown(vundefined_u32m4(), temp_2, n, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vextq_u64(uint64x2_t a, uint64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m4_t temp_0 = vlmul_ext_v_u64m2_u64m4(__builtin_rvv_vcast_from_fixed_64_u64m2(a));
	vuint64m4_t temp_1 = vlmul_ext_v_u64m2_u64m4(__builtin_rvv_vcast_from_fixed_64_u64m2(b));
	vuint64m4_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vlmul_trunc_v_u64m4_u64m2(vslidedown(vundefined_u64m4(), temp_2, n, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vextq_u8(uint8x16_t a, uint8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vlmul_ext_v_u8m2_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m2(a));
	vuint8m4_t temp_1 = vlmul_ext_v_u8m2_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m2(b));
	vuint8m4_t temp_2 = vslideup(temp_0, temp_1, 16, 32);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vlmul_trunc_v_u8m4_u8m2(vslidedown(vundefined_u8m4(), temp_2, n, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vext_s16(int16x4_t a, int16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = vlmul_ext_v_i16m1_i16m2(__builtin_rvv_vcast_from_fixed_64_i16m1(a));
	vint16m2_t temp_1 = vlmul_ext_v_i16m1_i16m2(__builtin_rvv_vcast_from_fixed_64_i16m1(b));
	vint16m2_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_2, n, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vext_s32(int32x2_t a, int32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = vlmul_ext_v_i32m1_i32m2(__builtin_rvv_vcast_from_fixed_64_i32m1(a));
	vint32m2_t temp_1 = vlmul_ext_v_i32m1_i32m2(__builtin_rvv_vcast_from_fixed_64_i32m1(b));
	vint32m2_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_2, n, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vext_s64(int64x1_t a, int64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = vlmul_ext_v_i64m1_i64m2(__builtin_rvv_vcast_from_fixed_64_i64m1(a));
	vint64m2_t temp_1 = vlmul_ext_v_i64m1_i64m2(__builtin_rvv_vcast_from_fixed_64_i64m1(b));
	vint64m2_t temp_2 = vslideup(temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vlmul_trunc_v_i64m2_i64m1(vslidedown(vundefined_i64m2(), temp_2, n, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vext_s8(int8x8_t a, int8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(a));
	vint8m2_t temp_1 = vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(b));
	vint8m2_t temp_2 = vslideup(temp_0, temp_1, 8, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_2, n, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vext_u16(uint16x4_t a, uint16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = vlmul_ext_v_u16m1_u16m2(__builtin_rvv_vcast_from_fixed_64_u16m1(a));
	vuint16m2_t temp_1 = vlmul_ext_v_u16m1_u16m2(__builtin_rvv_vcast_from_fixed_64_u16m1(b));
	vuint16m2_t temp_2 = vslideup(temp_0, temp_1, 4, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_2, n, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vext_u32(uint32x2_t a, uint32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = vlmul_ext_v_u32m1_u32m2(__builtin_rvv_vcast_from_fixed_64_u32m1(a));
	vuint32m2_t temp_1 = vlmul_ext_v_u32m1_u32m2(__builtin_rvv_vcast_from_fixed_64_u32m1(b));
	vuint32m2_t temp_2 = vslideup(temp_0, temp_1, 2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_2, n, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vext_u64(uint64x1_t a, uint64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = vlmul_ext_v_u64m1_u64m2(__builtin_rvv_vcast_from_fixed_64_u64m1(a));
	vuint64m2_t temp_1 = vlmul_ext_v_u64m1_u64m2(__builtin_rvv_vcast_from_fixed_64_u64m1(b));
	vuint64m2_t temp_2 = vslideup(temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vlmul_trunc_v_u64m2_u64m1(vslidedown(vundefined_u64m2(), temp_2, n, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vext_u8(uint8x8_t a, uint8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(a));
	vuint8m2_t temp_1 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(b));
	vuint8m2_t temp_2 = vslideup(temp_0, temp_1, 8, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_2, n, 8)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmad_lane_f64") float64_t vfmad_lane_f64(float64_t a, float64_t b, float64x1_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmad_laneq_f64") float64_t vfmad_laneq_f64(float64_t a, float64_t b, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vfma_f16(float16x4_t a, float16x4_t b, float16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vfma_f32(float32x2_t a, float32x2_t b, float32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vfma_f64(float64x1_t a, float64x1_t b, float64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmacc(temp_0, temp_1, temp_2, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmah_f16") float16_t vfmah_f16(float16_t a, float16_t b, float16_t c);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmah_lane_f16") float16_t vfmah_lane_f16(float16_t a, float16_t b, float16x4_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmah_laneq_f16") float16_t vfmah_laneq_f16(float16_t a, float16_t b, float16x8_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vfma_lane_f16(float16x4_t a, float16x4_t b, float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vfma_lane_f32(float32x2_t a, float32x2_t b, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vfma_lane_f64(float64x1_t a, float64x1_t b, float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 1), 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vfma_laneq_f16(float16x4_t a, float16x4_t b, float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2(v);
	vfloat16m1_t temp_3 = vlmul_trunc_v_f16m2_f16m1(vrgather(temp_2, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vfma_laneq_f32(float32x2_t a, float32x2_t b, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	vfloat32m1_t temp_3 = vlmul_trunc_v_f32m2_f32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vfma_laneq_f64(float64x1_t a, float64x1_t b, float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(v);
	vfloat64m1_t temp_3 = vlmul_trunc_v_f64m2_f64m1(vrgather(temp_2, lane, 1));
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmacc(temp_0, temp_1, temp_3, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vfma_n_f16(float16x4_t a, float16x4_t b, float16_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmacc(temp_0, n, temp_1, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vfma_n_f32(float32x2_t a, float32x2_t b, float32_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, n, temp_1, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vfma_n_f64(float64x1_t a, float64x1_t b, float64_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmacc(temp_0, n, temp_1, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vfmaq_f16(float16x8_t a, float16x8_t b, float16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vfmaq_f32(float32x4_t a, float32x4_t b, float32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vfmaq_f64(float64x2_t a, float64x2_t b, float64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vfmaq_lane_f16(float16x8_t a, float16x8_t b, float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1(v);
	vfloat16m2_t temp_3 = vrgather(vlmul_ext_v_f16m1_f16m2(temp_2), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmacc(temp_0, temp_1, temp_3, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vfmaq_lane_f32(float32x4_t a, float32x4_t b, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	vfloat32m2_t temp_3 = vrgather(vlmul_ext_v_f32m1_f32m2(temp_2), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vfmaq_lane_f64(float64x2_t a, float64x2_t b, float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(v);
	vfloat64m2_t temp_3 = vrgather(vlmul_ext_v_f64m1_f64m2(temp_2), lane, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vfmaq_laneq_f16(float16x8_t a, float16x8_t b, float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 8), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vfmaq_laneq_f32(float32x4_t a, float32x4_t b, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vfmaq_laneq_f64(float64x2_t a, float64x2_t b, float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vfmaq_n_f16(float16x8_t a, float16x8_t b, float16_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmacc(temp_0, n, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vfmaq_n_f32(float32x4_t a, float32x4_t b, float32_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, n, temp_1, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vfmaq_n_f64(float64x2_t a, float64x2_t b, float64_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmacc(temp_0, n, temp_1, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmas_lane_f32") float32_t vfmas_lane_f32(float32_t a, float32_t b, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmas_laneq_f32") float32_t vfmas_laneq_f32(float32_t a, float32_t b, float32x4_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_high_f16") float32x2_t vfmlal_high_f16(float32x2_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_lane_high_f16") float32x2_t vfmlal_lane_high_f16(float32x2_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_lane_low_f16") float32x2_t vfmlal_lane_low_f16(float32x2_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_laneq_high_f16") float32x2_t vfmlal_laneq_high_f16(float32x2_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_laneq_low_f16") float32x2_t vfmlal_laneq_low_f16(float32x2_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlal_low_f16") float32x2_t vfmlal_low_f16(float32x2_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_high_f16") float32x4_t vfmlalq_high_f16(float32x4_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_lane_high_f16") float32x4_t vfmlalq_lane_high_f16(float32x4_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_lane_low_f16") float32x4_t vfmlalq_lane_low_f16(float32x4_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_laneq_high_f16") float32x4_t vfmlalq_laneq_high_f16(float32x4_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_laneq_low_f16") float32x4_t vfmlalq_laneq_low_f16(float32x4_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlalq_low_f16") float32x4_t vfmlalq_low_f16(float32x4_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_high_f16") float32x2_t vfmlsl_high_f16(float32x2_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_lane_high_f16") float32x2_t vfmlsl_lane_high_f16(float32x2_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_lane_low_f16") float32x2_t vfmlsl_lane_low_f16(float32x2_t r, float16x4_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_laneq_high_f16") float32x2_t vfmlsl_laneq_high_f16(float32x2_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_laneq_low_f16") float32x2_t vfmlsl_laneq_low_f16(float32x2_t r, float16x4_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlsl_low_f16") float32x2_t vfmlsl_low_f16(float32x2_t r, float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_high_f16") float32x4_t vfmlslq_high_f16(float32x4_t r, float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_lane_high_f16") float32x4_t vfmlslq_lane_high_f16(float32x4_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_lane_low_f16") float32x4_t vfmlslq_lane_low_f16(float32x4_t r, float16x8_t a, float16x4_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_laneq_high_f16") float32x4_t vfmlslq_laneq_high_f16(float32x4_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_laneq_low_f16") float32x4_t vfmlslq_laneq_low_f16(float32x4_t r, float16x8_t a, float16x8_t b, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmlslq_low_f16") float32x4_t vfmlslq_low_f16(float32x4_t r, float16x8_t a, float16x8_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsd_lane_f64") float64_t vfmsd_lane_f64(float64_t a, float64_t b, float64x1_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsd_laneq_f64") float64_t vfmsd_laneq_f64(float64_t a, float64_t b, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vfms_f16(float16x4_t a, float16x4_t b, float16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vfms_f32(float32x2_t a, float32x2_t b, float32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vfms_f64(float64x1_t a, float64x1_t b, float64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfnmsac(temp_0, temp_1, temp_2, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsh_f16") float16_t vfmsh_f16(float16_t a, float16_t b, float16_t c);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsh_lane_f16") float16_t vfmsh_lane_f16(float16_t a, float16_t b, float16x4_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsh_laneq_f16") float16_t vfmsh_laneq_f16(float16_t a, float16_t b, float16x8_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfms_lane_f16") float16x4_t vfms_lane_f16(float16x4_t a, float16x4_t b, float16x4_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_lane_f32") float32x2_t vfms_lane_f32(float32x2_t a, float32x2_t b, float32x2_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_lane_f64") float64x1_t vfms_lane_f64(float64x1_t a, float64x1_t b, float64x1_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfms_laneq_f16") float16x4_t vfms_laneq_f16(float16x4_t a, float16x4_t b, float16x8_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_laneq_f32") float32x2_t vfms_laneq_f32(float32x2_t a, float32x2_t b, float32x4_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_laneq_f64") float64x1_t vfms_laneq_f64(float64x1_t a, float64x1_t b, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfms_n_f16") float16x4_t vfms_n_f16(float16x4_t a, float16x4_t b, float16_t n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_n_f32") float32x2_t vfms_n_f32(float32x2_t a, float32x2_t b, float32_t n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfms_n_f64") float64x1_t vfms_n_f64(float64x1_t a, float64x1_t b, float64_t n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vfmsq_f16(float16x8_t a, float16x8_t b, float16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfnmsac(temp_0, temp_1, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vfmsq_f32(float32x4_t a, float32x4_t b, float32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vfmsq_f64(float64x2_t a, float64x2_t b, float64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsq_lane_f16") float16x8_t vfmsq_lane_f16(float16x8_t a, float16x8_t b, float16x4_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_lane_f32") float32x4_t vfmsq_lane_f32(float32x4_t a, float32x4_t b, float32x2_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_lane_f64") float64x2_t vfmsq_lane_f64(float64x2_t a, float64x2_t b, float64x1_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsq_laneq_f16") float16x8_t vfmsq_laneq_f16(float16x8_t a, float16x8_t b, float16x8_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_laneq_f32") float32x4_t vfmsq_laneq_f32(float32x4_t a, float32x4_t b, float32x4_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_laneq_f64") float64x2_t vfmsq_laneq_f64(float64x2_t a, float64x2_t b, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vfmsq_n_f16") float16x8_t vfmsq_n_f16(float16x8_t a, float16x8_t b, float16_t n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_n_f32") float32x4_t vfmsq_n_f32(float32x4_t a, float32x4_t b, float32_t n);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmsq_n_f64") float64x2_t vfmsq_n_f64(float64x2_t a, float64x2_t b, float64_t n);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmss_lane_f32") float32_t vfmss_lane_f32(float32_t a, float32_t b, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vfmss_laneq_f32") float32_t vfmss_laneq_f32(float32_t a, float32_t b, float32x4_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vget_high_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vlmul_trunc_v_f16m2_f16m1(vslidedown(vundefined_f16m2(), temp_0, 4, 4)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vget_high_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vlmul_trunc_v_f32m2_f32m1(vslidedown(vundefined_f32m2(), temp_0, 2, 2)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vget_high_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vlmul_trunc_v_f64m2_f64m1(vslidedown(vundefined_f64m2(), temp_0, 1, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vget_high_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_0, 4, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vget_high_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_0, 2, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vget_high_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vlmul_trunc_v_i64m2_i64m1(vslidedown(vundefined_i64m2(), temp_0, 1, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vget_high_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_0, 8, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vget_high_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_0, 4, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vget_high_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_0, 2, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vget_high_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vlmul_trunc_v_u64m2_u64m1(vslidedown(vundefined_u64m2(), temp_0, 1, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vget_high_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_0, 8, 8)));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vget_lane_f16(float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vget_lane_f32(float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64_t vget_lane_f64(float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vget_lane_s16(int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vget_lane_s32(int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64_t vget_lane_s64(int64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vget_lane_s8(int8x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vget_lane_u16(uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vget_lane_u32(uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64_t vget_lane_u64(uint64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vget_lane_u8(uint8x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vget_low_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vlmul_trunc_v_f16m2_f16m1(vmv_v(temp_0, 4)));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vget_low_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vlmul_trunc_v_f32m2_f32m1(vmv_v(temp_0, 2)));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vget_low_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vlmul_trunc_v_f64m2_f64m1(vmv_v(temp_0, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vget_low_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vlmul_trunc_v_i16m2_i16m1(vmv_v(temp_0, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vget_low_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vlmul_trunc_v_i32m2_i32m1(vmv_v(temp_0, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vget_low_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vlmul_trunc_v_i64m2_i64m1(vmv_v(temp_0, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vget_low_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vmv_v(temp_0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vget_low_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vlmul_trunc_v_u16m2_u16m1(vmv_v(temp_0, 4)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vget_low_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vlmul_trunc_v_u32m2_u32m1(vmv_v(temp_0, 2)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vget_low_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vlmul_trunc_v_u64m2_u64m1(vmv_v(temp_0, 1)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vget_low_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vmv_v(temp_0, 8)));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vgetq_lane_f16(float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vgetq_lane_f32(float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64_t vgetq_lane_f64(float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vgetq_lane_s16(int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vgetq_lane_s32(int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64_t vgetq_lane_s64(int64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vgetq_lane_s8(int8x16_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vgetq_lane_u16(uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vgetq_lane_u32(uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64_t vgetq_lane_u64(uint64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vgetq_lane_u8(uint8x16_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	return v[lane];
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vhaddq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint16m2_t temp_4 = vaadd(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vhaddq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint32m2_t temp_4 = vaadd(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vhaddq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint8m2_t temp_4 = vaadd(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vhaddq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint16m2_t temp_4 = vaaddu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vhaddq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint32m2_t temp_4 = vaaddu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vhaddq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint8m2_t temp_4 = vaaddu(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vhadd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint16m1_t temp_4 = vaadd(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vhadd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint32m1_t temp_4 = vaadd(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vhadd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint8m1_t temp_4 = vaadd(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vhadd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vaaddu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vhadd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vaaddu(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vhadd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vaaddu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vhsubq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint16m2_t temp_4 = vasub(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vhsubq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint32m2_t temp_4 = vasub(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vhsubq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint8m2_t temp_4 = vasub(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vhsubq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint16m2_t temp_4 = vasubu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vhsubq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint32m2_t temp_4 = vasubu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vhsubq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint8m2_t temp_4 = vasubu(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vhsub_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint16m1_t temp_4 = vasub(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vhsub_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint32m1_t temp_4 = vasub(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vhsub_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vint8m1_t temp_4 = vasub(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vhsub_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vasubu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vhsub_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vasubu(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vhsub_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vasubu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vld1_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1((*ptr), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vld1_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1((*ptr), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vld1_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1((*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vld1_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1((*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vld1_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1((*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vld1_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1((*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vld1_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1((*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vld1_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1((*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vld1_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1((*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vld1_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1((*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vld1_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1((*ptr), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vld1_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1(ptr, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x2_t vld1_f16_x2(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x3_t vld1_f16_x3(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x3_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 8), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x4_t vld1_f16_x4(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x4_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((ptr + 12), 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vld1_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1(ptr, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x2_t vld1_f32_x2(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 2), 2))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x3_t vld1_f32_x3(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x3_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 4), 2))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x4_t vld1_f32_x4(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x4_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((ptr + 6), 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vld1_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1(ptr, 1));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x2_t vld1_f64_x2(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x2_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 1), 1))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x3_t vld1_f64_x3(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x3_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 2), 1))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x4_t vld1_f64_x4(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x4_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 2), 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vle64_v_f64m1((ptr + 3), 1))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vld1_lane_f16(const float16_t * ptr, float16x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(src);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmseq(vid_v_u16m1(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vld1_lane_f32(const float32_t * ptr, float32x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(src);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmseq(vid_v_u32m1(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vld1_lane_f64(const float64_t * ptr, float64x1_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(src);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmseq(vid_v_u64m1(1), lane, 1), temp_0, (*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vld1_lane_s16(const int16_t * ptr, int16x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(src);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmseq(vid_v_u16m1(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vld1_lane_s32(const int32_t * ptr, int32x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(src);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmseq(vid_v_u32m1(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vld1_lane_s64(const int64_t * ptr, int64x1_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(src);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmseq(vid_v_u64m1(1), lane, 1), temp_0, (*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vld1_lane_s8(const int8_t * ptr, int8x8_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(src);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmseq(vid_v_u8m1(8), lane, 8), temp_0, (*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vld1_lane_u16(const uint16_t * ptr, uint16x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(src);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmseq(vid_v_u16m1(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vld1_lane_u32(const uint32_t * ptr, uint32x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(src);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmseq(vid_v_u32m1(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vld1_lane_u64(const uint64_t * ptr, uint64x1_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(src);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmseq(vid_v_u64m1(1), lane, 1), temp_0, (*ptr), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vld1_lane_u8(const uint8_t * ptr, uint8x8_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(src);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmseq(vid_v_u8m1(8), lane, 8), temp_0, (*ptr), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vld1q_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2((*ptr), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vld1q_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2((*ptr), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vld1q_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2((*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vld1q_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2((*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vld1q_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2((*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vld1q_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2((*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vld1q_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2((*ptr), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vld1q_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2((*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vld1q_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2((*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vld1q_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2((*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vld1q_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2((*ptr), 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vld1q_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2(ptr, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x2_t vld1q_f16_x2(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 8), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x3_t vld1q_f16_x3(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x3_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 16), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x4_t vld1q_f16_x4(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x4_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 16), 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((ptr + 24), 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vld1q_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2(ptr, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x2_t vld1q_f32_x2(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 4), 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x3_t vld1q_f32_x3(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x3_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 8), 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x4_t vld1q_f32_x4(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x4_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((ptr + 12), 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vld1q_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2(ptr, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x2_t vld1q_f64_x2(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x2_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 2), 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x3_t vld1q_f64_x3(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x3_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 4), 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x4_t vld1q_f64_x4(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x4_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((ptr + 6), 2))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vld1q_lane_f16(const float16_t * ptr, float16x8_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(src);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmseq(vid_v_u16m2(8), lane, 8), temp_0, (*ptr), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vld1q_lane_f32(const float32_t * ptr, float32x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(src);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmseq(vid_v_u32m2(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vld1q_lane_f64(const float64_t * ptr, float64x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(src);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmseq(vid_v_u64m2(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vld1q_lane_s16(const int16_t * ptr, int16x8_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(src);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmseq(vid_v_u16m2(8), lane, 8), temp_0, (*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vld1q_lane_s32(const int32_t * ptr, int32x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(src);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmseq(vid_v_u32m2(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vld1q_lane_s64(const int64_t * ptr, int64x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(src);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmseq(vid_v_u64m2(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vld1q_lane_s8(const int8_t * ptr, int8x16_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(src);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmseq(vid_v_u8m2(16), lane, 16), temp_0, (*ptr), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vld1q_lane_u16(const uint16_t * ptr, uint16x8_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(src);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmseq(vid_v_u16m2(8), lane, 8), temp_0, (*ptr), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vld1q_lane_u32(const uint32_t * ptr, uint32x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(src);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmseq(vid_v_u32m2(4), lane, 4), temp_0, (*ptr), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vld1q_lane_u64(const uint64_t * ptr, uint64x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(src);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmseq(vid_v_u64m2(2), lane, 2), temp_0, (*ptr), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vld1q_lane_u8(const uint8_t * ptr, uint8x16_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(src);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmseq(vid_v_u8m2(16), lane, 16), temp_0, (*ptr), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vld1q_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2(ptr, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x2_t vld1q_s16_x2(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 8), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x3_t vld1q_s16_x3(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x3_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 16), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x4_t vld1q_s16_x4(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x4_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 16), 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((ptr + 24), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vld1q_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2(ptr, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x2_t vld1q_s32_x2(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 4), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x3_t vld1q_s32_x3(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x3_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 8), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x4_t vld1q_s32_x4(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x4_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((ptr + 12), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vld1q_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2(ptr, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x2_t vld1q_s64_x2(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x2_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 2), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x3_t vld1q_s64_x3(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x3_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 4), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x4_t vld1q_s64_x4(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x4_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((ptr + 6), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vld1q_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2(ptr, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x2_t vld1q_s8_x2(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 16), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x3_t vld1q_s8_x3(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x3_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 16), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 32), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x4_t vld1q_s8_x4(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x4_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 16), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 32), 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((ptr + 48), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vld1q_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2(ptr, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x2_t vld1q_u16_x2(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 8), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x3_t vld1q_u16_x3(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x3_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 16), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x4_t vld1q_u16_x4(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x4_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 16), 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((ptr + 24), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vld1q_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2(ptr, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x2_t vld1q_u32_x2(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 4), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x3_t vld1q_u32_x3(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x3_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 8), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x4_t vld1q_u32_x4(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x4_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((ptr + 12), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vld1q_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2(ptr, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x2_t vld1q_u64_x2(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x2_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 2), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x3_t vld1q_u64_x3(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x3_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 4), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x4_t vld1q_u64_x4(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x4_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((ptr + 6), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vld1q_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2(ptr, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x2_t vld1q_u8_x2(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 16), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x3_t vld1q_u8_x3(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x3_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 16), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 32), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x4_t vld1q_u8_x4(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x4_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 0), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 16), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 32), 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((ptr + 48), 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vld1_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1(ptr, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x2_t vld1_s16_x2(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 4), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x3_t vld1_s16_x3(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x3_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 8), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x4_t vld1_s16_x4(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x4_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((ptr + 12), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vld1_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1(ptr, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x2_t vld1_s32_x2(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 2), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x3_t vld1_s32_x3(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x3_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 4), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x4_t vld1_s32_x4(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x4_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((ptr + 6), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vld1_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1(ptr, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x2_t vld1_s64_x2(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x2_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 1), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x3_t vld1_s64_x3(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x3_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 2), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x4_t vld1_s64_x4(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x4_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 2), 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vle64_v_i64m1((ptr + 3), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vld1_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1(ptr, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x2_t vld1_s8_x2(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 8), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x3_t vld1_s8_x3(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x3_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 16), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x4_t vld1_s8_x4(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x4_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 16), 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((ptr + 24), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vld1_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1(ptr, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x2_t vld1_u16_x2(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 4), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x3_t vld1_u16_x3(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x3_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 8), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x4_t vld1_u16_x4(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x4_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 0), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 4), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 8), 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((ptr + 12), 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vld1_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1(ptr, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x2_t vld1_u32_x2(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 2), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x3_t vld1_u32_x3(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x3_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 4), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x4_t vld1_u32_x4(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x4_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 0), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 2), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 4), 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((ptr + 6), 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vld1_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1(ptr, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x2_t vld1_u64_x2(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x2_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 1), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x3_t vld1_u64_x3(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x3_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 2), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x4_t vld1_u64_x4(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x4_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 0), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 1), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 2), 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vle64_v_u64m1((ptr + 3), 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vld1_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1(ptr, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x2_t vld1_u8_x2(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 8), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x3_t vld1_u8_x3(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x3_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 16), 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x4_t vld1_u8_x4(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x4_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 0), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 8), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 16), 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((ptr + 24), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x2_t vld2_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[1], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x2_t vld2_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[1], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x2_t vld2_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x2_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x2_t vld2_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x2_t vld2_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x2_t vld2_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x2_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x2_t vld2_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x2_t vld2_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x2_t vld2_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x2_t vld2_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x2_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x2_t vld2_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[1], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4x2_t vld2_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vlseg2e16_v_f16m1((&temp_0), (&temp_1), ptr, 4);
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_1)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2x2_t vld2_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vlseg2e32_v_f32m1((&temp_0), (&temp_1), ptr, 2);
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_1)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x1x2_t vld2_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vlseg2e64_v_f64m1((&temp_0), (&temp_1), ptr, 1);
	return ((float64x1x2_t){__builtin_rvv_vcast_to_fixed_64_f64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x2_t vld2_lane_f16(const float16_t * ptr, float16x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x2_t vld2_lane_f32(const float32_t * ptr, float32x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x2_t vld2_lane_f64(const float64_t * ptr, float64x1x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[1]);
	vbool64_t temp_2 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((float64x1x2_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_2, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_2, temp_1, ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x2_t vld2_lane_s16(const int16_t * ptr, int16x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x2_t vld2_lane_s32(const int32_t * ptr, int32x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x2_t vld2_lane_s64(const int64_t * ptr, int64x1x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[1]);
	vbool64_t temp_2 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((int64x1x2_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_2, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_2, temp_1, ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x2_t vld2_lane_s8(const int8_t * ptr, int8x8x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[1]);
	vbool8_t temp_2 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_2, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_2, temp_1, ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x2_t vld2_lane_u16(const uint16_t * ptr, uint16x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x2_t vld2_lane_u32(const uint32_t * ptr, uint32x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x2_t vld2_lane_u64(const uint64_t * ptr, uint64x1x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[1]);
	vbool64_t temp_2 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((uint64x1x2_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_2, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_2, temp_1, ptr[1], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x2_t vld2_lane_u8(const uint8_t * ptr, uint8x8x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[1]);
	vbool8_t temp_2 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_2, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_2, temp_1, ptr[1], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x2_t vld2q_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[1], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x2_t vld2q_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[1], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x2_t vld2q_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x2_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x2_t vld2q_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x2_t vld2q_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x2_t vld2q_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x2_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x2_t vld2q_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[1], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x2_t vld2q_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x2_t vld2q_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x2_t vld2q_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x2_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x2_t vld2q_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[1], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8x2_t vld2q_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0;
	vfloat16m2_t temp_1;
	vlseg2e16_v_f16m2((&temp_0), (&temp_1), ptr, 8);
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_1)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4x2_t vld2q_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0;
	vfloat32m2_t temp_1;
	vlseg2e32_v_f32m2((&temp_0), (&temp_1), ptr, 4);
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_1)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2x2_t vld2q_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0;
	vfloat64m2_t temp_1;
	vlseg2e64_v_f64m2((&temp_0), (&temp_1), ptr, 2);
	return ((float64x2x2_t){__builtin_rvv_vcast_to_fixed_64_f64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x2_t vld2q_lane_f16(const float16_t * ptr, float16x8x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[1]);
	vbool8_t temp_2 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_2, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_2, temp_1, ptr[1], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x2_t vld2q_lane_f32(const float32_t * ptr, float32x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x2_t vld2q_lane_f64(const float64_t * ptr, float64x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((float64x2x2_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x2_t vld2q_lane_s16(const int16_t * ptr, int16x8x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[1]);
	vbool8_t temp_2 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_2, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_2, temp_1, ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x2_t vld2q_lane_s32(const int32_t * ptr, int32x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x2_t vld2q_lane_s64(const int64_t * ptr, int64x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((int64x2x2_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x2_t vld2q_lane_s8(const int8_t * ptr, int8x16x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[1]);
	vbool4_t temp_2 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_2, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_2, temp_1, ptr[1], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x2_t vld2q_lane_u16(const uint16_t * ptr, uint16x8x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[1]);
	vbool8_t temp_2 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_2, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_2, temp_1, ptr[1], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x2_t vld2q_lane_u32(const uint32_t * ptr, uint32x4x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[1]);
	vbool16_t temp_2 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_2, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_2, temp_1, ptr[1], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x2_t vld2q_lane_u64(const uint64_t * ptr, uint64x2x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[1]);
	vbool32_t temp_2 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((uint64x2x2_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_2, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_2, temp_1, ptr[1], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x2_t vld2q_lane_u8(const uint8_t * ptr, uint8x16x2_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[1]);
	vbool4_t temp_2 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_2, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_2, temp_1, ptr[1], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8x2_t vld2q_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0;
	vint16m2_t temp_1;
	vlseg2e16_v_i16m2((&temp_0), (&temp_1), ptr, 8);
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4x2_t vld2q_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0;
	vint32m2_t temp_1;
	vlseg2e32_v_i32m2((&temp_0), (&temp_1), ptr, 4);
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2x2_t vld2q_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0;
	vint64m2_t temp_1;
	vlseg2e64_v_i64m2((&temp_0), (&temp_1), ptr, 2);
	return ((int64x2x2_t){__builtin_rvv_vcast_to_fixed_64_i64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16x2_t vld2q_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0;
	vint8m2_t temp_1;
	vlseg2e8_v_i8m2((&temp_0), (&temp_1), ptr, 16);
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8x2_t vld2q_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0;
	vuint16m2_t temp_1;
	vlseg2e16_v_u16m2((&temp_0), (&temp_1), ptr, 8);
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4x2_t vld2q_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0;
	vuint32m2_t temp_1;
	vlseg2e32_v_u32m2((&temp_0), (&temp_1), ptr, 4);
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2x2_t vld2q_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0;
	vuint64m2_t temp_1;
	vlseg2e64_v_u64m2((&temp_0), (&temp_1), ptr, 2);
	return ((uint64x2x2_t){__builtin_rvv_vcast_to_fixed_64_u64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16x2_t vld2q_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0;
	vuint8m2_t temp_1;
	vlseg2e8_v_u8m2((&temp_0), (&temp_1), ptr, 16);
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4x2_t vld2_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vlseg2e16_v_i16m1((&temp_0), (&temp_1), ptr, 4);
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2x2_t vld2_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vlseg2e32_v_i32m1((&temp_0), (&temp_1), ptr, 2);
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x1x2_t vld2_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0;
	vint64m1_t temp_1;
	vlseg2e64_v_i64m1((&temp_0), (&temp_1), ptr, 1);
	return ((int64x1x2_t){__builtin_rvv_vcast_to_fixed_64_i64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8x2_t vld2_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vlseg2e8_v_i8m1((&temp_0), (&temp_1), ptr, 8);
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4x2_t vld2_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vlseg2e16_v_u16m1((&temp_0), (&temp_1), ptr, 4);
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2x2_t vld2_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vlseg2e32_v_u32m1((&temp_0), (&temp_1), ptr, 2);
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x1x2_t vld2_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0;
	vuint64m1_t temp_1;
	vlseg2e64_v_u64m1((&temp_0), (&temp_1), ptr, 1);
	return ((uint64x1x2_t){__builtin_rvv_vcast_to_fixed_64_u64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8x2_t vld2_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vlseg2e8_v_u8m1((&temp_0), (&temp_1), ptr, 8);
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x3_t vld3_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x3_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[2], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x3_t vld3_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x3_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[2], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x3_t vld3_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x3_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x3_t vld3_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x3_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x3_t vld3_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x3_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x3_t vld3_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x3_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x3_t vld3_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x3_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x3_t vld3_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x3_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x3_t vld3_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x3_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x3_t vld3_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x3_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x3_t vld3_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x3_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[2], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4x3_t vld3_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vfloat16m1_t temp_2;
	vlseg3e16_v_f16m1((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((float16x4x3_t){__builtin_rvv_vcast_to_fixed_64_f16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_2)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2x3_t vld3_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vfloat32m1_t temp_2;
	vlseg3e32_v_f32m1((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((float32x2x3_t){__builtin_rvv_vcast_to_fixed_64_f32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_2)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x1x3_t vld3_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vfloat64m1_t temp_2;
	vlseg3e64_v_f64m1((&temp_0), (&temp_1), (&temp_2), ptr, 1);
	return ((float64x1x3_t){__builtin_rvv_vcast_to_fixed_64_f64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x3_t vld3_lane_f16(const float16_t * ptr, float16x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((float16x4x3_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x3_t vld3_lane_f32(const float32_t * ptr, float32x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((float32x2x3_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x3_t vld3_lane_f64(const float64_t * ptr, float64x1x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[2]);
	vbool64_t temp_3 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((float64x1x3_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_3, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_3, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_3, temp_2, ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x3_t vld3_lane_s16(const int16_t * ptr, int16x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((int16x4x3_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x3_t vld3_lane_s32(const int32_t * ptr, int32x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((int32x2x3_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x3_t vld3_lane_s64(const int64_t * ptr, int64x1x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[2]);
	vbool64_t temp_3 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((int64x1x3_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_3, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_3, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_3, temp_2, ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x3_t vld3_lane_s8(const int8_t * ptr, int8x8x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[2]);
	vbool8_t temp_3 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((int8x8x3_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_3, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_3, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_3, temp_2, ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x3_t vld3_lane_u16(const uint16_t * ptr, uint16x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((uint16x4x3_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x3_t vld3_lane_u32(const uint32_t * ptr, uint32x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((uint32x2x3_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x3_t vld3_lane_u64(const uint64_t * ptr, uint64x1x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[2]);
	vbool64_t temp_3 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((uint64x1x3_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_3, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_3, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_3, temp_2, ptr[2], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x3_t vld3_lane_u8(const uint8_t * ptr, uint8x8x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[2]);
	vbool8_t temp_3 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((uint8x8x3_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_3, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_3, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_3, temp_2, ptr[2], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x3_t vld3q_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x3_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[2], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x3_t vld3q_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x3_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[2], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x3_t vld3q_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x3_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x3_t vld3q_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x3_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x3_t vld3q_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x3_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x3_t vld3q_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x3_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x3_t vld3q_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x3_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[2], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x3_t vld3q_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x3_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x3_t vld3q_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x3_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x3_t vld3q_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x3_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x3_t vld3q_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x3_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[2], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8x3_t vld3q_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0;
	vfloat16m2_t temp_1;
	vfloat16m2_t temp_2;
	vlseg3e16_v_f16m2((&temp_0), (&temp_1), (&temp_2), ptr, 8);
	return ((float16x8x3_t){__builtin_rvv_vcast_to_fixed_64_f16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_2)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4x3_t vld3q_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0;
	vfloat32m2_t temp_1;
	vfloat32m2_t temp_2;
	vlseg3e32_v_f32m2((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((float32x4x3_t){__builtin_rvv_vcast_to_fixed_64_f32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_2)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2x3_t vld3q_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0;
	vfloat64m2_t temp_1;
	vfloat64m2_t temp_2;
	vlseg3e64_v_f64m2((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((float64x2x3_t){__builtin_rvv_vcast_to_fixed_64_f64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x3_t vld3q_lane_f16(const float16_t * ptr, float16x8x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[2]);
	vbool8_t temp_3 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((float16x8x3_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_3, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_3, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_3, temp_2, ptr[2], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x3_t vld3q_lane_f32(const float32_t * ptr, float32x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((float32x4x3_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x3_t vld3q_lane_f64(const float64_t * ptr, float64x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((float64x2x3_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x3_t vld3q_lane_s16(const int16_t * ptr, int16x8x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[2]);
	vbool8_t temp_3 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((int16x8x3_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_3, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_3, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_3, temp_2, ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x3_t vld3q_lane_s32(const int32_t * ptr, int32x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((int32x4x3_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x3_t vld3q_lane_s64(const int64_t * ptr, int64x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((int64x2x3_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x3_t vld3q_lane_s8(const int8_t * ptr, int8x16x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[2]);
	vbool4_t temp_3 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((int8x16x3_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_3, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_3, temp_1, ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_3, temp_2, ptr[2], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x3_t vld3q_lane_u16(const uint16_t * ptr, uint16x8x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[2]);
	vbool8_t temp_3 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((uint16x8x3_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_3, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_3, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_3, temp_2, ptr[2], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x3_t vld3q_lane_u32(const uint32_t * ptr, uint32x4x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[2]);
	vbool16_t temp_3 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((uint32x4x3_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_3, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_3, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_3, temp_2, ptr[2], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x3_t vld3q_lane_u64(const uint64_t * ptr, uint64x2x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[2]);
	vbool32_t temp_3 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((uint64x2x3_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_3, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_3, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_3, temp_2, ptr[2], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x3_t vld3q_lane_u8(const uint8_t * ptr, uint8x16x3_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[2]);
	vbool4_t temp_3 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((uint8x16x3_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_3, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_3, temp_1, ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_3, temp_2, ptr[2], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8x3_t vld3q_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0;
	vint16m2_t temp_1;
	vint16m2_t temp_2;
	vlseg3e16_v_i16m2((&temp_0), (&temp_1), (&temp_2), ptr, 8);
	return ((int16x8x3_t){__builtin_rvv_vcast_to_fixed_64_i16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4x3_t vld3q_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0;
	vint32m2_t temp_1;
	vint32m2_t temp_2;
	vlseg3e32_v_i32m2((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((int32x4x3_t){__builtin_rvv_vcast_to_fixed_64_i32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2x3_t vld3q_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0;
	vint64m2_t temp_1;
	vint64m2_t temp_2;
	vlseg3e64_v_i64m2((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((int64x2x3_t){__builtin_rvv_vcast_to_fixed_64_i64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16x3_t vld3q_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0;
	vint8m2_t temp_1;
	vint8m2_t temp_2;
	vlseg3e8_v_i8m2((&temp_0), (&temp_1), (&temp_2), ptr, 16);
	return ((int8x16x3_t){__builtin_rvv_vcast_to_fixed_64_i8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8x3_t vld3q_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0;
	vuint16m2_t temp_1;
	vuint16m2_t temp_2;
	vlseg3e16_v_u16m2((&temp_0), (&temp_1), (&temp_2), ptr, 8);
	return ((uint16x8x3_t){__builtin_rvv_vcast_to_fixed_64_u16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4x3_t vld3q_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0;
	vuint32m2_t temp_1;
	vuint32m2_t temp_2;
	vlseg3e32_v_u32m2((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((uint32x4x3_t){__builtin_rvv_vcast_to_fixed_64_u32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2x3_t vld3q_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0;
	vuint64m2_t temp_1;
	vuint64m2_t temp_2;
	vlseg3e64_v_u64m2((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((uint64x2x3_t){__builtin_rvv_vcast_to_fixed_64_u64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16x3_t vld3q_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0;
	vuint8m2_t temp_1;
	vuint8m2_t temp_2;
	vlseg3e8_v_u8m2((&temp_0), (&temp_1), (&temp_2), ptr, 16);
	return ((uint8x16x3_t){__builtin_rvv_vcast_to_fixed_64_u8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4x3_t vld3_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vint16m1_t temp_2;
	vlseg3e16_v_i16m1((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((int16x4x3_t){__builtin_rvv_vcast_to_fixed_64_i16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2x3_t vld3_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vint32m1_t temp_2;
	vlseg3e32_v_i32m1((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((int32x2x3_t){__builtin_rvv_vcast_to_fixed_64_i32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x1x3_t vld3_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0;
	vint64m1_t temp_1;
	vint64m1_t temp_2;
	vlseg3e64_v_i64m1((&temp_0), (&temp_1), (&temp_2), ptr, 1);
	return ((int64x1x3_t){__builtin_rvv_vcast_to_fixed_64_i64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8x3_t vld3_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vint8m1_t temp_2;
	vlseg3e8_v_i8m1((&temp_0), (&temp_1), (&temp_2), ptr, 8);
	return ((int8x8x3_t){__builtin_rvv_vcast_to_fixed_64_i8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4x3_t vld3_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vuint16m1_t temp_2;
	vlseg3e16_v_u16m1((&temp_0), (&temp_1), (&temp_2), ptr, 4);
	return ((uint16x4x3_t){__builtin_rvv_vcast_to_fixed_64_u16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2x3_t vld3_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vuint32m1_t temp_2;
	vlseg3e32_v_u32m1((&temp_0), (&temp_1), (&temp_2), ptr, 2);
	return ((uint32x2x3_t){__builtin_rvv_vcast_to_fixed_64_u32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x1x3_t vld3_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0;
	vuint64m1_t temp_1;
	vuint64m1_t temp_2;
	vlseg3e64_v_u64m1((&temp_0), (&temp_1), (&temp_2), ptr, 1);
	return ((uint64x1x3_t){__builtin_rvv_vcast_to_fixed_64_u64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8x3_t vld3_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vuint8m1_t temp_2;
	vlseg3e8_v_u8m1((&temp_0), (&temp_1), (&temp_2), ptr, 8);
	return ((uint8x8x3_t){__builtin_rvv_vcast_to_fixed_64_u8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_2)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x4_t vld4_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4x4_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(ptr[3], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x4_t vld4_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2x4_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(ptr[3], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x4_t vld4_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1x4_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x4_t vld4_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4x4_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x4_t vld4_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2x4_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x4_t vld4_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1x4_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x4_t vld4_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8x4_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x4_t vld4_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4x4_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x4_t vld4_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2x4_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x4_t vld4_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1x4_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x4_t vld4_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8x4_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(ptr[3], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4x4_t vld4_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vfloat16m1_t temp_2;
	vfloat16m1_t temp_3;
	vlseg4e16_v_f16m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((float16x4x4_t){__builtin_rvv_vcast_to_fixed_64_f16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_2), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_3)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2x4_t vld4_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vfloat32m1_t temp_2;
	vfloat32m1_t temp_3;
	vlseg4e32_v_f32m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((float32x2x4_t){__builtin_rvv_vcast_to_fixed_64_f32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_2), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_3)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x1x4_t vld4_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vfloat64m1_t temp_2;
	vfloat64m1_t temp_3;
	vlseg4e64_v_f64m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 1);
	return ((float64x1x4_t){__builtin_rvv_vcast_to_fixed_64_f64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_2), __builtin_rvv_vcast_to_fixed_64_f64m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x4_t vld4_lane_f16(const float16_t * ptr, float16x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[2]);
	vfloat16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m1((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((float16x4x4_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x4_t vld4_lane_f32(const float32_t * ptr, float32x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[2]);
	vfloat32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m1((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((float32x2x4_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1x4_t vld4_lane_f64(const float64_t * ptr, float64x1x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[2]);
	vfloat64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m1((src.val)[3]);
	vbool64_t temp_4 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((float64x1x4_t){__builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_4, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_4, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_4, temp_2, ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(temp_4, temp_3, ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x4_t vld4_lane_s16(const int16_t * ptr, int16x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[2]);
	vint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m1((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((int16x4x4_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x4_t vld4_lane_s32(const int32_t * ptr, int32x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[2]);
	vint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m1((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((int32x2x4_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1x4_t vld4_lane_s64(const int64_t * ptr, int64x1x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[2]);
	vint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m1((src.val)[3]);
	vbool64_t temp_4 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((int64x1x4_t){__builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_4, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_4, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_4, temp_2, ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(temp_4, temp_3, ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x4_t vld4_lane_s8(const int8_t * ptr, int8x8x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[2]);
	vint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m1((src.val)[3]);
	vbool8_t temp_4 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((int8x8x4_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_4, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_4, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_4, temp_2, ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(temp_4, temp_3, ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x4_t vld4_lane_u16(const uint16_t * ptr, uint16x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[2]);
	vuint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m1((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u16m1(4), lane, 4);
	return ((uint16x4x4_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x4_t vld4_lane_u32(const uint32_t * ptr, uint32x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[2]);
	vuint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m1((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u32m1(2), lane, 2);
	return ((uint32x2x4_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1x4_t vld4_lane_u64(const uint64_t * ptr, uint64x1x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[2]);
	vuint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m1((src.val)[3]);
	vbool64_t temp_4 = vmseq(vid_v_u64m1(1), lane, 1);
	return ((uint64x1x4_t){__builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_4, temp_0, ptr[0], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_4, temp_1, ptr[1], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_4, temp_2, ptr[2], 1)), __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_4, temp_3, ptr[3], 1))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x4_t vld4_lane_u8(const uint8_t * ptr, uint8x8x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[2]);
	vuint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m1((src.val)[3]);
	vbool8_t temp_4 = vmseq(vid_v_u8m1(8), lane, 8);
	return ((uint8x8x4_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_4, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_4, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_4, temp_2, ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(temp_4, temp_3, ptr[3], 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x4_t vld4q_dup_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8x4_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(ptr[3], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x4_t vld4q_dup_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4x4_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(ptr[3], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x4_t vld4q_dup_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2x4_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x4_t vld4q_dup_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8x4_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x4_t vld4q_dup_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4x4_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x4_t vld4q_dup_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2x4_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x4_t vld4q_dup_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16x4_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[2], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(ptr[3], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x4_t vld4q_dup_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8x4_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x4_t vld4q_dup_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4x4_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x4_t vld4q_dup_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2x4_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x4_t vld4q_dup_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16x4_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[2], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(ptr[3], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8x4_t vld4q_f16(const float16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0;
	vfloat16m2_t temp_1;
	vfloat16m2_t temp_2;
	vfloat16m2_t temp_3;
	vlseg4e16_v_f16m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 8);
	return ((float16x8x4_t){__builtin_rvv_vcast_to_fixed_64_f16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_2), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_3)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4x4_t vld4q_f32(const float32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0;
	vfloat32m2_t temp_1;
	vfloat32m2_t temp_2;
	vfloat32m2_t temp_3;
	vlseg4e32_v_f32m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((float32x4x4_t){__builtin_rvv_vcast_to_fixed_64_f32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_2), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_3)});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2x4_t vld4q_f64(const float64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0;
	vfloat64m2_t temp_1;
	vfloat64m2_t temp_2;
	vfloat64m2_t temp_3;
	vlseg4e64_v_f64m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((float64x2x4_t){__builtin_rvv_vcast_to_fixed_64_f64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_2), __builtin_rvv_vcast_to_fixed_64_f64m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x4_t vld4q_lane_f16(const float16_t * ptr, float16x8x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[2]);
	vfloat16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m2((src.val)[3]);
	vbool8_t temp_4 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((float16x8x4_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_4, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_4, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_4, temp_2, ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(temp_4, temp_3, ptr[3], 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x4_t vld4q_lane_f32(const float32_t * ptr, float32x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[2]);
	vfloat32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m2((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((float32x4x4_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2x4_t vld4q_lane_f64(const float64_t * ptr, float64x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[2]);
	vfloat64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m2((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((float64x2x4_t){__builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x4_t vld4q_lane_s16(const int16_t * ptr, int16x8x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[2]);
	vint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m2((src.val)[3]);
	vbool8_t temp_4 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((int16x8x4_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_4, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_4, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_4, temp_2, ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(temp_4, temp_3, ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x4_t vld4q_lane_s32(const int32_t * ptr, int32x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[2]);
	vint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m2((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((int32x4x4_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2x4_t vld4q_lane_s64(const int64_t * ptr, int64x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[2]);
	vint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m2((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((int64x2x4_t){__builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x4_t vld4q_lane_s8(const int8_t * ptr, int8x16x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[2]);
	vint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m2((src.val)[3]);
	vbool4_t temp_4 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((int8x16x4_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_4, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_4, temp_1, ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_4, temp_2, ptr[2], 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(temp_4, temp_3, ptr[3], 16))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x4_t vld4q_lane_u16(const uint16_t * ptr, uint16x8x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[2]);
	vuint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m2((src.val)[3]);
	vbool8_t temp_4 = vmseq(vid_v_u16m2(8), lane, 8);
	return ((uint16x8x4_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_4, temp_0, ptr[0], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_4, temp_1, ptr[1], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_4, temp_2, ptr[2], 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(temp_4, temp_3, ptr[3], 8))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x4_t vld4q_lane_u32(const uint32_t * ptr, uint32x4x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[2]);
	vuint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m2((src.val)[3]);
	vbool16_t temp_4 = vmseq(vid_v_u32m2(4), lane, 4);
	return ((uint32x4x4_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_4, temp_0, ptr[0], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_4, temp_1, ptr[1], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_4, temp_2, ptr[2], 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(temp_4, temp_3, ptr[3], 4))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2x4_t vld4q_lane_u64(const uint64_t * ptr, uint64x2x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[2]);
	vuint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m2((src.val)[3]);
	vbool32_t temp_4 = vmseq(vid_v_u64m2(2), lane, 2);
	return ((uint64x2x4_t){__builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_4, temp_0, ptr[0], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_4, temp_1, ptr[1], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_4, temp_2, ptr[2], 2)), __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_4, temp_3, ptr[3], 2))});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x4_t vld4q_lane_u8(const uint8_t * ptr, uint8x16x4_t src, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[2]);
	vuint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m2((src.val)[3]);
	vbool4_t temp_4 = vmseq(vid_v_u8m2(16), lane, 16);
	return ((uint8x16x4_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_4, temp_0, ptr[0], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_4, temp_1, ptr[1], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_4, temp_2, ptr[2], 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(temp_4, temp_3, ptr[3], 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8x4_t vld4q_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0;
	vint16m2_t temp_1;
	vint16m2_t temp_2;
	vint16m2_t temp_3;
	vlseg4e16_v_i16m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 8);
	return ((int16x8x4_t){__builtin_rvv_vcast_to_fixed_64_i16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_2), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4x4_t vld4q_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0;
	vint32m2_t temp_1;
	vint32m2_t temp_2;
	vint32m2_t temp_3;
	vlseg4e32_v_i32m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((int32x4x4_t){__builtin_rvv_vcast_to_fixed_64_i32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_2), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2x4_t vld4q_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0;
	vint64m2_t temp_1;
	vint64m2_t temp_2;
	vint64m2_t temp_3;
	vlseg4e64_v_i64m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((int64x2x4_t){__builtin_rvv_vcast_to_fixed_64_i64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_2), __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16x4_t vld4q_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0;
	vint8m2_t temp_1;
	vint8m2_t temp_2;
	vint8m2_t temp_3;
	vlseg4e8_v_i8m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 16);
	return ((int8x16x4_t){__builtin_rvv_vcast_to_fixed_64_i8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_1), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_2), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8x4_t vld4q_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0;
	vuint16m2_t temp_1;
	vuint16m2_t temp_2;
	vuint16m2_t temp_3;
	vlseg4e16_v_u16m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 8);
	return ((uint16x8x4_t){__builtin_rvv_vcast_to_fixed_64_u16m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_2), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4x4_t vld4q_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0;
	vuint32m2_t temp_1;
	vuint32m2_t temp_2;
	vuint32m2_t temp_3;
	vlseg4e32_v_u32m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((uint32x4x4_t){__builtin_rvv_vcast_to_fixed_64_u32m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_2), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2x4_t vld4q_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0;
	vuint64m2_t temp_1;
	vuint64m2_t temp_2;
	vuint64m2_t temp_3;
	vlseg4e64_v_u64m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((uint64x2x4_t){__builtin_rvv_vcast_to_fixed_64_u64m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_2), __builtin_rvv_vcast_to_fixed_64_u64m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16x4_t vld4q_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0;
	vuint8m2_t temp_1;
	vuint8m2_t temp_2;
	vuint8m2_t temp_3;
	vlseg4e8_v_u8m2((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 16);
	return ((uint8x16x4_t){__builtin_rvv_vcast_to_fixed_64_u8m2(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_2), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4x4_t vld4_s16(const int16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vint16m1_t temp_2;
	vint16m1_t temp_3;
	vlseg4e16_v_i16m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((int16x4x4_t){__builtin_rvv_vcast_to_fixed_64_i16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_2), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2x4_t vld4_s32(const int32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vint32m1_t temp_2;
	vint32m1_t temp_3;
	vlseg4e32_v_i32m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((int32x2x4_t){__builtin_rvv_vcast_to_fixed_64_i32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_2), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x1x4_t vld4_s64(const int64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0;
	vint64m1_t temp_1;
	vint64m1_t temp_2;
	vint64m1_t temp_3;
	vlseg4e64_v_i64m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 1);
	return ((int64x1x4_t){__builtin_rvv_vcast_to_fixed_64_i64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_2), __builtin_rvv_vcast_to_fixed_64_i64m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8x4_t vld4_s8(const int8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vint8m1_t temp_2;
	vint8m1_t temp_3;
	vlseg4e8_v_i8m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 8);
	return ((int8x8x4_t){__builtin_rvv_vcast_to_fixed_64_i8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_1), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_2), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4x4_t vld4_u16(const uint16_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vuint16m1_t temp_2;
	vuint16m1_t temp_3;
	vlseg4e16_v_u16m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 4);
	return ((uint16x4x4_t){__builtin_rvv_vcast_to_fixed_64_u16m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_2), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2x4_t vld4_u32(const uint32_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vuint32m1_t temp_2;
	vuint32m1_t temp_3;
	vlseg4e32_v_u32m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 2);
	return ((uint32x2x4_t){__builtin_rvv_vcast_to_fixed_64_u32m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_2), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x1x4_t vld4_u64(const uint64_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0;
	vuint64m1_t temp_1;
	vuint64m1_t temp_2;
	vuint64m1_t temp_3;
	vlseg4e64_v_u64m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 1);
	return ((uint64x1x4_t){__builtin_rvv_vcast_to_fixed_64_u64m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_2), __builtin_rvv_vcast_to_fixed_64_u64m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8x4_t vld4_u8(const uint8_t * ptr)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vuint8m1_t temp_2;
	vuint8m1_t temp_3;
	vlseg4e8_v_u8m1((&temp_0), (&temp_1), (&temp_2), (&temp_3), ptr, 8);
	return ((uint8x8x4_t){__builtin_rvv_vcast_to_fixed_64_u8m1(temp_0), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_2), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmax_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmax(temp_0, temp_1, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmax_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmax(temp_0, temp_1, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmax_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmor(vmfne(temp_0, temp_0, 1), vmfne(temp_1, temp_1, 1), 1), vfmax(temp_0, temp_1, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmaxh_f16") float16_t vmaxh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmaxnm_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmseq(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 256, 4), vfmax(temp_0, temp_1, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmaxnm_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmseq(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 256, 2), vfmax(temp_0, temp_1, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmaxnm_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmseq(vor(vfclass(temp_0, 1), vfclass(temp_1, 1), 1), 256, 1), vfmax(temp_0, temp_1, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmaxnmh_f16") float16_t vmaxnmh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmaxnmq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmseq(vor(vfclass(temp_0, 8), vfclass(temp_1, 8), 8), 256, 8), vfmax(temp_0, temp_1, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmaxnmq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmseq(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 256, 4), vfmax(temp_0, temp_1, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmaxnmq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmseq(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 256, 2), vfmax(temp_0, temp_1, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmaxnmv_f16") float16_t vmaxnmv_f16(float16x4_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmaxnmv_f32") float32_t vmaxnmv_f32(float32x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmaxnmvq_f16") float16_t vmaxnmvq_f16(float16x8_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmaxnmvq_f32") float32_t vmaxnmvq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmaxnmvq_f64") float64_t vmaxnmvq_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmaxq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmor(vmfne(temp_0, temp_0, 8), vmfne(temp_1, temp_1, 8), 8), vfmax(temp_0, temp_1, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmaxq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmax(temp_0, temp_1, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmaxq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmax(temp_0, temp_1, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmaxq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmax(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmaxq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmax(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmaxq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmax(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmaxq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmaxu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmaxq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmaxu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmaxq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmaxu(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmax_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmax(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmax_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmax(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmax_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmax(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmax_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmaxu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmax_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmaxu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmax_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmaxu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vmaxv_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	float16_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 4), 4) != -1))
	{
		uint16_t temp_2 = 32256;
		temp_1 = (*((const float16_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmax(vundefined_f16m1(), temp_0, temp_0, 4));
	}
	return temp_1;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vmaxv_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	float32_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 2), 2) != -1))
	{
		uint32_t temp_2 = 2143289344L;
		temp_1 = (*((const float32_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmax(vundefined_f32m1(), temp_0, temp_0, 2));
	}
	return temp_1;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vmaxvq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	float16_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 8), 8) != -1))
	{
		uint16_t temp_2 = 32256;
		temp_1 = (*((const float16_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmax(vundefined_f16m1(), temp_0, vlmul_trunc_v_f16m2_f16m1(temp_0), 8));
	}
	return temp_1;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vmaxvq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	float32_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 4), 4) != -1))
	{
		uint32_t temp_2 = 2143289344L;
		temp_1 = (*((const float32_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmax(vundefined_f32m1(), temp_0, vlmul_trunc_v_f32m2_f32m1(temp_0), 4));
	}
	return temp_1;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64_t vmaxvq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	float64_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 2), 2) != -1))
	{
		uint64_t temp_2 = 9221120237041090560LL;
		temp_1 = (*((const float64_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmax(vundefined_f64m1(), temp_0, vlmul_trunc_v_f64m2_f64m1(temp_0), 2));
	}
	return temp_1;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vmaxvq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return vmv_x(vredmax(vundefined_i16m1(), temp_0, vlmul_trunc_v_i16m2_i16m1(temp_0), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vmaxvq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return vmv_x(vredmax(vundefined_i32m1(), temp_0, vlmul_trunc_v_i32m2_i32m1(temp_0), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vmaxvq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return vmv_x(vredmax(vundefined_i8m1(), temp_0, vlmul_trunc_v_i8m2_i8m1(temp_0), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vmaxvq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return vmv_x(vredmaxu(vundefined_u16m1(), temp_0, vlmul_trunc_v_u16m2_u16m1(temp_0), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vmaxvq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return vmv_x(vredmaxu(vundefined_u32m1(), temp_0, vlmul_trunc_v_u32m2_u32m1(temp_0), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vmaxvq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return vmv_x(vredmaxu(vundefined_u8m1(), temp_0, vlmul_trunc_v_u8m2_u8m1(temp_0), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vmaxv_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return vmv_x(vredmax(vundefined_i16m1(), temp_0, temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vmaxv_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return vmv_x(vredmax(vundefined_i32m1(), temp_0, temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vmaxv_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return vmv_x(vredmax(vundefined_i8m1(), temp_0, temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vmaxv_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return vmv_x(vredmaxu(vundefined_u16m1(), temp_0, temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vmaxv_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return vmv_x(vredmaxu(vundefined_u32m1(), temp_0, temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vmaxv_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return vmv_x(vredmaxu(vundefined_u8m1(), temp_0, temp_0, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmin_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmin(temp_0, temp_1, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmin_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmin(temp_0, temp_1, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmin_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmor(vmfne(temp_0, temp_0, 1), vmfne(temp_1, temp_1, 1), 1), vfmin(temp_0, temp_1, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vminh_f16") float16_t vminh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vminnm_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmseq(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 256, 4), vfmin(temp_0, temp_1, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vminnm_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmseq(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 256, 2), vfmin(temp_0, temp_1, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vminnm_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmseq(vor(vfclass(temp_0, 1), vfclass(temp_1, 1), 1), 256, 1), vfmin(temp_0, temp_1, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vminnmh_f16") float16_t vminnmh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vminnmq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmseq(vor(vfclass(temp_0, 8), vfclass(temp_1, 8), 8), 256, 8), vfmin(temp_0, temp_1, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vminnmq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmseq(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 256, 4), vfmin(temp_0, temp_1, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vminnmq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmseq(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 256, 2), vfmin(temp_0, temp_1, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vminnmv_f16") float16_t vminnmv_f16(float16x4_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vminnmv_f32") float32_t vminnmv_f32(float32x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vminnmvq_f16") float16_t vminnmvq_f16(float16x8_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vminnmvq_f32") float32_t vminnmvq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vminnmvq_f64") float64_t vminnmvq_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vminq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmor(vmfne(temp_0, temp_0, 8), vmfne(temp_1, temp_1, 8), 8), vfmin(temp_0, temp_1, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vminq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmin(temp_0, temp_1, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vminq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmin(temp_0, temp_1, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vminq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmin(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vminq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmin(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vminq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmin(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vminq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vminu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vminq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vminu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vminq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vminu(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmin_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmin(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmin_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmin(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmin_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmin(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmin_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vminu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmin_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vminu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmin_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vminu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vminv_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	float16_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 4), 4) != -1))
	{
		uint16_t temp_2 = 32256;
		temp_1 = (*((const float16_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmin(vundefined_f16m1(), temp_0, temp_0, 4));
	}
	return temp_1;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vminv_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	float32_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 2), 2) != -1))
	{
		uint32_t temp_2 = 2143289344L;
		temp_1 = (*((const float32_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmin(vundefined_f32m1(), temp_0, temp_0, 2));
	}
	return temp_1;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vminvq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	float16_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 8), 8) != -1))
	{
		uint16_t temp_2 = 32256;
		temp_1 = (*((const float16_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmin(vundefined_f16m1(), temp_0, vlmul_trunc_v_f16m2_f16m1(temp_0), 8));
	}
	return temp_1;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32_t vminvq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	float32_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 4), 4) != -1))
	{
		uint32_t temp_2 = 2143289344L;
		temp_1 = (*((const float32_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmin(vundefined_f32m1(), temp_0, vlmul_trunc_v_f32m2_f32m1(temp_0), 4));
	}
	return temp_1;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64_t vminvq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	float64_t temp_1;
	if ((vfirst(vmfne(temp_0, temp_0, 2), 2) != -1))
	{
		uint64_t temp_2 = 9221120237041090560LL;
		temp_1 = (*((const float64_t *)((&temp_2))));
	}
	else
	{
		temp_1 = vfmv_f(vfredmin(vundefined_f64m1(), temp_0, vlmul_trunc_v_f64m2_f64m1(temp_0), 2));
	}
	return temp_1;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vminvq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return vmv_x(vredmin(vundefined_i16m1(), temp_0, vlmul_trunc_v_i16m2_i16m1(temp_0), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vminvq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return vmv_x(vredmin(vundefined_i32m1(), temp_0, vlmul_trunc_v_i32m2_i32m1(temp_0), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vminvq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return vmv_x(vredmin(vundefined_i8m1(), temp_0, vlmul_trunc_v_i8m2_i8m1(temp_0), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vminvq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return vmv_x(vredminu(vundefined_u16m1(), temp_0, vlmul_trunc_v_u16m2_u16m1(temp_0), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vminvq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return vmv_x(vredminu(vundefined_u32m1(), temp_0, vlmul_trunc_v_u32m2_u32m1(temp_0), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vminvq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return vmv_x(vredminu(vundefined_u8m1(), temp_0, vlmul_trunc_v_u8m2_u8m1(temp_0), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16_t vminv_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return vmv_x(vredmin(vundefined_i16m1(), temp_0, temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32_t vminv_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return vmv_x(vredmin(vundefined_i32m1(), temp_0, temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8_t vminv_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return vmv_x(vredmin(vundefined_i8m1(), temp_0, temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16_t vminv_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return vmv_x(vredminu(vundefined_u16m1(), temp_0, temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32_t vminv_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return vmv_x(vredminu(vundefined_u32m1(), temp_0, temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8_t vminv_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return vmv_x(vredminu(vundefined_u8m1(), temp_0, temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmla_f32(float32x2_t a, float32x2_t b, float32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmla_f64(float64x1_t a, float64x1_t b, float64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmacc(temp_0, temp_1, temp_2, 1));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmla_lane_f32(float32x2_t a, float32x2_t b, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmla_laneq_f32(float32x2_t a, float32x2_t b, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	vfloat32m1_t temp_3 = vlmul_trunc_v_f32m2_f32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmla_laneq_s16(int16x4_t a, int16x4_t b, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(v);
	vint16m1_t temp_3 = vlmul_trunc_v_i16m2_i16m1(vrgather(temp_2, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmla_laneq_s32(int32x2_t a, int32x2_t b, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(v);
	vint32m1_t temp_3 = vlmul_trunc_v_i32m2_i32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmla_laneq_u16(uint16x4_t a, uint16x4_t b, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(v);
	vuint16m1_t temp_3 = vlmul_trunc_v_u16m2_u16m1(vrgather(temp_2, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmla_laneq_u32(uint32x2_t a, uint32x2_t b, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(v);
	vuint32m1_t temp_3 = vlmul_trunc_v_u32m2_u32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmla_lane_s16(int16x4_t a, int16x4_t b, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmla_lane_s32(int32x2_t a, int32x2_t b, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmla_lane_u16(uint16x4_t a, uint16x4_t b, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmla_lane_u32(uint32x2_t a, uint32x2_t b, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_laneq_s16") int32x4_t vmlal_high_laneq_s16(int32x4_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_laneq_s32") int64x2_t vmlal_high_laneq_s32(int64x2_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_laneq_u16") uint32x4_t vmlal_high_laneq_u16(uint32x4_t a, uint16x8_t b, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_laneq_u32") uint64x2_t vmlal_high_laneq_u32(uint64x2_t a, uint32x4_t b, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_lane_s16") int32x4_t vmlal_high_lane_s16(int32x4_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_lane_s32") int64x2_t vmlal_high_lane_s32(int64x2_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_lane_u16") uint32x4_t vmlal_high_lane_u16(uint32x4_t a, uint16x8_t b, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_lane_u32") uint64x2_t vmlal_high_lane_u32(uint64x2_t a, uint32x4_t b, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_n_s16") int32x4_t vmlal_high_n_s16(int32x4_t a, int16x8_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_n_s32") int64x2_t vmlal_high_n_s32(int64x2_t a, int32x4_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_n_u16") uint32x4_t vmlal_high_n_u16(uint32x4_t a, uint16x8_t b, uint16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlal_high_n_u32") uint64x2_t vmlal_high_n_u32(uint64x2_t a, uint32x4_t b, uint32_t c);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlal_high_s16(int32x4_t a, int16x8_t b, int16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(c);
	vint16m1_t temp_3 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_1, 4, 4));
	vint16m1_t temp_4 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_2, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(temp_0, temp_3, temp_4, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlal_high_s32(int64x2_t a, int32x4_t b, int32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(c);
	vint32m1_t temp_3 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_1, 2, 2));
	vint32m1_t temp_4 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_2, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmacc(temp_0, temp_3, temp_4, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlal_high_s8(int16x8_t a, int8x16_t b, int8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2(c);
	vint8m1_t temp_3 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_1, 8, 8));
	vint8m1_t temp_4 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_2, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwmacc(temp_0, temp_3, temp_4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlal_high_u16(uint32x4_t a, uint16x8_t b, uint16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(c);
	vuint16m1_t temp_3 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_1, 4, 4));
	vuint16m1_t temp_4 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_2, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(temp_0, temp_3, temp_4, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlal_high_u32(uint64x2_t a, uint32x4_t b, uint32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(c);
	vuint32m1_t temp_3 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_1, 2, 2));
	vuint32m1_t temp_4 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_2, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmaccu(temp_0, temp_3, temp_4, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlal_high_u8(uint16x8_t a, uint8x16_t b, uint8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2(c);
	vuint8m1_t temp_3 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_1, 8, 8));
	vuint8m1_t temp_4 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_2, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwmaccu(temp_0, temp_3, temp_4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlal_laneq_s16(int32x4_t a, int16x4_t b, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(v);
	vint16m1_t temp_3 = vlmul_trunc_v_i16m2_i16m1(vrgather(temp_2, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlal_laneq_s32(int64x2_t a, int32x2_t b, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(v);
	vint32m1_t temp_3 = vlmul_trunc_v_i32m2_i32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmacc(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlal_laneq_u16(uint32x4_t a, uint16x4_t b, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(v);
	vuint16m1_t temp_3 = vlmul_trunc_v_u16m2_u16m1(vrgather(temp_2, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(temp_0, temp_1, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlal_laneq_u32(uint64x2_t a, uint32x2_t b, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(v);
	vuint32m1_t temp_3 = vlmul_trunc_v_u32m2_u32m1(vrgather(temp_2, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmaccu(temp_0, temp_1, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlal_lane_s16(int32x4_t a, int16x4_t b, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlal_lane_s32(int64x2_t a, int32x2_t b, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmacc(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlal_lane_u16(uint32x4_t a, uint16x4_t b, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlal_lane_u32(uint64x2_t a, uint32x2_t b, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmaccu(temp_0, temp_1, vrgather(temp_2, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlal_n_s16(int32x4_t a, int16x4_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlal_n_s32(int64x2_t a, int32x2_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmacc(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlal_n_u16(uint32x4_t a, uint16x4_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlal_n_u32(uint64x2_t a, uint32x2_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmaccu(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlal_s32(int64x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlal_s8(int16x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlal_u16(uint32x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmaccu(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlal_u32(uint64x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmaccu(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlal_u8(uint16x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwmaccu(temp_0, temp_1, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmla_n_f32(float32x2_t a, float32x2_t b, float32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmacc(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmla_n_s16(int16x4_t a, int16x4_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmla_n_s32(int32x2_t a, int32x2_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmacc(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmla_n_u16(uint16x4_t a, uint16x4_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmla_n_u32(uint32x2_t a, uint32x2_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmacc(temp_0, c, temp_1, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlaq_f32(float32x4_t a, float32x4_t b, float32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmlaq_f64(float64x2_t a, float64x2_t b, float64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlaq_lane_f32(float32x4_t a, float32x4_t b, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	vfloat32m2_t temp_3 = vrgather(vlmul_ext_v_f32m1_f32m2(temp_2), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlaq_laneq_f32(float32x4_t a, float32x4_t b, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlaq_laneq_s16(int16x8_t a, int16x8_t b, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlaq_laneq_s32(int32x4_t a, int32x4_t b, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlaq_laneq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlaq_laneq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmacc(temp_0, temp_1, vrgather(temp_2, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlaq_lane_s16(int16x8_t a, int16x8_t b, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(v);
	vint16m2_t temp_3 = vrgather(vlmul_ext_v_i16m1_i16m2(temp_2), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmacc(temp_0, temp_1, temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlaq_lane_s32(int32x4_t a, int32x4_t b, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(v);
	vint32m2_t temp_3 = vrgather(vlmul_ext_v_i32m1_i32m2(temp_2), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlaq_lane_u16(uint16x8_t a, uint16x8_t b, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(v);
	vuint16m2_t temp_3 = vrgather(vlmul_ext_v_u16m1_u16m2(temp_2), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmacc(temp_0, temp_1, temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlaq_lane_u32(uint32x4_t a, uint32x4_t b, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(v);
	vuint32m2_t temp_3 = vrgather(vlmul_ext_v_u32m1_u32m2(temp_2), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmacc(temp_0, temp_1, temp_3, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlaq_n_f32(float32x4_t a, float32x4_t b, float32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlaq_n_s16(int16x8_t a, int16x8_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmacc(temp_0, c, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlaq_n_s32(int32x4_t a, int32x4_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlaq_n_u16(uint16x8_t a, uint16x8_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmacc(temp_0, c, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlaq_n_u32(uint32x4_t a, uint32x4_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmacc(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlaq_s16(int16x8_t a, int16x8_t b, int16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlaq_s32(int32x4_t a, int32x4_t b, int32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmlaq_s8(int8x16_t a, int8x16_t b, int8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmacc(temp_0, temp_1, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlaq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlaq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmlaq_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmacc(temp_0, temp_1, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmla_s16(int16x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmla_s32(int32x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmla_s8(int8x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmla_u16(uint16x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmacc(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmla_u32(uint32x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmacc(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmla_u8(uint8x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmacc(temp_0, temp_1, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmls_f32(float32x2_t a, float32x2_t b, float32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmls_f64(float64x1_t a, float64x1_t b, float64x1_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfnmsac(temp_0, temp_1, temp_2, 1));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_lane_f32") float32x2_t vmls_lane_f32(float32x2_t a, float32x2_t b, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_laneq_f32") float32x2_t vmls_laneq_f32(float32x2_t a, float32x2_t b, float32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_laneq_s16") int16x4_t vmls_laneq_s16(int16x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_laneq_s32") int32x2_t vmls_laneq_s32(int32x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_laneq_u16") uint16x4_t vmls_laneq_u16(uint16x4_t a, uint16x4_t b, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_laneq_u32") uint32x2_t vmls_laneq_u32(uint32x2_t a, uint32x2_t b, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_lane_s16") int16x4_t vmls_lane_s16(int16x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_lane_s32") int32x2_t vmls_lane_s32(int32x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_lane_u16") uint16x4_t vmls_lane_u16(uint16x4_t a, uint16x4_t b, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmls_lane_u32") uint32x2_t vmls_lane_u32(uint32x2_t a, uint32x2_t b, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_laneq_s16") int32x4_t vmlsl_high_laneq_s16(int32x4_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_laneq_s32") int64x2_t vmlsl_high_laneq_s32(int64x2_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_laneq_u16") uint32x4_t vmlsl_high_laneq_u16(uint32x4_t a, uint16x8_t b, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_laneq_u32") uint64x2_t vmlsl_high_laneq_u32(uint64x2_t a, uint32x4_t b, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_lane_s16") int32x4_t vmlsl_high_lane_s16(int32x4_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_lane_s32") int64x2_t vmlsl_high_lane_s32(int64x2_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_lane_u16") uint32x4_t vmlsl_high_lane_u16(uint32x4_t a, uint16x8_t b, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_lane_u32") uint64x2_t vmlsl_high_lane_u32(uint64x2_t a, uint32x4_t b, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_n_s16") int32x4_t vmlsl_high_n_s16(int32x4_t a, int16x8_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_n_s32") int64x2_t vmlsl_high_n_s32(int64x2_t a, int32x4_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_n_u16") uint32x4_t vmlsl_high_n_u16(uint32x4_t a, uint16x8_t b, uint16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_n_u32") uint64x2_t vmlsl_high_n_u32(uint64x2_t a, uint32x4_t b, uint32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_s16") int32x4_t vmlsl_high_s16(int32x4_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_s32") int64x2_t vmlsl_high_s32(int64x2_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_s8") int16x8_t vmlsl_high_s8(int16x8_t a, int8x16_t b, int8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_u16") uint32x4_t vmlsl_high_u16(uint32x4_t a, uint16x8_t b, uint16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_u32") uint64x2_t vmlsl_high_u32(uint64x2_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_high_u8") uint16x8_t vmlsl_high_u8(uint16x8_t a, uint8x16_t b, uint8x16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_laneq_s16") int32x4_t vmlsl_laneq_s16(int32x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_laneq_s32") int64x2_t vmlsl_laneq_s32(int64x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_laneq_u16") uint32x4_t vmlsl_laneq_u16(uint32x4_t a, uint16x4_t b, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_laneq_u32") uint64x2_t vmlsl_laneq_u32(uint64x2_t a, uint32x2_t b, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_lane_s16") int32x4_t vmlsl_lane_s16(int32x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_lane_s32") int64x2_t vmlsl_lane_s32(int64x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_lane_u16") uint32x4_t vmlsl_lane_u16(uint32x4_t a, uint16x4_t b, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsl_lane_u32") uint64x2_t vmlsl_lane_u32(uint64x2_t a, uint32x2_t b, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlsl_n_s16(int32x4_t a, int16x4_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint32m2_t temp_2 = vwmul(temp_1, c, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsub(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlsl_n_s32(int64x2_t a, int32x2_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint64m2_t temp_2 = vwmul(temp_1, c, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsub(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlsl_n_u16(uint32x4_t a, uint16x4_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint32m2_t temp_2 = vwmulu(temp_1, c, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsub(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlsl_n_u32(uint64x2_t a, uint32x2_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint64m2_t temp_2 = vwmulu(temp_1, c, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsub(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlsl_s16(int32x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	vint32m2_t temp_3 = vwmul(temp_1, temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsub(temp_0, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmlsl_s32(int64x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	vint64m2_t temp_3 = vwmul(temp_1, temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsub(temp_0, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlsl_s8(int16x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	vint16m2_t temp_3 = vwmul(temp_1, temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsub(temp_0, temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlsl_u16(uint32x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	vuint32m2_t temp_3 = vwmulu(temp_1, temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsub(temp_0, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmlsl_u32(uint64x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	vuint64m2_t temp_3 = vwmulu(temp_1, temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsub(temp_0, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlsl_u8(uint16x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	vuint16m2_t temp_3 = vwmulu(temp_1, temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsub(temp_0, temp_3, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmls_n_f32(float32x2_t a, float32x2_t b, float32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfnmsac(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmls_n_s16(int16x4_t a, int16x4_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnmsac(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmls_n_s32(int32x2_t a, int32x2_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnmsac(temp_0, c, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmls_n_u16(uint16x4_t a, uint16x4_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnmsac(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmls_n_u32(uint32x2_t a, uint32x2_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnmsac(temp_0, c, temp_1, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlsq_f32(float32x4_t a, float32x4_t b, float32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmlsq_f64(float64x2_t a, float64x2_t b, float64x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2(c);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_lane_f32") float32x4_t vmlsq_lane_f32(float32x4_t a, float32x4_t b, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_laneq_f32") float32x4_t vmlsq_laneq_f32(float32x4_t a, float32x4_t b, float32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_laneq_s16") int16x8_t vmlsq_laneq_s16(int16x8_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_laneq_s32") int32x4_t vmlsq_laneq_s32(int32x4_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_laneq_u16") uint16x8_t vmlsq_laneq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_laneq_u32") uint32x4_t vmlsq_laneq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_lane_s16") int16x8_t vmlsq_lane_s16(int16x8_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_lane_s32") int32x4_t vmlsq_lane_s32(int32x4_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_lane_u16") uint16x8_t vmlsq_lane_u16(uint16x8_t a, uint16x8_t b, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmlsq_lane_u32") uint32x4_t vmlsq_lane_u32(uint32x4_t a, uint32x4_t b, uint32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmlsq_n_f32(float32x4_t a, float32x4_t b, float32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfnmsac(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlsq_n_s16(int16x8_t a, int16x8_t b, int16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vnmsac(temp_0, c, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlsq_n_s32(int32x4_t a, int32x4_t b, int32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vnmsac(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlsq_n_u16(uint16x8_t a, uint16x8_t b, uint16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vnmsac(temp_0, c, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlsq_n_u32(uint32x4_t a, uint32x4_t b, uint32_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vnmsac(temp_0, c, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmlsq_s16(int16x8_t a, int16x8_t b, int16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vnmsac(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmlsq_s32(int32x4_t a, int32x4_t b, int32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmlsq_s8(int8x16_t a, int8x16_t b, int8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2(c);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vnmsac(temp_0, temp_1, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmlsq_u16(uint16x8_t a, uint16x8_t b, uint16x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vnmsac(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmlsq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmlsq_u8(uint8x16_t a, uint8x16_t b, uint8x16_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vnmsac(temp_0, temp_1, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmls_s16(int16x4_t a, int16x4_t b, int16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmls_s32(int32x2_t a, int32x2_t b, int32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmls_s8(int8x8_t a, int8x8_t b, int8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vnmsac(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmls_u16(uint16x4_t a, uint16x4_t b, uint16x4_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnmsac(temp_0, temp_1, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmls_u32(uint32x2_t a, uint32x2_t b, uint32x2_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnmsac(temp_0, temp_1, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmls_u8(uint8x8_t a, uint8x8_t b, uint8x8_t c)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(c);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vnmsac(temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmmlaq_s32") int32x4_t vmmlaq_s32(int32x4_t r, int8x16_t a, int8x16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmmlaq_u32") uint32x4_t vmmlaq_u32(uint32x4_t r, uint8x16_t a, uint8x16_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmovl_high_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m1_t temp_1 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_0, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwcvt_x(temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmovl_high_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m1_t temp_1 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_0, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwcvt_x(temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmovl_high_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m1_t temp_1 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_0, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwcvt_x(temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmovl_high_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m1_t temp_1 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_0, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwcvtu_x(temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmovl_high_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m1_t temp_1 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_0, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwcvtu_x(temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmovl_high_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m1_t temp_1 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_0, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwcvtu_x(temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmovl_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwcvt_x(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmovl_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwcvt_x(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmovl_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwcvt_x(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmovl_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwcvtu_x(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmovl_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwcvtu_x(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmovl_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwcvtu_x(temp_0, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmov_n_f16(float16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmv_v_f_f16m1(value, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmov_n_f32(float32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmv_v_f_f32m1(value, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmov_n_f64(float64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmv_v_f_f64m1(value, 1));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_s16") int8x16_t vmovn_high_s16(int8x8_t r, int16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_s32") int16x8_t vmovn_high_s32(int16x4_t r, int32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_s64") int32x4_t vmovn_high_s64(int32x2_t r, int64x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_u16") uint8x16_t vmovn_high_u16(uint8x8_t r, uint16x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_u32") uint16x8_t vmovn_high_u32(uint16x4_t r, uint32x4_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmovn_high_u64") uint32x4_t vmovn_high_u64(uint32x2_t r, uint64x2_t a);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmov_n_s16(int16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmv_v_x_i16m1(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmovn_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vnsra(temp_0, 0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmov_n_s32(int32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmv_v_x_i32m1(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmovn_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnsra(temp_0, 0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vmov_n_s64(int64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmv_v_x_i64m1(value, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmovn_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnsra(temp_0, 0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmov_n_s8(int8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmv_v_x_i8m1(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmov_n_u16(uint16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmv_v_x_u16m1(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmovn_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vnsrl(temp_0, 0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmov_n_u32(uint32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmv_v_x_u32m1(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmovn_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnsrl(temp_0, 0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vmov_n_u64(uint64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmv_v_x_u64m1(value, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmovn_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnsrl(temp_0, 0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmov_n_u8(uint8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmv_v_x_u8m1(value, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmovq_n_f16(float16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmv_v_f_f16m2(value, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmovq_n_f32(float32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmv_v_f_f32m2(value, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmovq_n_f64(float64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmv_v_f_f64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmovq_n_s16(int16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmv_v_x_i16m2(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmovq_n_s32(int32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmv_v_x_i32m2(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmovq_n_s64(int64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmv_v_x_i64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmovq_n_s8(int8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmv_v_x_i8m2(value, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmovq_n_u16(uint16_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmv_v_x_u16m2(value, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmovq_n_u32(uint32_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmv_v_x_u32m2(value, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmovq_n_u64(uint64_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmv_v_x_u64m2(value, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmovq_n_u8(uint8_t value)
{
	NEON2RVV_DEBUG_FUNCTION;
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmv_v_x_u8m2(value, 16));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmuld_lane_f64") float64_t vmuld_lane_f64(float64_t a, float64x1_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmuld_laneq_f64") float64_t vmuld_laneq_f64(float64_t a, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmul_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmul(temp_0, temp_1, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmul_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmul(temp_0, temp_1, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmul_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmul(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vmulh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a * b);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulh_lane_f16") float16_t vmulh_lane_f16(float16_t a, float16x4_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulh_laneq_f16") float16_t vmulh_laneq_f16(float16_t a, float16x8_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmul_lane_f16(float16x4_t a, float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmul_lane_f32(float32x2_t a, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmul(temp_0, vrgather(temp_1, lane, 2), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmul_lane_f64(float64x1_t a, float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(v);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmul(temp_0, vrgather(temp_1, lane, 1), 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmul_laneq_f16(float16x4_t a, float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(v);
	vfloat16m1_t temp_2 = vlmul_trunc_v_f16m2_f16m1(vrgather(temp_1, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmul(temp_0, temp_2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmul_laneq_f32(float32x2_t a, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	vfloat32m1_t temp_2 = vlmul_trunc_v_f32m2_f32m1(vrgather(temp_1, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmul(temp_0, temp_2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmul_laneq_f64(float64x1_t a, float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(v);
	vfloat64m1_t temp_2 = vlmul_trunc_v_f64m2_f64m1(vrgather(temp_1, lane, 1));
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmul(temp_0, temp_2, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmul_laneq_s16(int16x4_t a, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(v);
	vint16m1_t temp_2 = vlmul_trunc_v_i16m2_i16m1(vrgather(temp_1, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmul(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmul_laneq_s32(int32x2_t a, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(v);
	vint32m1_t temp_2 = vlmul_trunc_v_i32m2_i32m1(vrgather(temp_1, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmul(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmul_laneq_u16(uint16x4_t a, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(v);
	vuint16m1_t temp_2 = vlmul_trunc_v_u16m2_u16m1(vrgather(temp_1, lane, 4));
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmul(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmul_laneq_u32(uint32x2_t a, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(v);
	vuint32m1_t temp_2 = vlmul_trunc_v_u32m2_u32m1(vrgather(temp_1, lane, 2));
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmul(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmul_lane_s16(int16x4_t a, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmul_lane_s32(int32x2_t a, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmul(temp_0, vrgather(temp_1, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmul_lane_u16(uint16x4_t a, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmul_lane_u32(uint32x2_t a, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(v);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmul(temp_0, vrgather(temp_1, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_laneq_s16") int32x4_t vmull_high_laneq_s16(int16x8_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_laneq_s32") int64x2_t vmull_high_laneq_s32(int32x4_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_laneq_u16") uint32x4_t vmull_high_laneq_u16(uint16x8_t a, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_laneq_u32") uint64x2_t vmull_high_laneq_u32(uint32x4_t a, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_lane_s16") int32x4_t vmull_high_lane_s16(int16x8_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_lane_s32") int64x2_t vmull_high_lane_s32(int32x4_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_lane_u16") uint32x4_t vmull_high_lane_u16(uint16x8_t a, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_lane_u32") uint64x2_t vmull_high_lane_u32(uint32x4_t a, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_n_s16") int32x4_t vmull_high_n_s16(int16x8_t a, int16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_n_s32") int64x2_t vmull_high_n_s32(int32x4_t a, int32_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_n_u16") uint32x4_t vmull_high_n_u16(uint16x8_t a, uint16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_high_n_u32") uint64x2_t vmull_high_n_u32(uint32x4_t a, uint32_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmull_high_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m1_t temp_2 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_0, 4, 4));
	vint16m1_t temp_3 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmul(temp_2, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmull_high_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m1_t temp_2 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_0, 2, 2));
	vint32m1_t temp_3 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmul(temp_2, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmull_high_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m1_t temp_2 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_0, 8, 8));
	vint8m1_t temp_3 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwmul(temp_2, temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmull_high_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m1_t temp_2 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_0, 4, 4));
	vuint16m1_t temp_3 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmulu(temp_2, temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmull_high_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m1_t temp_2 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_0, 2, 2));
	vuint32m1_t temp_3 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmulu(temp_2, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmull_high_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m1_t temp_2 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_0, 8, 8));
	vuint8m1_t temp_3 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwmulu(temp_2, temp_3, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_laneq_s16") int32x4_t vmull_laneq_s16(int16x4_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_laneq_s32") int64x2_t vmull_laneq_s32(int32x2_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_laneq_u16") uint32x4_t vmull_laneq_u16(uint16x4_t a, uint16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_laneq_u32") uint64x2_t vmull_laneq_u32(uint32x2_t a, uint32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_lane_s16") int32x4_t vmull_lane_s16(int16x4_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_lane_s32") int64x2_t vmull_lane_s32(int32x2_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_lane_u16") uint32x4_t vmull_lane_u16(uint16x4_t a, uint16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmull_lane_u32") uint64x2_t vmull_lane_u32(uint32x2_t a, uint32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmull_n_s16(int16x4_t a, int16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmul(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmull_n_s32(int32x2_t a, int32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmul(temp_0, b, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmull_n_u16(uint16x4_t a, uint16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmulu(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmull_n_u32(uint32x2_t a, uint32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmulu(temp_0, b, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmull_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwmul(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vmull_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwmul(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmull_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwmul(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmull_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwmulu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vmull_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwmulu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmull_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwmulu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vmul_n_f16(float16x4_t a, float16_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmul(temp_0, n, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vmul_n_f32(float32x2_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmul(temp_0, b, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vmul_n_f64(float64x1_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmul(temp_0, b, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmul_n_s16(int16x4_t a, int16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmul(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmul_n_s32(int32x2_t a, int32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmul(temp_0, b, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmul_n_u16(uint16x4_t a, uint16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmul(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmul_n_u32(uint32x2_t a, uint32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmul(temp_0, b, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmulq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmul(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmulq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmul(temp_0, temp_1, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmulq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmul(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmulq_lane_f16(float16x8_t a, float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(v);
	vfloat16m2_t temp_2 = vrgather(vlmul_ext_v_f16m1_f16m2(temp_1), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmul(temp_0, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmulq_lane_f32(float32x4_t a, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(v);
	vfloat32m2_t temp_2 = vrgather(vlmul_ext_v_f32m1_f32m2(temp_1), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmul(temp_0, temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmulq_lane_f64(float64x2_t a, float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(v);
	vfloat64m2_t temp_2 = vrgather(vlmul_ext_v_f64m1_f64m2(temp_1), lane, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmul(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmulq_laneq_f16(float16x8_t a, float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmul(temp_0, vrgather(temp_1, lane, 8), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmulq_laneq_f32(float32x4_t a, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmulq_laneq_f64(float64x2_t a, float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(v);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmul(temp_0, vrgather(temp_1, lane, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmulq_laneq_s16(int16x8_t a, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmul(temp_0, vrgather(temp_1, lane, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmulq_laneq_s32(int32x4_t a, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmulq_laneq_u16(uint16x8_t a, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(v);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmul(temp_0, vrgather(temp_1, lane, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmulq_laneq_u32(uint32x4_t a, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(v);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmul(temp_0, vrgather(temp_1, lane, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmulq_lane_s16(int16x8_t a, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(v);
	vint16m2_t temp_2 = vrgather(vlmul_ext_v_i16m1_i16m2(temp_1), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmul(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmulq_lane_s32(int32x4_t a, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(v);
	vint32m2_t temp_2 = vrgather(vlmul_ext_v_i32m1_i32m2(temp_1), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmul(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmulq_lane_u16(uint16x8_t a, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(v);
	vuint16m2_t temp_2 = vrgather(vlmul_ext_v_u16m1_u16m2(temp_1), lane, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmul(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmulq_lane_u32(uint32x4_t a, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(v);
	vuint32m2_t temp_2 = vrgather(vlmul_ext_v_u32m1_u32m2(temp_1), lane, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmul(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vmulq_n_f16(float16x8_t a, float16_t n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmul(temp_0, n, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vmulq_n_f32(float32x4_t a, float32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmul(temp_0, b, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vmulq_n_f64(float64x2_t a, float64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmul(temp_0, b, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmulq_n_s16(int16x8_t a, int16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmul(temp_0, b, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmulq_n_s32(int32x4_t a, int32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmul(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmulq_n_u16(uint16x8_t a, uint16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmul(temp_0, b, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmulq_n_u32(uint32x4_t a, uint32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmul(temp_0, b, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmulq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmul(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmulq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmul(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmulq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmul(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmulq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmul(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmulq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmul(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmulq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmul(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmul_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmul(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmul_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmul(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmul_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmul(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmuls_lane_f32") float32_t vmuls_lane_f32(float32_t a, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmuls_laneq_f32") float32_t vmuls_laneq_f32(float32_t a, float32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmul_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmul(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmul_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmul(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmul_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmul(temp_0, temp_1, 8));
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vmulxd_f64") float64_t vmulxd_f64(float64_t a, float64_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxd_lane_f64") float64_t vmulxd_lane_f64(float64_t a, float64x1_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxd_laneq_f64") float64_t vmulxd_laneq_f64(float64_t a, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulx_f16") float16x4_t vmulx_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_f32") float32x2_t vmulx_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_f64") float64x1_t vmulx_f64(float64x1_t a, float64x1_t b);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxh_f16") float16_t vmulxh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxh_lane_f16") float16_t vmulxh_lane_f16(float16_t a, float16x4_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxh_laneq_f16") float16_t vmulxh_laneq_f16(float16_t a, float16x8_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulx_lane_f16") float16x4_t vmulx_lane_f16(float16x4_t a, float16x4_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_lane_f32") float32x2_t vmulx_lane_f32(float32x2_t a, float32x2_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_lane_f64") float64x1_t vmulx_lane_f64(float64x1_t a, float64x1_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulx_laneq_f16") float16x4_t vmulx_laneq_f16(float16x4_t a, float16x8_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_laneq_f32") float32x2_t vmulx_laneq_f32(float32x2_t a, float32x4_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulx_laneq_f64") float64x1_t vmulx_laneq_f64(float64x1_t a, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulx_n_f16") float16x4_t vmulx_n_f16(float16x4_t a, float16_t n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxq_f16") float16x8_t vmulxq_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_f32") float32x4_t vmulxq_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_f64") float64x2_t vmulxq_f64(float64x2_t a, float64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxq_lane_f16") float16x8_t vmulxq_lane_f16(float16x8_t a, float16x4_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_lane_f32") float32x4_t vmulxq_lane_f32(float32x4_t a, float32x2_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_lane_f64") float64x2_t vmulxq_lane_f64(float64x2_t a, float64x1_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxq_laneq_f16") float16x8_t vmulxq_laneq_f16(float16x8_t a, float16x8_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_laneq_f32") float32x4_t vmulxq_laneq_f32(float32x4_t a, float32x4_t v, const int lane);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxq_laneq_f64") float64x2_t vmulxq_laneq_f64(float64x2_t a, float64x2_t v, const int lane);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vmulxq_n_f16") float16x8_t vmulxq_n_f16(float16x8_t a, float16_t n);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vmulxs_f32") float32_t vmulxs_f32(float32_t a, float32_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxs_lane_f32") float32_t vmulxs_lane_f32(float32_t a, float32x2_t v, const int lane);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vmulxs_laneq_f32") float32_t vmulxs_laneq_f32(float32_t a, float32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vmvnq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vnot(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vmvnq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vnot(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vmvnq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vnot(temp_0, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vmvnq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vnot(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vmvnq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vnot(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vmvnq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vnot(temp_0, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vmvn_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnot(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vmvn_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnot(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vmvn_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vnot(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vmvn_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnot(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vmvn_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnot(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vmvn_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vnot(temp_0, 8));
}
#endif
__attribute__((always_inline)) inline int64_t vnegd_s64(int64_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (((uint64_t)(a)) * -1);
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vneg_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfneg(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vneg_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfneg(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vneg_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfneg(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vnegh_f16(float16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a * -1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vnegq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfneg(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vnegq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfneg(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vnegq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfneg(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vnegq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vneg(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vnegq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vneg(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vnegq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vneg(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vnegq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vneg(temp_0, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vneg_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vneg(temp_0, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vneg_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vneg(temp_0, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vneg_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vneg(temp_0, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vneg_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vneg(temp_0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vornq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vor(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vornq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vor(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vornq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vor(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vornq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vor(temp_0, vnot(temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vornq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vor(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vornq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vor(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vornq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vor(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vornq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vor(temp_0, vnot(temp_1, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vorn_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vor(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vorn_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vor(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vorn_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vor(temp_0, vnot(temp_1, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vorn_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vor(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vorn_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vor(temp_0, vnot(temp_1, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vorn_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vor(temp_0, vnot(temp_1, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vorn_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vor(temp_0, vnot(temp_1, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vorn_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vor(temp_0, vnot(temp_1, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vorrq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vorrq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vorrq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vorrq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vor(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vorrq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vorrq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vorrq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vorrq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vor(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vorr_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vorr_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vorr_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vor(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vorr_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vorr_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vor(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vorr_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vor(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vorr_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vor(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vorr_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vor(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vpadalq_s16(int32x4_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1;
	vint16m1_t temp_2;
	vlseg2e16_v_i16m1((&temp_1), (&temp_2), ((const int16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwadd_wv(vwadd_wv(temp_0, temp_1, 4), temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vpadalq_s32(int64x2_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1;
	vint32m1_t temp_2;
	vlseg2e32_v_i32m1((&temp_1), (&temp_2), ((const int32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwadd_wv(vwadd_wv(temp_0, temp_1, 2), temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vpadalq_s8(int16x8_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1;
	vint8m1_t temp_2;
	vlseg2e8_v_i8m1((&temp_1), (&temp_2), ((const int8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwadd_wv(vwadd_wv(temp_0, temp_1, 8), temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vpadalq_u16(uint32x4_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1;
	vuint16m1_t temp_2;
	vlseg2e16_v_u16m1((&temp_1), (&temp_2), ((const uint16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 4), temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vpadalq_u32(uint64x2_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1;
	vuint32m1_t temp_2;
	vlseg2e32_v_u32m1((&temp_1), (&temp_2), ((const uint32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 2), temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vpadalq_u8(uint16x8_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1;
	vuint8m1_t temp_2;
	vlseg2e8_v_u8m1((&temp_1), (&temp_2), ((const uint8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 8), temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vpadal_s16(int32x2_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint16mf2_t temp_1;
	vint16mf2_t temp_2;
	vlseg2e16_v_i16mf2((&temp_1), (&temp_2), ((const int16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vwadd_wv(vwadd_wv(temp_0, temp_1, 2), temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x1_t vpadal_s32(int64x1_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint32mf2_t temp_1;
	vint32mf2_t temp_2;
	vlseg2e32_v_i32mf2((&temp_1), (&temp_2), ((const int32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vwadd_wv(vwadd_wv(temp_0, temp_1, 1), temp_2, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vpadal_s8(int16x4_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint8mf2_t temp_1;
	vint8mf2_t temp_2;
	vlseg2e8_v_i8mf2((&temp_1), (&temp_2), ((const int8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vwadd_wv(vwadd_wv(temp_0, temp_1, 4), temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vpadal_u16(uint32x2_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint16mf2_t temp_1;
	vuint16mf2_t temp_2;
	vlseg2e16_v_u16mf2((&temp_1), (&temp_2), ((const uint16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 2), temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x1_t vpadal_u32(uint64x1_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint32mf2_t temp_1;
	vuint32mf2_t temp_2;
	vlseg2e32_v_u32mf2((&temp_1), (&temp_2), ((const uint32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 1), temp_2, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vpadal_u8(uint16x4_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint8mf2_t temp_1;
	vuint8mf2_t temp_2;
	vlseg2e8_v_u8mf2((&temp_1), (&temp_2), ((const uint8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vwaddu_wv(vwaddu_wv(temp_0, temp_1, 4), temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpaddd_f64") float64_t vpaddd_f64(float64x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpaddd_s64") int64_t vpaddd_s64(int64x2_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpaddd_u64") uint64_t vpaddd_u64(uint64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vpadd_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16mf2_t temp_0;
	vfloat16mf2_t temp_1;
	vlseg2e16_v_f16mf2((&temp_0), (&temp_1), ((const float16_t *)((&a))), 2);
	vfloat16mf2_t temp_2;
	vfloat16mf2_t temp_3;
	vlseg2e16_v_f16mf2((&temp_2), (&temp_3), ((const float16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vslideup(vlmul_ext_v_f16mf2_f16m1(vfadd(temp_0, temp_1, 4)), vlmul_ext_v_f16mf2_f16m1(vfadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vpadd_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32mf2_t temp_0;
	vfloat32mf2_t temp_1;
	vlseg2e32_v_f32mf2((&temp_0), (&temp_1), ((const float32_t *)((&a))), 1);
	vfloat32mf2_t temp_2;
	vfloat32mf2_t temp_3;
	vlseg2e32_v_f32mf2((&temp_2), (&temp_3), ((const float32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vslideup(vlmul_ext_v_f32mf2_f32m1(vfadd(temp_0, temp_1, 2)), vlmul_ext_v_f32mf2_f32m1(vfadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vpaddlq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vlseg2e16_v_i16m1((&temp_0), (&temp_1), ((const int16_t *)((&a))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwadd_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vpaddlq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vlseg2e32_v_i32m1((&temp_0), (&temp_1), ((const int32_t *)((&a))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwadd_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vpaddlq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vlseg2e8_v_i8m1((&temp_0), (&temp_1), ((const int8_t *)((&a))), 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwadd_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vpaddlq_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vlseg2e16_v_u16m1((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwaddu_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vpaddlq_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vlseg2e32_v_u32m1((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwaddu_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vpaddlq_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vlseg2e8_v_u8m1((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwaddu_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vpaddl_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16mf2_t temp_0;
	vint16mf2_t temp_1;
	vlseg2e16_v_i16mf2((&temp_0), (&temp_1), ((const int16_t *)((&a))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vwadd_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x1_t vpaddl_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32mf2_t temp_0;
	vint32mf2_t temp_1;
	vlseg2e32_v_i32mf2((&temp_0), (&temp_1), ((const int32_t *)((&a))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vwadd_vv(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vpaddl_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8mf2_t temp_0;
	vint8mf2_t temp_1;
	vlseg2e8_v_i8mf2((&temp_0), (&temp_1), ((const int8_t *)((&a))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vwadd_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vpaddl_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16mf2_t temp_0;
	vuint16mf2_t temp_1;
	vlseg2e16_v_u16mf2((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vwaddu_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x1_t vpaddl_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32mf2_t temp_0;
	vuint32mf2_t temp_1;
	vlseg2e32_v_u32mf2((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vwaddu_vv(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vpaddl_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8mf2_t temp_0;
	vuint8mf2_t temp_1;
	vlseg2e8_v_u8mf2((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vwaddu_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vpaddq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vlseg2e16_v_f16m1((&temp_0), (&temp_1), ((const float16_t *)((&a))), 4);
	vfloat16m1_t temp_2;
	vfloat16m1_t temp_3;
	vlseg2e16_v_f16m1((&temp_2), (&temp_3), ((const float16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vslideup(vlmul_ext_v_f16m1_f16m2(vfadd(temp_0, temp_1, 8)), vlmul_ext_v_f16m1_f16m2(vfadd(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vpaddq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vlseg2e32_v_f32m1((&temp_0), (&temp_1), ((const float32_t *)((&a))), 2);
	vfloat32m1_t temp_2;
	vfloat32m1_t temp_3;
	vlseg2e32_v_f32m1((&temp_2), (&temp_3), ((const float32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vslideup(vlmul_ext_v_f32m1_f32m2(vfadd(temp_0, temp_1, 4)), vlmul_ext_v_f32m1_f32m2(vfadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vpaddq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vlseg2e64_v_f64m1((&temp_0), (&temp_1), ((const float64_t *)((&a))), 1);
	vfloat64m1_t temp_2;
	vfloat64m1_t temp_3;
	vlseg2e64_v_f64m1((&temp_2), (&temp_3), ((const float64_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vslideup(vlmul_ext_v_f64m1_f64m2(vfadd(temp_0, temp_1, 2)), vlmul_ext_v_f64m1_f64m2(vfadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vpaddq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vlseg2e16_v_i16m1((&temp_0), (&temp_1), ((const int16_t *)((&a))), 4);
	vint16m1_t temp_2;
	vint16m1_t temp_3;
	vlseg2e16_v_i16m1((&temp_2), (&temp_3), ((const int16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vslideup(vlmul_ext_v_i16m1_i16m2(vadd(temp_0, temp_1, 8)), vlmul_ext_v_i16m1_i16m2(vadd(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vpaddq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vlseg2e32_v_i32m1((&temp_0), (&temp_1), ((const int32_t *)((&a))), 2);
	vint32m1_t temp_2;
	vint32m1_t temp_3;
	vlseg2e32_v_i32m1((&temp_2), (&temp_3), ((const int32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vslideup(vlmul_ext_v_i32m1_i32m2(vadd(temp_0, temp_1, 4)), vlmul_ext_v_i32m1_i32m2(vadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vpaddq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0;
	vint64m1_t temp_1;
	vlseg2e64_v_i64m1((&temp_0), (&temp_1), ((const int64_t *)((&a))), 1);
	vint64m1_t temp_2;
	vint64m1_t temp_3;
	vlseg2e64_v_i64m1((&temp_2), (&temp_3), ((const int64_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vslideup(vlmul_ext_v_i64m1_i64m2(vadd(temp_0, temp_1, 2)), vlmul_ext_v_i64m1_i64m2(vadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vpaddq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vlseg2e8_v_i8m1((&temp_0), (&temp_1), ((const int8_t *)((&a))), 8);
	vint8m1_t temp_2;
	vint8m1_t temp_3;
	vlseg2e8_v_i8m1((&temp_2), (&temp_3), ((const int8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vslideup(vlmul_ext_v_i8m1_i8m2(vadd(temp_0, temp_1, 16)), vlmul_ext_v_i8m1_i8m2(vadd(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vpaddq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vlseg2e16_v_u16m1((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 4);
	vuint16m1_t temp_2;
	vuint16m1_t temp_3;
	vlseg2e16_v_u16m1((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(vlmul_ext_v_u16m1_u16m2(vadd(temp_0, temp_1, 8)), vlmul_ext_v_u16m1_u16m2(vadd(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vpaddq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vlseg2e32_v_u32m1((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 2);
	vuint32m1_t temp_2;
	vuint32m1_t temp_3;
	vlseg2e32_v_u32m1((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(vlmul_ext_v_u32m1_u32m2(vadd(temp_0, temp_1, 4)), vlmul_ext_v_u32m1_u32m2(vadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vpaddq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0;
	vuint64m1_t temp_1;
	vlseg2e64_v_u64m1((&temp_0), (&temp_1), ((const uint64_t *)((&a))), 1);
	vuint64m1_t temp_2;
	vuint64m1_t temp_3;
	vlseg2e64_v_u64m1((&temp_2), (&temp_3), ((const uint64_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vslideup(vlmul_ext_v_u64m1_u64m2(vadd(temp_0, temp_1, 2)), vlmul_ext_v_u64m1_u64m2(vadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vpaddq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vlseg2e8_v_u8m1((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 8);
	vuint8m1_t temp_2;
	vuint8m1_t temp_3;
	vlseg2e8_v_u8m1((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(vlmul_ext_v_u8m1_u8m2(vadd(temp_0, temp_1, 16)), vlmul_ext_v_u8m1_u8m2(vadd(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vpadd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16mf2_t temp_0;
	vint16mf2_t temp_1;
	vlseg2e16_v_i16mf2((&temp_0), (&temp_1), ((const int16_t *)((&a))), 2);
	vint16mf2_t temp_2;
	vint16mf2_t temp_3;
	vlseg2e16_v_i16mf2((&temp_2), (&temp_3), ((const int16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vslideup(vlmul_ext_v_i16mf2_i16m1(vadd(temp_0, temp_1, 4)), vlmul_ext_v_i16mf2_i16m1(vadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vpadd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32mf2_t temp_0;
	vint32mf2_t temp_1;
	vlseg2e32_v_i32mf2((&temp_0), (&temp_1), ((const int32_t *)((&a))), 1);
	vint32mf2_t temp_2;
	vint32mf2_t temp_3;
	vlseg2e32_v_i32mf2((&temp_2), (&temp_3), ((const int32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vslideup(vlmul_ext_v_i32mf2_i32m1(vadd(temp_0, temp_1, 2)), vlmul_ext_v_i32mf2_i32m1(vadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vpadd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8mf2_t temp_0;
	vint8mf2_t temp_1;
	vlseg2e8_v_i8mf2((&temp_0), (&temp_1), ((const int8_t *)((&a))), 4);
	vint8mf2_t temp_2;
	vint8mf2_t temp_3;
	vlseg2e8_v_i8mf2((&temp_2), (&temp_3), ((const int8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vslideup(vlmul_ext_v_i8mf2_i8m1(vadd(temp_0, temp_1, 8)), vlmul_ext_v_i8mf2_i8m1(vadd(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpadds_f32") float32_t vpadds_f32(float32x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vpadd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16mf2_t temp_0;
	vuint16mf2_t temp_1;
	vlseg2e16_v_u16mf2((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 2);
	vuint16mf2_t temp_2;
	vuint16mf2_t temp_3;
	vlseg2e16_v_u16mf2((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vslideup(vlmul_ext_v_u16mf2_u16m1(vadd(temp_0, temp_1, 4)), vlmul_ext_v_u16mf2_u16m1(vadd(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vpadd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32mf2_t temp_0;
	vuint32mf2_t temp_1;
	vlseg2e32_v_u32mf2((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 1);
	vuint32mf2_t temp_2;
	vuint32mf2_t temp_3;
	vlseg2e32_v_u32mf2((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vslideup(vlmul_ext_v_u32mf2_u32m1(vadd(temp_0, temp_1, 2)), vlmul_ext_v_u32mf2_u32m1(vadd(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vpadd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8mf2_t temp_0;
	vuint8mf2_t temp_1;
	vlseg2e8_v_u8mf2((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 4);
	vuint8mf2_t temp_2;
	vuint8mf2_t temp_3;
	vlseg2e8_v_u8mf2((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vslideup(vlmul_ext_v_u8mf2_u8m1(vadd(temp_0, temp_1, 8)), vlmul_ext_v_u8mf2_u8m1(vadd(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vpmax_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16mf2_t temp_0;
	vfloat16mf2_t temp_1;
	vlseg2e16_v_f16mf2((&temp_0), (&temp_1), ((const float16_t *)((&a))), 2);
	vfloat16mf2_t temp_2;
	vfloat16mf2_t temp_3;
	vlseg2e16_v_f16mf2((&temp_2), (&temp_3), ((const float16_t *)((&b))), 2);
	uint16_t temp_4 = 32256;
	const float16_t temp_5 = (*((const float16_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vslideup(vlmul_ext_v_f16mf2_f16m1(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmax(temp_0, temp_1, 4), temp_5, 4)), vlmul_ext_v_f16mf2_f16m1(vfmerge(vmor(vmfne(temp_2, temp_2, 4), vmfne(temp_3, temp_3, 4), 4), vfmax(temp_2, temp_3, 4), temp_5, 4)), 2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vpmax_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32mf2_t temp_0;
	vfloat32mf2_t temp_1;
	vlseg2e32_v_f32mf2((&temp_0), (&temp_1), ((const float32_t *)((&a))), 1);
	vfloat32mf2_t temp_2;
	vfloat32mf2_t temp_3;
	vlseg2e32_v_f32mf2((&temp_2), (&temp_3), ((const float32_t *)((&b))), 1);
	uint32_t temp_4 = 2143289344L;
	const float32_t temp_5 = (*((const float32_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vslideup(vlmul_ext_v_f32mf2_f32m1(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmax(temp_0, temp_1, 2), temp_5, 2)), vlmul_ext_v_f32mf2_f32m1(vfmerge(vmor(vmfne(temp_2, temp_2, 2), vmfne(temp_3, temp_3, 2), 2), vfmax(temp_2, temp_3, 2), temp_5, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vpmaxnm_f16") float16x4_t vpmaxnm_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxnm_f32") float32x2_t vpmaxnm_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxnmqd_f64") float64_t vpmaxnmqd_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vpmaxnmq_f16") float16x8_t vpmaxnmq_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxnmq_f32") float32x4_t vpmaxnmq_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxnmq_f64") float64x2_t vpmaxnmq_f64(float64x2_t a, float64x2_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxnms_f32") float32_t vpmaxnms_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxqd_f64") float64_t vpmaxqd_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vpmaxq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vlseg2e16_v_f16m1((&temp_0), (&temp_1), ((const float16_t *)((&a))), 4);
	vfloat16m1_t temp_2;
	vfloat16m1_t temp_3;
	vlseg2e16_v_f16m1((&temp_2), (&temp_3), ((const float16_t *)((&b))), 4);
	uint16_t temp_4 = 32256;
	const float16_t temp_5 = (*((const float16_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vslideup(vlmul_ext_v_f16m1_f16m2(vfmerge(vmor(vmfne(temp_0, temp_0, 8), vmfne(temp_1, temp_1, 8), 8), vfmax(temp_0, temp_1, 8), temp_5, 8)), vlmul_ext_v_f16m1_f16m2(vfmerge(vmor(vmfne(temp_2, temp_2, 8), vmfne(temp_3, temp_3, 8), 8), vfmax(temp_2, temp_3, 8), temp_5, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vpmaxq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vlseg2e32_v_f32m1((&temp_0), (&temp_1), ((const float32_t *)((&a))), 2);
	vfloat32m1_t temp_2;
	vfloat32m1_t temp_3;
	vlseg2e32_v_f32m1((&temp_2), (&temp_3), ((const float32_t *)((&b))), 2);
	uint32_t temp_4 = 2143289344L;
	const float32_t temp_5 = (*((const float32_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vslideup(vlmul_ext_v_f32m1_f32m2(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmax(temp_0, temp_1, 4), temp_5, 4)), vlmul_ext_v_f32m1_f32m2(vfmerge(vmor(vmfne(temp_2, temp_2, 4), vmfne(temp_3, temp_3, 4), 4), vfmax(temp_2, temp_3, 4), temp_5, 4)), 2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vpmaxq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vlseg2e64_v_f64m1((&temp_0), (&temp_1), ((const float64_t *)((&a))), 1);
	vfloat64m1_t temp_2;
	vfloat64m1_t temp_3;
	vlseg2e64_v_f64m1((&temp_2), (&temp_3), ((const float64_t *)((&b))), 1);
	uint64_t temp_4 = 9221120237041090560LL;
	const float64_t temp_5 = (*((const float64_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vslideup(vlmul_ext_v_f64m1_f64m2(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmax(temp_0, temp_1, 2), temp_5, 2)), vlmul_ext_v_f64m1_f64m2(vfmerge(vmor(vmfne(temp_2, temp_2, 2), vmfne(temp_3, temp_3, 2), 2), vfmax(temp_2, temp_3, 2), temp_5, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vpmaxq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vlseg2e16_v_i16m1((&temp_0), (&temp_1), ((const int16_t *)((&a))), 4);
	vint16m1_t temp_2;
	vint16m1_t temp_3;
	vlseg2e16_v_i16m1((&temp_2), (&temp_3), ((const int16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vslideup(vlmul_ext_v_i16m1_i16m2(vmax(temp_0, temp_1, 8)), vlmul_ext_v_i16m1_i16m2(vmax(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vpmaxq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vlseg2e32_v_i32m1((&temp_0), (&temp_1), ((const int32_t *)((&a))), 2);
	vint32m1_t temp_2;
	vint32m1_t temp_3;
	vlseg2e32_v_i32m1((&temp_2), (&temp_3), ((const int32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vslideup(vlmul_ext_v_i32m1_i32m2(vmax(temp_0, temp_1, 4)), vlmul_ext_v_i32m1_i32m2(vmax(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vpmaxq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vlseg2e8_v_i8m1((&temp_0), (&temp_1), ((const int8_t *)((&a))), 8);
	vint8m1_t temp_2;
	vint8m1_t temp_3;
	vlseg2e8_v_i8m1((&temp_2), (&temp_3), ((const int8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vslideup(vlmul_ext_v_i8m1_i8m2(vmax(temp_0, temp_1, 16)), vlmul_ext_v_i8m1_i8m2(vmax(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vpmaxq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vlseg2e16_v_u16m1((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 4);
	vuint16m1_t temp_2;
	vuint16m1_t temp_3;
	vlseg2e16_v_u16m1((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(vlmul_ext_v_u16m1_u16m2(vmaxu(temp_0, temp_1, 8)), vlmul_ext_v_u16m1_u16m2(vmaxu(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vpmaxq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vlseg2e32_v_u32m1((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 2);
	vuint32m1_t temp_2;
	vuint32m1_t temp_3;
	vlseg2e32_v_u32m1((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(vlmul_ext_v_u32m1_u32m2(vmaxu(temp_0, temp_1, 4)), vlmul_ext_v_u32m1_u32m2(vmaxu(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vpmaxq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vlseg2e8_v_u8m1((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 8);
	vuint8m1_t temp_2;
	vuint8m1_t temp_3;
	vlseg2e8_v_u8m1((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(vlmul_ext_v_u8m1_u8m2(vmaxu(temp_0, temp_1, 16)), vlmul_ext_v_u8m1_u8m2(vmaxu(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vpmax_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16mf2_t temp_0;
	vint16mf2_t temp_1;
	vlseg2e16_v_i16mf2((&temp_0), (&temp_1), ((const int16_t *)((&a))), 2);
	vint16mf2_t temp_2;
	vint16mf2_t temp_3;
	vlseg2e16_v_i16mf2((&temp_2), (&temp_3), ((const int16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vslideup(vlmul_ext_v_i16mf2_i16m1(vmax(temp_0, temp_1, 4)), vlmul_ext_v_i16mf2_i16m1(vmax(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vpmax_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32mf2_t temp_0;
	vint32mf2_t temp_1;
	vlseg2e32_v_i32mf2((&temp_0), (&temp_1), ((const int32_t *)((&a))), 1);
	vint32mf2_t temp_2;
	vint32mf2_t temp_3;
	vlseg2e32_v_i32mf2((&temp_2), (&temp_3), ((const int32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vslideup(vlmul_ext_v_i32mf2_i32m1(vmax(temp_0, temp_1, 2)), vlmul_ext_v_i32mf2_i32m1(vmax(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vpmax_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8mf2_t temp_0;
	vint8mf2_t temp_1;
	vlseg2e8_v_i8mf2((&temp_0), (&temp_1), ((const int8_t *)((&a))), 4);
	vint8mf2_t temp_2;
	vint8mf2_t temp_3;
	vlseg2e8_v_i8mf2((&temp_2), (&temp_3), ((const int8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vslideup(vlmul_ext_v_i8mf2_i8m1(vmax(temp_0, temp_1, 8)), vlmul_ext_v_i8mf2_i8m1(vmax(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmaxs_f32") float32_t vpmaxs_f32(float32x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vpmax_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16mf2_t temp_0;
	vuint16mf2_t temp_1;
	vlseg2e16_v_u16mf2((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 2);
	vuint16mf2_t temp_2;
	vuint16mf2_t temp_3;
	vlseg2e16_v_u16mf2((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vslideup(vlmul_ext_v_u16mf2_u16m1(vmaxu(temp_0, temp_1, 4)), vlmul_ext_v_u16mf2_u16m1(vmaxu(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vpmax_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32mf2_t temp_0;
	vuint32mf2_t temp_1;
	vlseg2e32_v_u32mf2((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 1);
	vuint32mf2_t temp_2;
	vuint32mf2_t temp_3;
	vlseg2e32_v_u32mf2((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vslideup(vlmul_ext_v_u32mf2_u32m1(vmaxu(temp_0, temp_1, 2)), vlmul_ext_v_u32mf2_u32m1(vmaxu(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vpmax_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8mf2_t temp_0;
	vuint8mf2_t temp_1;
	vlseg2e8_v_u8mf2((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 4);
	vuint8mf2_t temp_2;
	vuint8mf2_t temp_3;
	vlseg2e8_v_u8mf2((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vslideup(vlmul_ext_v_u8mf2_u8m1(vmaxu(temp_0, temp_1, 8)), vlmul_ext_v_u8mf2_u8m1(vmaxu(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vpmin_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16mf2_t temp_0;
	vfloat16mf2_t temp_1;
	vlseg2e16_v_f16mf2((&temp_0), (&temp_1), ((const float16_t *)((&a))), 2);
	vfloat16mf2_t temp_2;
	vfloat16mf2_t temp_3;
	vlseg2e16_v_f16mf2((&temp_2), (&temp_3), ((const float16_t *)((&b))), 2);
	uint16_t temp_4 = 32256;
	const float16_t temp_5 = (*((const float16_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vslideup(vlmul_ext_v_f16mf2_f16m1(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmin(temp_0, temp_1, 4), temp_5, 4)), vlmul_ext_v_f16mf2_f16m1(vfmerge(vmor(vmfne(temp_2, temp_2, 4), vmfne(temp_3, temp_3, 4), 4), vfmin(temp_2, temp_3, 4), temp_5, 4)), 2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vpmin_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32mf2_t temp_0;
	vfloat32mf2_t temp_1;
	vlseg2e32_v_f32mf2((&temp_0), (&temp_1), ((const float32_t *)((&a))), 1);
	vfloat32mf2_t temp_2;
	vfloat32mf2_t temp_3;
	vlseg2e32_v_f32mf2((&temp_2), (&temp_3), ((const float32_t *)((&b))), 1);
	uint32_t temp_4 = 2143289344L;
	const float32_t temp_5 = (*((const float32_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vslideup(vlmul_ext_v_f32mf2_f32m1(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmin(temp_0, temp_1, 2), temp_5, 2)), vlmul_ext_v_f32mf2_f32m1(vfmerge(vmor(vmfne(temp_2, temp_2, 2), vmfne(temp_3, temp_3, 2), 2), vfmin(temp_2, temp_3, 2), temp_5, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vpminnm_f16") float16x4_t vpminnm_f16(float16x4_t a, float16x4_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminnm_f32") float32x2_t vpminnm_f32(float32x2_t a, float32x2_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminnmqd_f64") float64_t vpminnmqd_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vpminnmq_f16") float16x8_t vpminnmq_f16(float16x8_t a, float16x8_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminnmq_f32") float32x4_t vpminnmq_f32(float32x4_t a, float32x4_t b);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminnmq_f64") float64x2_t vpminnmq_f64(float64x2_t a, float64x2_t b);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminnms_f32") float32_t vpminnms_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpminqd_f64") float64_t vpminqd_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vpminq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0;
	vfloat16m1_t temp_1;
	vlseg2e16_v_f16m1((&temp_0), (&temp_1), ((const float16_t *)((&a))), 4);
	vfloat16m1_t temp_2;
	vfloat16m1_t temp_3;
	vlseg2e16_v_f16m1((&temp_2), (&temp_3), ((const float16_t *)((&b))), 4);
	uint16_t temp_4 = 32256;
	const float16_t temp_5 = (*((const float16_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vslideup(vlmul_ext_v_f16m1_f16m2(vfmerge(vmor(vmfne(temp_0, temp_0, 8), vmfne(temp_1, temp_1, 8), 8), vfmin(temp_0, temp_1, 8), temp_5, 8)), vlmul_ext_v_f16m1_f16m2(vfmerge(vmor(vmfne(temp_2, temp_2, 8), vmfne(temp_3, temp_3, 8), 8), vfmin(temp_2, temp_3, 8), temp_5, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vpminq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0;
	vfloat32m1_t temp_1;
	vlseg2e32_v_f32m1((&temp_0), (&temp_1), ((const float32_t *)((&a))), 2);
	vfloat32m1_t temp_2;
	vfloat32m1_t temp_3;
	vlseg2e32_v_f32m1((&temp_2), (&temp_3), ((const float32_t *)((&b))), 2);
	uint32_t temp_4 = 2143289344L;
	const float32_t temp_5 = (*((const float32_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vslideup(vlmul_ext_v_f32m1_f32m2(vfmerge(vmor(vmfne(temp_0, temp_0, 4), vmfne(temp_1, temp_1, 4), 4), vfmin(temp_0, temp_1, 4), temp_5, 4)), vlmul_ext_v_f32m1_f32m2(vfmerge(vmor(vmfne(temp_2, temp_2, 4), vmfne(temp_3, temp_3, 4), 4), vfmin(temp_2, temp_3, 4), temp_5, 4)), 2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vpminq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0;
	vfloat64m1_t temp_1;
	vlseg2e64_v_f64m1((&temp_0), (&temp_1), ((const float64_t *)((&a))), 1);
	vfloat64m1_t temp_2;
	vfloat64m1_t temp_3;
	vlseg2e64_v_f64m1((&temp_2), (&temp_3), ((const float64_t *)((&b))), 1);
	uint64_t temp_4 = 9221120237041090560LL;
	const float64_t temp_5 = (*((const float64_t *)((&temp_4))));
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vslideup(vlmul_ext_v_f64m1_f64m2(vfmerge(vmor(vmfne(temp_0, temp_0, 2), vmfne(temp_1, temp_1, 2), 2), vfmin(temp_0, temp_1, 2), temp_5, 2)), vlmul_ext_v_f64m1_f64m2(vfmerge(vmor(vmfne(temp_2, temp_2, 2), vmfne(temp_3, temp_3, 2), 2), vfmin(temp_2, temp_3, 2), temp_5, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vpminq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0;
	vint16m1_t temp_1;
	vlseg2e16_v_i16m1((&temp_0), (&temp_1), ((const int16_t *)((&a))), 4);
	vint16m1_t temp_2;
	vint16m1_t temp_3;
	vlseg2e16_v_i16m1((&temp_2), (&temp_3), ((const int16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vslideup(vlmul_ext_v_i16m1_i16m2(vmin(temp_0, temp_1, 8)), vlmul_ext_v_i16m1_i16m2(vmin(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vpminq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0;
	vint32m1_t temp_1;
	vlseg2e32_v_i32m1((&temp_0), (&temp_1), ((const int32_t *)((&a))), 2);
	vint32m1_t temp_2;
	vint32m1_t temp_3;
	vlseg2e32_v_i32m1((&temp_2), (&temp_3), ((const int32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vslideup(vlmul_ext_v_i32m1_i32m2(vmin(temp_0, temp_1, 4)), vlmul_ext_v_i32m1_i32m2(vmin(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vpminq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0;
	vint8m1_t temp_1;
	vlseg2e8_v_i8m1((&temp_0), (&temp_1), ((const int8_t *)((&a))), 8);
	vint8m1_t temp_2;
	vint8m1_t temp_3;
	vlseg2e8_v_i8m1((&temp_2), (&temp_3), ((const int8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vslideup(vlmul_ext_v_i8m1_i8m2(vmin(temp_0, temp_1, 16)), vlmul_ext_v_i8m1_i8m2(vmin(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vpminq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0;
	vuint16m1_t temp_1;
	vlseg2e16_v_u16m1((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 4);
	vuint16m1_t temp_2;
	vuint16m1_t temp_3;
	vlseg2e16_v_u16m1((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(vlmul_ext_v_u16m1_u16m2(vminu(temp_0, temp_1, 8)), vlmul_ext_v_u16m1_u16m2(vminu(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vpminq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0;
	vuint32m1_t temp_1;
	vlseg2e32_v_u32m1((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 2);
	vuint32m1_t temp_2;
	vuint32m1_t temp_3;
	vlseg2e32_v_u32m1((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(vlmul_ext_v_u32m1_u32m2(vminu(temp_0, temp_1, 4)), vlmul_ext_v_u32m1_u32m2(vminu(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vpminq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0;
	vuint8m1_t temp_1;
	vlseg2e8_v_u8m1((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 8);
	vuint8m1_t temp_2;
	vuint8m1_t temp_3;
	vlseg2e8_v_u8m1((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(vlmul_ext_v_u8m1_u8m2(vminu(temp_0, temp_1, 16)), vlmul_ext_v_u8m1_u8m2(vminu(temp_2, temp_3, 16)), 8, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vpmin_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16mf2_t temp_0;
	vint16mf2_t temp_1;
	vlseg2e16_v_i16mf2((&temp_0), (&temp_1), ((const int16_t *)((&a))), 2);
	vint16mf2_t temp_2;
	vint16mf2_t temp_3;
	vlseg2e16_v_i16mf2((&temp_2), (&temp_3), ((const int16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vslideup(vlmul_ext_v_i16mf2_i16m1(vmin(temp_0, temp_1, 4)), vlmul_ext_v_i16mf2_i16m1(vmin(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vpmin_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32mf2_t temp_0;
	vint32mf2_t temp_1;
	vlseg2e32_v_i32mf2((&temp_0), (&temp_1), ((const int32_t *)((&a))), 1);
	vint32mf2_t temp_2;
	vint32mf2_t temp_3;
	vlseg2e32_v_i32mf2((&temp_2), (&temp_3), ((const int32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vslideup(vlmul_ext_v_i32mf2_i32m1(vmin(temp_0, temp_1, 2)), vlmul_ext_v_i32mf2_i32m1(vmin(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vpmin_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8mf2_t temp_0;
	vint8mf2_t temp_1;
	vlseg2e8_v_i8mf2((&temp_0), (&temp_1), ((const int8_t *)((&a))), 4);
	vint8mf2_t temp_2;
	vint8mf2_t temp_3;
	vlseg2e8_v_i8mf2((&temp_2), (&temp_3), ((const int8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vslideup(vlmul_ext_v_i8mf2_i8m1(vmin(temp_0, temp_1, 8)), vlmul_ext_v_i8mf2_i8m1(vmin(temp_2, temp_3, 8)), 4, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vpmins_f32") float32_t vpmins_f32(float32x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vpmin_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16mf2_t temp_0;
	vuint16mf2_t temp_1;
	vlseg2e16_v_u16mf2((&temp_0), (&temp_1), ((const uint16_t *)((&a))), 2);
	vuint16mf2_t temp_2;
	vuint16mf2_t temp_3;
	vlseg2e16_v_u16mf2((&temp_2), (&temp_3), ((const uint16_t *)((&b))), 2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vslideup(vlmul_ext_v_u16mf2_u16m1(vminu(temp_0, temp_1, 4)), vlmul_ext_v_u16mf2_u16m1(vminu(temp_2, temp_3, 4)), 2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vpmin_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32mf2_t temp_0;
	vuint32mf2_t temp_1;
	vlseg2e32_v_u32mf2((&temp_0), (&temp_1), ((const uint32_t *)((&a))), 1);
	vuint32mf2_t temp_2;
	vuint32mf2_t temp_3;
	vlseg2e32_v_u32mf2((&temp_2), (&temp_3), ((const uint32_t *)((&b))), 1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vslideup(vlmul_ext_v_u32mf2_u32m1(vminu(temp_0, temp_1, 2)), vlmul_ext_v_u32mf2_u32m1(vminu(temp_2, temp_3, 2)), 1, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vpmin_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8mf2_t temp_0;
	vuint8mf2_t temp_1;
	vlseg2e8_v_u8mf2((&temp_0), (&temp_1), ((const uint8_t *)((&a))), 4);
	vuint8mf2_t temp_2;
	vuint8mf2_t temp_3;
	vlseg2e8_v_u8mf2((&temp_2), (&temp_3), ((const uint8_t *)((&b))), 4);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vslideup(vlmul_ext_v_u8mf2_u8m1(vminu(temp_0, temp_1, 8)), vlmul_ext_v_u8mf2_u8m1(vminu(temp_2, temp_3, 8)), 4, 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqabsb_s8") int8_t vqabsb_s8(int8_t a);
NEON2RVV_NOT_IMPLEMENT("vqabsd_s64") int64_t vqabsd_s64(int64_t a);
NEON2RVV_NOT_IMPLEMENT("vqabsh_s16") int16_t vqabsh_s16(int16_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqabsq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vsmul(vmslt(temp_0, 0, 8), temp_0, temp_0, (-32767 - 1), 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqabsq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vsmul(vmslt(temp_0, 0, 4), temp_0, temp_0, (-2147483647L - 1), 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqabsq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vsmul(vmslt(temp_0, 0, 2), temp_0, temp_0, (-9223372036854775807LL - 1), 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqabsq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint8m2_t temp_3 = vsmul(vmslt(temp_0, 0, 16), temp_0, temp_0, -128, 16);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqabs_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vsmul(vmslt(temp_0, 0, 4), temp_0, temp_0, (-32767 - 1), 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqabs_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vsmul(vmslt(temp_0, 0, 2), temp_0, temp_0, (-2147483647L - 1), 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqabs_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vsmul(vmslt(temp_0, 0, 1), temp_0, temp_0, (-9223372036854775807LL - 1), 1);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqabs_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint8m1_t temp_3 = vsmul(vmslt(temp_0, 0, 8), temp_0, temp_0, -128, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqabss_s32") int32_t vqabss_s32(int32_t a);
NEON2RVV_NOT_IMPLEMENT("vqaddb_s8") int8_t vqaddb_s8(int8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqaddb_u8") uint8_t vqaddb_u8(uint8_t a, uint8_t b);
NEON2RVV_NOT_IMPLEMENT("vqaddd_s64") int64_t vqaddd_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqaddd_u64") uint64_t vqaddd_u64(uint64_t a, uint64_t b);
NEON2RVV_NOT_IMPLEMENT("vqaddh_s16") int16_t vqaddh_s16(int16_t a, int16_t b);
NEON2RVV_NOT_IMPLEMENT("vqaddh_u16") uint16_t vqaddh_u16(uint16_t a, uint16_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqaddq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsadd(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqaddq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqaddq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqaddq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vsadd(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqaddq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsaddu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqaddq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsaddu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vqaddq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsaddu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqaddq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vsaddu(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqadd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vsadd(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqadd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vsadd(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqadd_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vsadd(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqadd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vsadd(temp_0, temp_1, 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqadds_s32") int32_t vqadds_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqadds_u32") uint32_t vqadds_u32(uint32_t a, uint32_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqadd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vsaddu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqadd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vsaddu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vqadd_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vsaddu(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqadd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vsaddu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_laneq_s16") int32x4_t vqdmlal_high_laneq_s16(int32x4_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_laneq_s32") int64x2_t vqdmlal_high_laneq_s32(int64x2_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_lane_s16") int32x4_t vqdmlal_high_lane_s16(int32x4_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_lane_s32") int64x2_t vqdmlal_high_lane_s32(int64x2_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_n_s16") int32x4_t vqdmlal_high_n_s16(int32x4_t a, int16x8_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_n_s32") int64x2_t vqdmlal_high_n_s32(int64x2_t a, int32x4_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_s16") int32x4_t vqdmlal_high_s16(int32x4_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_high_s32") int64x2_t vqdmlal_high_s32(int64x2_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlalh_laneq_s16") int32_t vqdmlalh_laneq_s16(int32_t a, int16_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlalh_lane_s16") int32_t vqdmlalh_lane_s16(int32_t a, int16_t b, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmlalh_s16") int32_t vqdmlalh_s16(int32_t a, int16_t b, int16_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_laneq_s16") int32x4_t vqdmlal_laneq_s16(int32x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_laneq_s32") int64x2_t vqdmlal_laneq_s32(int64x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_lane_s16") int32x4_t vqdmlal_lane_s16(int32x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_lane_s32") int64x2_t vqdmlal_lane_s32(int64x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_n_s16") int32x4_t vqdmlal_n_s16(int32x4_t a, int16x4_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_n_s32") int64x2_t vqdmlal_n_s32(int64x2_t a, int32x2_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_s16") int32x4_t vqdmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlal_s32") int64x2_t vqdmlal_s32(int64x2_t a, int32x2_t b, int32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlals_laneq_s32") int64_t vqdmlals_laneq_s32(int64_t a, int32_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlals_lane_s32") int64_t vqdmlals_lane_s32(int64_t a, int32_t b, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmlals_s32") int64_t vqdmlals_s32(int64_t a, int32_t b, int32_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_laneq_s16") int32x4_t vqdmlsl_high_laneq_s16(int32x4_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_laneq_s32") int64x2_t vqdmlsl_high_laneq_s32(int64x2_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_lane_s16") int32x4_t vqdmlsl_high_lane_s16(int32x4_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_lane_s32") int64x2_t vqdmlsl_high_lane_s32(int64x2_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_n_s16") int32x4_t vqdmlsl_high_n_s16(int32x4_t a, int16x8_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_n_s32") int64x2_t vqdmlsl_high_n_s32(int64x2_t a, int32x4_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_s16") int32x4_t vqdmlsl_high_s16(int32x4_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_high_s32") int64x2_t vqdmlsl_high_s32(int64x2_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlslh_laneq_s16") int32_t vqdmlslh_laneq_s16(int32_t a, int16_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlslh_lane_s16") int32_t vqdmlslh_lane_s16(int32_t a, int16_t b, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmlslh_s16") int32_t vqdmlslh_s16(int32_t a, int16_t b, int16_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_laneq_s16") int32x4_t vqdmlsl_laneq_s16(int32x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_laneq_s32") int64x2_t vqdmlsl_laneq_s32(int64x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_lane_s16") int32x4_t vqdmlsl_lane_s16(int32x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_lane_s32") int64x2_t vqdmlsl_lane_s32(int64x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_n_s16") int32x4_t vqdmlsl_n_s16(int32x4_t a, int16x4_t b, int16_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_n_s32") int64x2_t vqdmlsl_n_s32(int64x2_t a, int32x2_t b, int32_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_s16") int32x4_t vqdmlsl_s16(int32x4_t a, int16x4_t b, int16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsl_s32") int64x2_t vqdmlsl_s32(int64x2_t a, int32x2_t b, int32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsls_laneq_s32") int64_t vqdmlsls_laneq_s32(int64_t a, int32_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmlsls_lane_s32") int64_t vqdmlsls_lane_s32(int64_t a, int32_t b, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmlsls_s32") int64_t vqdmlsls_s32(int64_t a, int32_t b, int32_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhh_laneq_s16") int16_t vqdmulhh_laneq_s16(int16_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhh_lane_s16") int16_t vqdmulhh_lane_s16(int16_t a, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmulhh_s16") int16_t vqdmulhh_s16(int16_t a, int16_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_laneq_s16") int16x4_t vqdmulh_laneq_s16(int16x4_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_laneq_s32") int32x2_t vqdmulh_laneq_s32(int32x2_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_lane_s16") int16x4_t vqdmulh_lane_s16(int16x4_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_lane_s32") int32x2_t vqdmulh_lane_s32(int32x2_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_n_s16") int16x4_t vqdmulh_n_s16(int16x4_t a, int16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulh_n_s32") int32x2_t vqdmulh_n_s32(int32x2_t a, int32_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_laneq_s16") int16x8_t vqdmulhq_laneq_s16(int16x8_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_laneq_s32") int32x4_t vqdmulhq_laneq_s32(int32x4_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_lane_s16") int16x8_t vqdmulhq_lane_s16(int16x8_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_lane_s32") int32x4_t vqdmulhq_lane_s32(int32x4_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_n_s16") int16x8_t vqdmulhq_n_s16(int16x8_t a, int16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhq_n_s32") int32x4_t vqdmulhq_n_s32(int32x4_t a, int32_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqdmulhq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint32m4_t temp_2 = vwmul(temp_0, temp_1, 8);
	vint32m4_t temp_3 = vsadd(temp_2, temp_2, 8);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_DOWNWARD);
	assert(temp_5 == 0);
	vint16m2_t temp_6 = vnclip(temp_3, 16, 8);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqdmulhq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint64m4_t temp_2 = vwmul(temp_0, temp_1, 4);
	vint64m4_t temp_3 = vsadd(temp_2, temp_2, 4);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_DOWNWARD);
	assert(temp_5 == 0);
	vint32m2_t temp_6 = vnclip(temp_3, 32, 4);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqdmulh_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint32m2_t temp_2 = vwmul(temp_0, temp_1, 4);
	vint32m2_t temp_3 = vsadd(temp_2, temp_2, 4);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_DOWNWARD);
	assert(temp_5 == 0);
	vint16m1_t temp_6 = vnclip(temp_3, 16, 4);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqdmulh_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint64m2_t temp_2 = vwmul(temp_0, temp_1, 2);
	vint64m2_t temp_3 = vsadd(temp_2, temp_2, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_DOWNWARD);
	assert(temp_5 == 0);
	vint32m1_t temp_6 = vnclip(temp_3, 32, 2);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_6);
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhs_laneq_s32") int32_t vqdmulhs_laneq_s32(int32_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulhs_lane_s32") int32_t vqdmulhs_lane_s32(int32_t a, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmulhs_s32") int32_t vqdmulhs_s32(int32_t a, int32_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_laneq_s16") int32x4_t vqdmull_high_laneq_s16(int16x8_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_laneq_s32") int64x2_t vqdmull_high_laneq_s32(int32x4_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_lane_s16") int32x4_t vqdmull_high_lane_s16(int16x8_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_lane_s32") int64x2_t vqdmull_high_lane_s32(int32x4_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_n_s16") int32x4_t vqdmull_high_n_s16(int16x8_t a, int16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_n_s32") int64x2_t vqdmull_high_n_s32(int32x4_t a, int32_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_s16") int32x4_t vqdmull_high_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_high_s32") int64x2_t vqdmull_high_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmullh_laneq_s16") int32_t vqdmullh_laneq_s16(int16_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmullh_lane_s16") int32_t vqdmullh_lane_s16(int16_t a, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmullh_s16") int32_t vqdmullh_s16(int16_t a, int16_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_laneq_s16") int32x4_t vqdmull_laneq_s16(int16x4_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_laneq_s32") int64x2_t vqdmull_laneq_s32(int32x2_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_lane_s16") int32x4_t vqdmull_lane_s16(int16x4_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_lane_s32") int64x2_t vqdmull_lane_s32(int32x2_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_n_s16") int32x4_t vqdmull_n_s16(int16x4_t a, int16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_n_s32") int64x2_t vqdmull_n_s32(int32x2_t a, int32_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_s16") int32x4_t vqdmull_s16(int16x4_t a, int16x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmull_s32") int64x2_t vqdmull_s32(int32x2_t a, int32x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulls_laneq_s32") int64_t vqdmulls_laneq_s32(int32_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqdmulls_lane_s32") int64_t vqdmulls_lane_s32(int32_t a, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqdmulls_s32") int64_t vqdmulls_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqmovnd_s64") int32_t vqmovnd_s64(int64_t a);
NEON2RVV_NOT_IMPLEMENT("vqmovnd_u64") uint32_t vqmovnd_u64(uint64_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqmovn_high_s16(int8x8_t r, int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(r);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint8m1_t temp_4 = vnclip(temp_1, 0, 8);
	vesetxround(temp_2);
	vint8m2_t temp_5 = vlmul_ext_v_i8m1_i8m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vslideup(vlmul_ext_v_i8m1_i8m2(temp_0), temp_5, 8, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqmovn_high_s32(int16x4_t r, int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(r);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint16m1_t temp_4 = vnclip(temp_1, 0, 4);
	vesetxround(temp_2);
	vint16m2_t temp_5 = vlmul_ext_v_i16m1_i16m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vslideup(vlmul_ext_v_i16m1_i16m2(temp_0), temp_5, 4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqmovn_high_s64(int32x2_t r, int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(r);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint32m1_t temp_4 = vnclip(temp_1, 0, 2);
	vesetxround(temp_2);
	vint32m2_t temp_5 = vlmul_ext_v_i32m1_i32m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vslideup(vlmul_ext_v_i32m1_i32m2(temp_0), temp_5, 2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqmovn_high_u16(uint8x8_t r, uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(r);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vnclipu(temp_1, 0, 8);
	vesetxround(temp_2);
	vuint8m2_t temp_5 = vlmul_ext_v_u8m1_u8m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(vlmul_ext_v_u8m1_u8m2(temp_0), temp_5, 8, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqmovn_high_u32(uint16x4_t r, uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(r);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vnclipu(temp_1, 0, 4);
	vesetxround(temp_2);
	vuint16m2_t temp_5 = vlmul_ext_v_u16m1_u16m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(vlmul_ext_v_u16m1_u16m2(temp_0), temp_5, 4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqmovn_high_u64(uint32x2_t r, uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(r);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vnclipu(temp_1, 0, 2);
	vesetxround(temp_2);
	vuint32m2_t temp_5 = vlmul_ext_v_u32m1_u32m2(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(vlmul_ext_v_u32m1_u32m2(temp_0), temp_5, 2, 4));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqmovnh_s16") int8_t vqmovnh_s16(int16_t a);
NEON2RVV_NOT_IMPLEMENT("vqmovnh_u16") uint8_t vqmovnh_u16(uint16_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqmovn_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint8m1_t temp_3 = vnclip(temp_0, 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqmovn_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vnclip(temp_0, 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqmovn_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vnclip(temp_0, 0, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqmovns_s32") int16_t vqmovns_s32(int32_t a);
NEON2RVV_NOT_IMPLEMENT("vqmovns_u32") uint16_t vqmovns_u32(uint32_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqmovn_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint8m1_t temp_3 = vnclipu(temp_0, 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqmovn_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vnclipu(temp_0, 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqmovn_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vnclipu(temp_0, 0, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqmovund_s64") uint32_t vqmovund_s64(int64_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqmovun_high_s16(uint8x8_t r, int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(r);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vuint16m2_t temp_2 = vreinterpret_v_i16m2_u16m2(vmax(temp_1, 0, 8));
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint8m1_t temp_5 = vnclipu(temp_2, 0, 8);
	vesetxround(temp_3);
	vuint8m2_t temp_6 = vlmul_ext_v_u8m1_u8m2(temp_5);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vslideup(vlmul_ext_v_u8m1_u8m2(temp_0), temp_6, 8, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqmovun_high_s32(uint16x4_t r, int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(r);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vuint32m2_t temp_2 = vreinterpret_v_i32m2_u32m2(vmax(temp_1, 0, 4));
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m1_t temp_5 = vnclipu(temp_2, 0, 4);
	vesetxround(temp_3);
	vuint16m2_t temp_6 = vlmul_ext_v_u16m1_u16m2(temp_5);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vslideup(vlmul_ext_v_u16m1_u16m2(temp_0), temp_6, 4, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqmovun_high_s64(uint32x2_t r, int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(r);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vuint64m2_t temp_2 = vreinterpret_v_i64m2_u64m2(vmax(temp_1, 0, 2));
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m1_t temp_5 = vnclipu(temp_2, 0, 2);
	vesetxround(temp_3);
	vuint32m2_t temp_6 = vlmul_ext_v_u32m1_u32m2(temp_5);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vslideup(vlmul_ext_v_u32m1_u32m2(temp_0), temp_6, 2, 4));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqmovunh_s16") uint8_t vqmovunh_s16(int16_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqmovun_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vuint16m2_t temp_1 = vreinterpret_v_i16m2_u16m2(vmax(temp_0, 0, 8));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vnclipu(temp_1, 0, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqmovun_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vuint32m2_t temp_1 = vreinterpret_v_i32m2_u32m2(vmax(temp_0, 0, 4));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vnclipu(temp_1, 0, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqmovun_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vuint64m2_t temp_1 = vreinterpret_v_i64m2_u64m2(vmax(temp_0, 0, 2));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vnclipu(temp_1, 0, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqmovuns_s32") uint16_t vqmovuns_s32(int32_t a);
NEON2RVV_NOT_IMPLEMENT("vqnegb_s8") int8_t vqnegb_s8(int8_t a);
NEON2RVV_NOT_IMPLEMENT("vqnegd_s64") int64_t vqnegd_s64(int64_t a);
NEON2RVV_NOT_IMPLEMENT("vqnegh_s16") int16_t vqnegh_s16(int16_t a);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqnegq_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vsmul(temp_0, (-32767 - 1), 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqnegq_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vsmul(temp_0, (-2147483647L - 1), 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqnegq_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vsmul(temp_0, (-9223372036854775807LL - 1), 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqnegq_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint8m2_t temp_3 = vsmul(temp_0, -128, 16);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqneg_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vsmul(temp_0, (-32767 - 1), 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqneg_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vsmul(temp_0, (-2147483647L - 1), 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqneg_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vsmul(temp_0, (-9223372036854775807LL - 1), 1);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqneg_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint8m1_t temp_3 = vsmul(temp_0, -128, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqnegs_s32") int32_t vqnegs_s32(int32_t a);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahh_laneq_s16") int16_t vqrdmlahh_laneq_s16(int16_t a, int16_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahh_lane_s16") int16_t vqrdmlahh_lane_s16(int16_t a, int16_t b, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmlahh_s16") int16_t vqrdmlahh_s16(int16_t a, int16_t b, int16_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_laneq_s16") int16x4_t vqrdmlah_laneq_s16(int16x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_laneq_s32") int32x2_t vqrdmlah_laneq_s32(int32x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_lane_s16") int16x4_t vqrdmlah_lane_s16(int16x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_lane_s32") int32x2_t vqrdmlah_lane_s32(int32x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_laneq_s16") int16x8_t vqrdmlahq_laneq_s16(int16x8_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_laneq_s32") int32x4_t vqrdmlahq_laneq_s32(int32x4_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_lane_s16") int16x8_t vqrdmlahq_lane_s16(int16x8_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_lane_s32") int32x4_t vqrdmlahq_lane_s32(int32x4_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_s16") int16x8_t vqrdmlahq_s16(int16x8_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahq_s32") int32x4_t vqrdmlahq_s32(int32x4_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_s16") int16x4_t vqrdmlah_s16(int16x4_t a, int16x4_t b, int16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlah_s32") int32x2_t vqrdmlah_s32(int32x2_t a, int32x2_t b, int32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahs_laneq_s32") int32_t vqrdmlahs_laneq_s32(int32_t a, int32_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlahs_lane_s32") int32_t vqrdmlahs_lane_s32(int32_t a, int32_t b, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmlahs_s32") int32_t vqrdmlahs_s32(int32_t a, int32_t b, int32_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshh_laneq_s16") int16_t vqrdmlshh_laneq_s16(int16_t a, int16_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshh_lane_s16") int16_t vqrdmlshh_lane_s16(int16_t a, int16_t b, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmlshh_s16") int16_t vqrdmlshh_s16(int16_t a, int16_t b, int16_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_laneq_s16") int16x4_t vqrdmlsh_laneq_s16(int16x4_t a, int16x4_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_laneq_s32") int32x2_t vqrdmlsh_laneq_s32(int32x2_t a, int32x2_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_lane_s16") int16x4_t vqrdmlsh_lane_s16(int16x4_t a, int16x4_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_lane_s32") int32x2_t vqrdmlsh_lane_s32(int32x2_t a, int32x2_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_laneq_s16") int16x8_t vqrdmlshq_laneq_s16(int16x8_t a, int16x8_t b, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_laneq_s32") int32x4_t vqrdmlshq_laneq_s32(int32x4_t a, int32x4_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_lane_s16") int16x8_t vqrdmlshq_lane_s16(int16x8_t a, int16x8_t b, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_lane_s32") int32x4_t vqrdmlshq_lane_s32(int32x4_t a, int32x4_t b, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_s16") int16x8_t vqrdmlshq_s16(int16x8_t a, int16x8_t b, int16x8_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshq_s32") int32x4_t vqrdmlshq_s32(int32x4_t a, int32x4_t b, int32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_s16") int16x4_t vqrdmlsh_s16(int16x4_t a, int16x4_t b, int16x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlsh_s32") int32x2_t vqrdmlsh_s32(int32x2_t a, int32x2_t b, int32x2_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshs_laneq_s32") int32_t vqrdmlshs_laneq_s32(int32_t a, int32_t b, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmlshs_lane_s32") int32_t vqrdmlshs_lane_s32(int32_t a, int32_t b, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmlshs_s32") int32_t vqrdmlshs_s32(int32_t a, int32_t b, int32_t c);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhh_laneq_s16") int16_t vqrdmulhh_laneq_s16(int16_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhh_lane_s16") int16_t vqrdmulhh_lane_s16(int16_t a, int16x4_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmulhh_s16") int16_t vqrdmulhh_s16(int16_t a, int16_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulh_laneq_s16") int16x4_t vqrdmulh_laneq_s16(int16x4_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulh_laneq_s32") int32x2_t vqrdmulh_laneq_s32(int32x2_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulh_lane_s16") int16x4_t vqrdmulh_lane_s16(int16x4_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulh_lane_s32") int32x2_t vqrdmulh_lane_s32(int32x2_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqrdmulh_n_s16(int16x4_t a, int16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint32m2_t temp_1 = vwmul(temp_0, b, 4);
	vint32m2_t temp_2 = vsadd(temp_1, temp_1, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m1_t temp_5 = vnclip(temp_2, 16, 4);
	vesetxround(temp_3);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_5);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqrdmulh_n_s32(int32x2_t a, int32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint64m2_t temp_1 = vwmul(temp_0, b, 2);
	vint64m2_t temp_2 = vsadd(temp_1, temp_1, 2);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m1_t temp_5 = vnclip(temp_2, 32, 2);
	vesetxround(temp_3);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_5);
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhq_laneq_s16") int16x8_t vqrdmulhq_laneq_s16(int16x8_t a, int16x8_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhq_laneq_s32") int32x4_t vqrdmulhq_laneq_s32(int32x4_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhq_lane_s16") int16x8_t vqrdmulhq_lane_s16(int16x8_t a, int16x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhq_lane_s32") int32x4_t vqrdmulhq_lane_s32(int32x4_t a, int32x2_t v, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqrdmulhq_n_s16(int16x8_t a, int16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint32m4_t temp_1 = vwmul(temp_0, b, 8);
	vint32m4_t temp_2 = vsadd(temp_1, temp_1, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m2_t temp_5 = vnclip(temp_2, 16, 8);
	vesetxround(temp_3);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_5);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqrdmulhq_n_s32(int32x4_t a, int32_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint64m4_t temp_1 = vwmul(temp_0, b, 4);
	vint64m4_t temp_2 = vsadd(temp_1, temp_1, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m2_t temp_5 = vnclip(temp_2, 32, 4);
	vesetxround(temp_3);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_5);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqrdmulhq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint32m4_t temp_2 = vwmul(temp_0, temp_1, 8);
	vint32m4_t temp_3 = vsadd(temp_2, temp_2, 8);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TONEARESTUP);
	assert(temp_5 == 0);
	vint16m2_t temp_6 = vnclip(temp_3, 16, 8);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqrdmulhq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint64m4_t temp_2 = vwmul(temp_0, temp_1, 4);
	vint64m4_t temp_3 = vsadd(temp_2, temp_2, 4);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TONEARESTUP);
	assert(temp_5 == 0);
	vint32m2_t temp_6 = vnclip(temp_3, 32, 4);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqrdmulh_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint32m2_t temp_2 = vwmul(temp_0, temp_1, 4);
	vint32m2_t temp_3 = vsadd(temp_2, temp_2, 4);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TONEARESTUP);
	assert(temp_5 == 0);
	vint16m1_t temp_6 = vnclip(temp_3, 16, 4);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_6);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqrdmulh_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint64m2_t temp_2 = vwmul(temp_0, temp_1, 2);
	vint64m2_t temp_3 = vsadd(temp_2, temp_2, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TONEARESTUP);
	assert(temp_5 == 0);
	vint32m1_t temp_6 = vnclip(temp_3, 32, 2);
	vesetxround(temp_4);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_6);
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhs_laneq_s32") int32_t vqrdmulhs_laneq_s32(int32_t a, int32x4_t v, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrdmulhs_lane_s32") int32_t vqrdmulhs_lane_s32(int32_t a, int32x2_t v, const int lane);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrdmulhs_s32") int32_t vqrdmulhs_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshlb_s8") int8_t vqrshlb_s8(int8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshlb_u8") uint8_t vqrshlb_u8(uint8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshld_s64") int64_t vqrshld_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshld_u64") uint64_t vqrshld_u64(uint64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshlh_s16") int16_t vqrshlh_s16(int16_t a, int16_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshlh_u16") uint16_t vqrshlh_u16(uint16_t a, int16_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqrshlq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vuint16m2_t temp_2 = vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m4_t temp_5 = vssra(vwcvt_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vint16m2_t temp_6 = vncvt_x(temp_5, 8);
	vbool8_t temp_7 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), 8);
	if ((vfirst(temp_7, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint16m2_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m2(1, 8), vreinterpret_v_i16m2_u16m2(temp_1), 8), 8), 0, 8);
	vesetxround(temp_8);
	vint16m2_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i16m2(32767, 8), (-32767 - 1), 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), temp_11, vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), temp_6, 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqrshlq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint32m2_t temp_2 = vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint64m4_t temp_5 = vssra(vwcvt_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vint32m2_t temp_6 = vncvt_x(temp_5, 4);
	vbool16_t temp_7 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), 4);
	if ((vfirst(temp_7, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint32m2_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m2(1, 4), vreinterpret_v_i32m2_u32m2(temp_1), 4), 4), 0, 4);
	vesetxround(temp_8);
	vint32m2_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i32m2(2147483647L, 4), (-2147483647L - 1), 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), temp_11, vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqrshlq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint64m2_t temp_2 = vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2);
	vbool32_t temp_3 = vmseq(temp_2, 64, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vint64m2_t temp_6 = vssra(temp_3, temp_0, temp_0, 1, 2);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vint64m2_t temp_9 = vssra(temp_6, vsub(temp_3, temp_2, temp_2, 1, 2), 2);
	vesetxround(temp_7);
	vbool32_t temp_10 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), 2);
	vint64m2_t temp_11 = vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2);
	vbool32_t temp_12 = vmsne(temp_0, vsra(temp_11, vreinterpret_v_i64m2_u64m2(temp_1), 2), 2);
	vint64m2_t temp_13 = vmerge(temp_12, temp_11, (-9223372036854775807LL - 1), 2);
	if ((vfirst(vmor(temp_12, temp_10, 2), 2) != -1))
	{
		vesetxsat(1);
	}
	vint64m2_t temp_14 = vmerge(temp_10, vnot(vmsge(temp_12, temp_12, temp_0, 0, 2), temp_13, temp_13, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i64m2(9223372036854775807LL, 2), (-9223372036854775807LL - 1), 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), temp_14, vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), temp_9, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqrshlq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m4_t temp_5 = vssra(vwcvt_x(temp_0, 16), vwcvtu_x(temp_2, 16), 16);
	vesetxround(temp_3);
	vint8m2_t temp_6 = vncvt_x(temp_5, 16);
	vbool4_t temp_7 = vmand(vmsne(temp_0, 0, 16), vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), 16);
	if ((vfirst(temp_7, 16) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint8m2_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m2(1, 16), vreinterpret_v_i8m2_u8m2(temp_1), 16), 16), 0, 16);
	vesetxround(temp_8);
	vint8m2_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 16), vmv_v_x_i8m2(127, 16), -128, 16), 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), temp_11, vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), temp_6, 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqrshlq_u16(uint16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vuint16m2_t temp_2 = vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vuint16m2_t temp_6 = vncvt_x(temp_5, 8);
	vbool8_t temp_7 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), 8);
	if ((vfirst(temp_7, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint16m2_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m2(1, 8), vreinterpret_v_i16m2_u16m2(temp_1), 8), 8), 0, 8);
	vesetxround(temp_8);
	vuint16m2_t temp_11 = vmerge(temp_7, temp_10, 65535U, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), temp_11, vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), temp_6, 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqrshlq_u32(uint32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint32m2_t temp_2 = vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint64m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vuint32m2_t temp_6 = vncvt_x(temp_5, 4);
	vbool16_t temp_7 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), 4);
	if ((vfirst(temp_7, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint32m2_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m2(1, 4), vreinterpret_v_i32m2_u32m2(temp_1), 4), 4), 0, 4);
	vesetxround(temp_8);
	vuint32m2_t temp_11 = vmerge(temp_7, temp_10, 4294967295UL, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), temp_11, vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vqrshlq_u64(uint64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint64m2_t temp_2 = vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2);
	vbool32_t temp_3 = vmseq(temp_2, 64, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vuint64m2_t temp_6 = vssrl(temp_3, temp_0, temp_0, 1, 2);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vuint64m2_t temp_9 = vssrl(temp_6, vsub(temp_3, temp_2, temp_2, 1, 2), 2);
	vesetxround(temp_7);
	vbool32_t temp_10 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), 2);
	vuint64m2_t temp_11 = vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2);
	vbool32_t temp_12 = vmsne(temp_0, vsrl(temp_11, vreinterpret_v_i64m2_u64m2(temp_1), 2), 2);
	if ((vfirst(vmor(temp_12, temp_10, 2), 2) != -1))
	{
		vesetxsat(1);
	}
	vuint64m2_t temp_13 = vmerge(temp_10, vmerge(temp_12, temp_11, 18446744073709551615ULL, 2), 18446744073709551615ULL, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), temp_13, vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), temp_9, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqrshlq_u8(uint8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 16), vwcvtu_x(temp_2, 16), 16);
	vesetxround(temp_3);
	vuint8m2_t temp_6 = vncvt_x(temp_5, 16);
	vbool4_t temp_7 = vmand(vmsne(temp_0, 0, 16), vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), 16);
	if ((vfirst(temp_7, 16) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint8m2_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m2(1, 16), vreinterpret_v_i8m2_u8m2(temp_1), 16), 16), 0, 16);
	vesetxround(temp_8);
	vuint8m2_t temp_11 = vmerge(temp_7, temp_10, 255, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), temp_11, vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), temp_6, 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqrshl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint16m1_t temp_2 = vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m2_t temp_5 = vssra(vwcvt_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vint16m1_t temp_6 = vncvt_x(temp_5, 4);
	vbool16_t temp_7 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), 4);
	if ((vfirst(temp_7, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint16m1_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m1(1, 4), vreinterpret_v_i16m1_u16m1(temp_1), 4), 4), 0, 4);
	vesetxround(temp_8);
	vint16m1_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i16m1(32767, 4), (-32767 - 1), 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), temp_11, vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqrshl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint32m1_t temp_2 = vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint64m2_t temp_5 = vssra(vwcvt_x(temp_0, 2), vwcvtu_x(temp_2, 2), 2);
	vesetxround(temp_3);
	vint32m1_t temp_6 = vncvt_x(temp_5, 2);
	vbool32_t temp_7 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), 2);
	if ((vfirst(temp_7, 2) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint32m1_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m1(1, 2), vreinterpret_v_i32m1_u32m1(temp_1), 2), 2), 0, 2);
	vesetxround(temp_8);
	vint32m1_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i32m1(2147483647L, 2), (-2147483647L - 1), 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), temp_11, vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), temp_6, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqrshl_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vuint64m1_t temp_2 = vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1);
	vbool64_t temp_3 = vmseq(temp_2, 64, 1);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vint64m1_t temp_6 = vssra(temp_3, temp_0, temp_0, 1, 1);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vint64m1_t temp_9 = vssra(temp_6, vsub(temp_3, temp_2, temp_2, 1, 1), 1);
	vesetxround(temp_7);
	vbool64_t temp_10 = vmand(vmsne(temp_0, 0, 1), vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), 1);
	vint64m1_t temp_11 = vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1);
	vbool64_t temp_12 = vmsne(temp_0, vsra(temp_11, vreinterpret_v_i64m1_u64m1(temp_1), 1), 1);
	vint64m1_t temp_13 = vmerge(temp_12, temp_11, (-9223372036854775807LL - 1), 1);
	if ((vfirst(vmor(temp_12, temp_10, 1), 1) != -1))
	{
		vesetxsat(1);
	}
	vint64m1_t temp_14 = vmerge(temp_10, vnot(vmsge(temp_12, temp_12, temp_0, 0, 1), temp_13, temp_13, 1), vmerge(vmslt(temp_0, 0, 1), vmv_v_x_i64m1(9223372036854775807LL, 1), (-9223372036854775807LL - 1), 1), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), temp_14, vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), temp_9, 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqrshl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m2_t temp_5 = vssra(vwcvt_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vint8m1_t temp_6 = vncvt_x(temp_5, 8);
	vbool8_t temp_7 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), 8);
	if ((vfirst(temp_7, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vint8m1_t temp_10 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m1(1, 8), vreinterpret_v_i8m1_u8m1(temp_1), 8), 8), 0, 8);
	vesetxround(temp_8);
	vint8m1_t temp_11 = vmerge(temp_7, temp_10, vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i8m1(127, 8), -128, 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), temp_11, vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), temp_6, 0, 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshls_s32") int32_t vqrshls_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqrshls_u32") uint32_t vqrshls_u32(uint32_t a, int32_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqrshl_u16(uint16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint16m1_t temp_2 = vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vuint16m1_t temp_6 = vncvt_x(temp_5, 4);
	vbool16_t temp_7 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), 4);
	if ((vfirst(temp_7, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint16m1_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m1(1, 4), vreinterpret_v_i16m1_u16m1(temp_1), 4), 4), 0, 4);
	vesetxround(temp_8);
	vuint16m1_t temp_11 = vmerge(temp_7, temp_10, 65535U, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), temp_11, vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqrshl_u32(uint32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint32m1_t temp_2 = vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint64m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 2), vwcvtu_x(temp_2, 2), 2);
	vesetxround(temp_3);
	vuint32m1_t temp_6 = vncvt_x(temp_5, 2);
	vbool32_t temp_7 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), 2);
	if ((vfirst(temp_7, 2) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint32m1_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m1(1, 2), vreinterpret_v_i32m1_u32m1(temp_1), 2), 2), 0, 2);
	vesetxround(temp_8);
	vuint32m1_t temp_11 = vmerge(temp_7, temp_10, 4294967295UL, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), temp_11, vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), temp_6, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vqrshl_u64(uint64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vuint64m1_t temp_2 = vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1);
	vbool64_t temp_3 = vmseq(temp_2, 64, 1);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vuint64m1_t temp_6 = vssrl(temp_3, temp_0, temp_0, 1, 1);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vuint64m1_t temp_9 = vssrl(temp_6, vsub(temp_3, temp_2, temp_2, 1, 1), 1);
	vesetxround(temp_7);
	vbool64_t temp_10 = vmand(vmsne(temp_0, 0, 1), vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), 1);
	vuint64m1_t temp_11 = vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1);
	vbool64_t temp_12 = vmsne(temp_0, vsrl(temp_11, vreinterpret_v_i64m1_u64m1(temp_1), 1), 1);
	if ((vfirst(vmor(temp_12, temp_10, 1), 1) != -1))
	{
		vesetxsat(1);
	}
	vuint64m1_t temp_13 = vmerge(temp_10, vmerge(temp_12, temp_11, 18446744073709551615ULL, 1), 18446744073709551615ULL, 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), temp_13, vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), temp_9, 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqrshl_u8(uint8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vuint8m1_t temp_6 = vncvt_x(temp_5, 8);
	vbool8_t temp_7 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), 8);
	if ((vfirst(temp_7, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_8 = vegetxround();
	int temp_9 = vesetxround(VE_TONEARESTUP);
	assert(temp_9 == 0);
	vuint8m1_t temp_10 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m1(1, 8), vreinterpret_v_i8m1_u8m1(temp_1), 8), 8), 0, 8);
	vesetxround(temp_8);
	vuint8m1_t temp_11 = vmerge(temp_7, temp_10, 255, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), temp_11, vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), temp_6, 0, 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshrnd_n_s64") int32_t vqrshrnd_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqrshrnd_n_u64") uint32_t vqrshrnd_n_u64(uint64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_s16") int8x16_t vqrshrn_high_n_s16(int8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_s32") int16x8_t vqrshrn_high_n_s32(int16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_s64") int32x4_t vqrshrn_high_n_s64(int32x2_t r, int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_u16") uint8x16_t vqrshrn_high_n_u16(uint8x8_t r, uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_u32") uint16x8_t vqrshrn_high_n_u32(uint16x4_t r, uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_high_n_u64") uint32x4_t vqrshrn_high_n_u64(uint32x2_t r, uint64x2_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshrnh_n_s16") int8_t vqrshrnh_n_s16(int16_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqrshrnh_n_u16") uint8_t vqrshrnh_n_u16(uint16_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_s16") int8x8_t vqrshrn_n_s16(int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_s32") int16x4_t vqrshrn_n_s32(int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_s64") int32x2_t vqrshrn_n_s64(int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_u16") uint8x8_t vqrshrn_n_u16(uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_u32") uint16x4_t vqrshrn_n_u32(uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrn_n_u64") uint32x2_t vqrshrn_n_u64(uint64x2_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshrns_n_s32") int16_t vqrshrns_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqrshrns_n_u32") uint16_t vqrshrns_n_u32(uint32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqrshrund_n_s64") uint32_t vqrshrund_n_s64(int64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrun_high_n_s16") uint8x16_t vqrshrun_high_n_s16(uint8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrun_high_n_s32") uint16x8_t vqrshrun_high_n_s32(uint16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqrshrun_high_n_s64") uint32x4_t vqrshrun_high_n_s64(uint32x2_t r, int64x2_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshrunh_n_s16") uint8_t vqrshrunh_n_s16(int16_t a, const int n);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqrshrun_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vuint16m2_t temp_1 = vreinterpret_v_i16m2_u16m2(vmax(temp_0, 0, 8));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vnclipu(temp_1, n, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqrshrun_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vuint32m2_t temp_1 = vreinterpret_v_i32m2_u32m2(vmax(temp_0, 0, 4));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vnclipu(temp_1, n, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqrshrun_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vuint64m2_t temp_1 = vreinterpret_v_i64m2_u64m2(vmax(temp_0, 0, 2));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vnclipu(temp_1, n, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqrshruns_n_s32") uint16_t vqrshruns_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlb_n_s8") int8_t vqshlb_n_s8(int8_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlb_n_u8") uint8_t vqshlb_n_u8(uint8_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlb_s8") int8_t vqshlb_s8(int8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqshlb_u8") uint8_t vqshlb_u8(uint8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqshld_n_s64") int64_t vqshld_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshld_n_u64") uint64_t vqshld_n_u64(uint64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshld_s64") int64_t vqshld_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqshld_u64") uint64_t vqshld_u64(uint64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqshlh_n_s16") int16_t vqshlh_n_s16(int16_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlh_n_u16") uint16_t vqshlh_n_u16(uint16_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlh_s16") int16_t vqshlh_s16(int16_t a, int16_t b);
NEON2RVV_NOT_IMPLEMENT("vqshlh_u16") uint16_t vqshlh_u16(uint16_t a, int16_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqshl_n_s16(int16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m1(1, 4), n, 4), 4), 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqshl_n_s32(int32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m1(1, 2), n, 2), 2), 0, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqshl_n_s64(int64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = vsll(temp_0, n, 1);
	vbool64_t temp_2 = vmsne(temp_0, vsra(temp_1, n, 1), 1);
	if ((vfirst(temp_2, 1) != -1))
	{
		vesetxsat(1);
	}
	vint64m1_t temp_3 = vmerge(temp_2, temp_1, (-9223372036854775807LL - 1), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vnot(vmsge(temp_2, temp_2, temp_0, 0, 1), temp_3, temp_3, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqshl_n_s8(int8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint8m1_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m1(1, 8), n, 8), 8), 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqshl_n_u16(uint16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m1(1, 4), n, 4), 4), 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqshl_n_u32(uint32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m1(1, 2), n, 2), 2), 0, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vqshl_n_u64(uint64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = vsll(temp_0, n, 1);
	vbool64_t temp_2 = vmsne(temp_0, vsrl(temp_1, n, 1), 1);
	if ((vfirst(temp_2, 1) != -1))
	{
		vesetxsat(1);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(temp_2, temp_1, 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqshl_n_u8(uint8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint8m1_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m1(1, 8), n, 8), 8), 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqshlq_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m2(1, 8), n, 8), 8), 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqshlq_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m2(1, 4), n, 4), 4), 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqshlq_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = vsll(temp_0, n, 2);
	vbool32_t temp_2 = vmsne(temp_0, vsra(temp_1, n, 2), 2);
	if ((vfirst(temp_2, 2) != -1))
	{
		vesetxsat(1);
	}
	vint64m2_t temp_3 = vmerge(temp_2, temp_1, (-9223372036854775807LL - 1), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vnot(vmsge(temp_2, temp_2, temp_0, 0, 2), temp_3, temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqshlq_n_s8(int8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint8m2_t temp_3 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m2(1, 16), n, 16), 16), 0, 16);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqshlq_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m2(1, 8), n, 8), 8), 0, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqshlq_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m2(1, 4), n, 4), 4), 0, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vqshlq_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = vsll(temp_0, n, 2);
	vbool32_t temp_2 = vmsne(temp_0, vsrl(temp_1, n, 2), 2);
	if ((vfirst(temp_2, 2) != -1))
	{
		vesetxsat(1);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(temp_2, temp_1, 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqshlq_n_u8(uint8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint8m2_t temp_3 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m2(1, 16), n, 16), 16), 0, 16);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqshlq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vbool8_t temp_2 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), 8);
	if ((vfirst(temp_2, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m2_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m2(1, 8), vreinterpret_v_i16m2_u16m2(temp_1), 8), 8), 0, 8);
	vesetxround(temp_3);
	vint16m2_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i16m2(32767, 8), (-32767 - 1), 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), temp_6, vsra(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i16m2(0, 8), -1, 8), temp_0, vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8), 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqshlq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vbool16_t temp_2 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), 4);
	if ((vfirst(temp_2, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m2_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m2(1, 4), vreinterpret_v_i32m2_u32m2(temp_1), 4), 4), 0, 4);
	vesetxround(temp_3);
	vint32m2_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i32m2(2147483647L, 4), (-2147483647L - 1), 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), temp_6, vsra(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i32m2(0, 4), -1, 4), temp_0, vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4), 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqshlq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vbool32_t temp_2 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), 2);
	vint64m2_t temp_3 = vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2);
	vbool32_t temp_4 = vmsne(temp_0, vsra(temp_3, vreinterpret_v_i64m2_u64m2(temp_1), 2), 2);
	vint64m2_t temp_5 = vmerge(temp_4, temp_3, (-9223372036854775807LL - 1), 2);
	if ((vfirst(vmor(temp_4, temp_2, 2), 2) != -1))
	{
		vesetxsat(1);
	}
	vint64m2_t temp_6 = vmerge(temp_2, vnot(vmsge(temp_4, temp_4, temp_0, 0, 2), temp_5, temp_5, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i64m2(9223372036854775807LL, 2), (-9223372036854775807LL - 1), 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), temp_6, vsra(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i64m2(0, 2), -1, 2), temp_0, vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2), 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqshlq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vbool4_t temp_2 = vmand(vmsne(temp_0, 0, 16), vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), 16);
	if ((vfirst(temp_2, 16) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint8m2_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m2(1, 16), vreinterpret_v_i8m2_u8m2(temp_1), 16), 16), 0, 16);
	vesetxround(temp_3);
	vint8m2_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 16), vmv_v_x_i8m2(127, 16), -128, 16), 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), temp_6, vsra(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), vmerge(vmslt(temp_0, 0, 16), vmv_v_x_i8m2(0, 16), -1, 16), temp_0, vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16), 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqshlq_u16(uint16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vbool8_t temp_2 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), 8);
	if ((vfirst(temp_2, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m2_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m2(1, 8), vreinterpret_v_i16m2_u16m2(temp_1), 8), 8), 0, 8);
	vesetxround(temp_3);
	vuint16m2_t temp_6 = vmerge(temp_2, temp_5, 65535U, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), temp_6, vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 241, 8), vsrl(temp_0, vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8), 8), 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqshlq_u32(uint32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vbool16_t temp_2 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), 4);
	if ((vfirst(temp_2, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m2_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m2(1, 4), vreinterpret_v_i32m2_u32m2(temp_1), 4), 4), 0, 4);
	vesetxround(temp_3);
	vuint32m2_t temp_6 = vmerge(temp_2, temp_5, 4294967295UL, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), temp_6, vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 225, 4), vsrl(temp_0, vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4), 4), 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vqshlq_u64(uint64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vbool32_t temp_2 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), 2);
	vuint64m2_t temp_3 = vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2);
	vbool32_t temp_4 = vmsne(temp_0, vsrl(temp_3, vreinterpret_v_i64m2_u64m2(temp_1), 2), 2);
	if ((vfirst(vmor(temp_4, temp_2, 2), 2) != -1))
	{
		vesetxsat(1);
	}
	vuint64m2_t temp_5 = vmerge(temp_2, vmerge(temp_4, temp_3, 18446744073709551615ULL, 2), 18446744073709551615ULL, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), temp_5, vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 193, 2), vsrl(temp_0, vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2), 2), 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqshlq_u8(uint8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vbool4_t temp_2 = vmand(vmsne(temp_0, 0, 16), vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), 16);
	if ((vfirst(temp_2, 16) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint8m2_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m2(1, 16), vreinterpret_v_i8m2_u8m2(temp_1), 16), 16), 0, 16);
	vesetxround(temp_3);
	vuint8m2_t temp_6 = vmerge(temp_2, temp_5, 255, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), temp_6, vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 249, 16), vsrl(temp_0, vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16), 16), 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqshl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vbool16_t temp_2 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), 4);
	if ((vfirst(temp_2, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m1_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u16m1(1, 4), vreinterpret_v_i16m1_u16m1(temp_1), 4), 4), 0, 4);
	vesetxround(temp_3);
	vint16m1_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i16m1(32767, 4), (-32767 - 1), 4), 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), temp_6, vsra(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i16m1(0, 4), -1, 4), temp_0, vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4), 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqshl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vbool32_t temp_2 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), 2);
	if ((vfirst(temp_2, 2) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m1_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u32m1(1, 2), vreinterpret_v_i32m1_u32m1(temp_1), 2), 2), 0, 2);
	vesetxround(temp_3);
	vint32m1_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i32m1(2147483647L, 2), (-2147483647L - 1), 2), 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), temp_6, vsra(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i32m1(0, 2), -1, 2), temp_0, vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2), 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqshl_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vbool64_t temp_2 = vmand(vmsne(temp_0, 0, 1), vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), 1);
	vint64m1_t temp_3 = vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1);
	vbool64_t temp_4 = vmsne(temp_0, vsra(temp_3, vreinterpret_v_i64m1_u64m1(temp_1), 1), 1);
	vint64m1_t temp_5 = vmerge(temp_4, temp_3, (-9223372036854775807LL - 1), 1);
	if ((vfirst(vmor(temp_4, temp_2, 1), 1) != -1))
	{
		vesetxsat(1);
	}
	vint64m1_t temp_6 = vmerge(temp_2, vnot(vmsge(temp_4, temp_4, temp_0, 0, 1), temp_5, temp_5, 1), vmerge(vmslt(temp_0, 0, 1), vmv_v_x_i64m1(9223372036854775807LL, 1), (-9223372036854775807LL - 1), 1), 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), temp_6, vsra(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), vmerge(vmslt(temp_0, 0, 1), vmv_v_x_i64m1(0, 1), -1, 1), temp_0, vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1), 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqshl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vbool8_t temp_2 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), 8);
	if ((vfirst(temp_2, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint8m1_t temp_5 = vnclip(vwmulsu(temp_0, vsll(vmv_v_x_u8m1(1, 8), vreinterpret_v_i8m1_u8m1(temp_1), 8), 8), 0, 8);
	vesetxround(temp_3);
	vint8m1_t temp_6 = vmerge(temp_2, temp_5, vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i8m1(127, 8), -128, 8), 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), temp_6, vsra(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i8m1(0, 8), -1, 8), temp_0, vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8), 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqshls_n_s32") int32_t vqshls_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshls_n_u32") uint32_t vqshls_n_u32(uint32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshls_s32") int32_t vqshls_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqshls_u32") uint32_t vqshls_u32(uint32_t a, int32_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqshl_u16(uint16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vbool16_t temp_2 = vmand(vmsne(temp_0, 0, 4), vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), 4);
	if ((vfirst(temp_2, 4) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m1_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u16m1(1, 4), vreinterpret_v_i16m1_u16m1(temp_1), 4), 4), 0, 4);
	vesetxround(temp_3);
	vuint16m1_t temp_6 = vmerge(temp_2, temp_5, 65535U, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), temp_6, vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 241, 4), vsrl(temp_0, vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4), 4), 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqshl_u32(uint32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vbool32_t temp_2 = vmand(vmsne(temp_0, 0, 2), vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), 2);
	if ((vfirst(temp_2, 2) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m1_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u32m1(1, 2), vreinterpret_v_i32m1_u32m1(temp_1), 2), 2), 0, 2);
	vesetxround(temp_3);
	vuint32m1_t temp_6 = vmerge(temp_2, temp_5, 4294967295UL, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), temp_6, vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 225, 2), vsrl(temp_0, vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2), 2), 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vqshl_u64(uint64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vbool64_t temp_2 = vmand(vmsne(temp_0, 0, 1), vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), 1);
	vuint64m1_t temp_3 = vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1);
	vbool64_t temp_4 = vmsne(temp_0, vsrl(temp_3, vreinterpret_v_i64m1_u64m1(temp_1), 1), 1);
	if ((vfirst(vmor(temp_4, temp_2, 1), 1) != -1))
	{
		vesetxsat(1);
	}
	vuint64m1_t temp_5 = vmerge(temp_2, vmerge(temp_4, temp_3, 18446744073709551615ULL, 1), 18446744073709551615ULL, 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), temp_5, vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 193, 1), vsrl(temp_0, vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1), 1), 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqshl_u8(uint8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vbool8_t temp_2 = vmand(vmsne(temp_0, 0, 8), vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), 8);
	if ((vfirst(temp_2, 8) != -1))
	{
		vesetxsat(1);
	}
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint8m1_t temp_5 = vnclipu(vwmulu(temp_0, vsll(vmv_v_x_u8m1(1, 8), vreinterpret_v_i8m1_u8m1(temp_1), 8), 8), 0, 8);
	vesetxround(temp_3);
	vuint8m1_t temp_6 = vmerge(temp_2, temp_5, 255, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), temp_6, vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 249, 8), vsrl(temp_0, vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8), 8), 0, 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqshlub_n_s8") uint8_t vqshlub_n_s8(int8_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshlud_n_s64") uint64_t vqshlud_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshluh_n_s16") uint16_t vqshluh_n_s16(int16_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshlu_n_s16") uint16x4_t vqshlu_n_s16(int16x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshlu_n_s32") uint32x2_t vqshlu_n_s32(int32x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshlu_n_s64") uint64x1_t vqshlu_n_s64(int64x1_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshlu_n_s8") uint8x8_t vqshlu_n_s8(int8x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshluq_n_s16") uint16x8_t vqshluq_n_s16(int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshluq_n_s32") uint32x4_t vqshluq_n_s32(int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshluq_n_s64") uint64x2_t vqshluq_n_s64(int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshluq_n_s8") uint8x16_t vqshluq_n_s8(int8x16_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqshlus_n_s32") uint32_t vqshlus_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshrnd_n_s64") int32_t vqshrnd_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshrnd_n_u64") uint32_t vqshrnd_n_u64(uint64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_s16") int8x16_t vqshrn_high_n_s16(int8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_s32") int16x8_t vqshrn_high_n_s32(int16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_s64") int32x4_t vqshrn_high_n_s64(int32x2_t r, int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_u16") uint8x16_t vqshrn_high_n_u16(uint8x8_t r, uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_u32") uint16x8_t vqshrn_high_n_u32(uint16x4_t r, uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrn_high_n_u64") uint32x4_t vqshrn_high_n_u64(uint32x2_t r, uint64x2_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqshrnh_n_s16") int8_t vqshrnh_n_s16(int16_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshrnh_n_u16") uint8_t vqshrnh_n_u16(uint16_t a, const int n);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqshrn_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint8m1_t temp_3 = vnclip(temp_0, n, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqshrn_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vnclip(temp_0, n, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqshrn_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vnclip(temp_0, n, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqshrn_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vuint8m1_t temp_3 = vnclipu(temp_0, n, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqshrn_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vuint16m1_t temp_3 = vnclipu(temp_0, n, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqshrn_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_DOWNWARD);
	assert(temp_2 == 0);
	vuint32m1_t temp_3 = vnclipu(temp_0, n, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqshrns_n_s32") int16_t vqshrns_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshrns_n_u32") uint16_t vqshrns_n_u32(uint32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqshrund_n_s64") uint32_t vqshrund_n_s64(int64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrun_high_n_s16") uint8x16_t vqshrun_high_n_s16(uint8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrun_high_n_s32") uint16x8_t vqshrun_high_n_s32(uint16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqshrun_high_n_s64") uint32x4_t vqshrun_high_n_s64(uint32x2_t r, int64x2_t a, const int n);
#endif
NEON2RVV_NOT_IMPLEMENT("vqshrunh_n_s16") uint8_t vqshrunh_n_s16(int16_t a, const int n);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqshrun_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vuint16m2_t temp_1 = vreinterpret_v_i16m2_u16m2(vmax(temp_0, 0, 8));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vnclipu(temp_1, n, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqshrun_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vuint32m2_t temp_1 = vreinterpret_v_i32m2_u32m2(vmax(temp_0, 0, 4));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vnclipu(temp_1, n, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqshrun_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vuint64m2_t temp_1 = vreinterpret_v_i64m2_u64m2(vmax(temp_0, 0, 2));
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_DOWNWARD);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vnclipu(temp_1, n, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqshruns_n_s32") uint16_t vqshruns_n_s32(int32_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vqsubb_s8") int8_t vqsubb_s8(int8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubb_u8") uint8_t vqsubb_u8(uint8_t a, uint8_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubd_s64") int64_t vqsubd_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubd_u64") uint64_t vqsubd_u64(uint64_t a, uint64_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubh_s16") int16_t vqsubh_s16(int16_t a, int16_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubh_u16") uint16_t vqsubh_u16(uint16_t a, uint16_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vqsubq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vssub(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vqsubq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vssub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vqsubq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vssub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqsubq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vssub(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vqsubq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vssubu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vqsubq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vssubu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vqsubq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vssubu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqsubq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vssubu(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vqsub_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vssub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vqsub_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vssub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vqsub_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vssub(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqsub_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vssub(temp_0, temp_1, 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vqsubs_s32") int32_t vqsubs_s32(int32_t a, int32_t b);
NEON2RVV_NOT_IMPLEMENT("vqsubs_u32") uint32_t vqsubs_u32(uint32_t a, uint32_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vqsub_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vssubu(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vqsub_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vssubu(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vqsub_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vssubu(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqsub_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vssubu(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqtbl1q_s8(int8x16_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vle8_v_i8m2(((const int8_t *)((&t))), 16);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(idx);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmsgtu(temp_1, 15, 16), vrgather(temp_0, temp_1, 16), 0, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqtbl1q_u8(uint8x16_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vle8_v_u8m2(((const uint8_t *)((&t))), 16);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(idx);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(temp_1, 15, 16), vrgather(temp_0, temp_1, 16), 0, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqtbl1_s8(int8x16_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vle8_v_i8m2(((const int8_t *)((&t))), 16);
	vuint8m2_t temp_1 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vmerge(vmsgtu(temp_1, 15, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqtbl1_u8(uint8x16_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vle8_v_u8m2(((const uint8_t *)((&t))), 16);
	vuint8m2_t temp_1 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vmerge(vmsgtu(temp_1, 15, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqtbl2q_s8(int8x16x2_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vle8_v_i8m4(((const int8_t *)((&t))), 32);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m2_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vlmul_trunc_v_i8m4_i8m2(vmerge(vmsgtu(temp_1, 31, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqtbl2q_u8(uint8x16x2_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vle8_v_u8m4(((const uint8_t *)((&t))), 32);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m2_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vlmul_trunc_v_u8m4_u8m2(vmerge(vmsgtu(temp_1, 31, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqtbl2_s8(int8x16x2_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vle8_v_i8m4(((const int8_t *)((&t))), 32);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m4_i8m1(vmerge(vmsgtu(temp_1, 31, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqtbl2_u8(uint8x16x2_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vle8_v_u8m4(((const uint8_t *)((&t))), 32);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m4_u8m1(vmerge(vmsgtu(temp_1, 31, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqtbl3q_s8(int8x16x3_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m8_t temp_0 = vle8_v_i8m8(((const int8_t *)((&t))), 48);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m2_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vlmul_trunc_v_i8m8_i8m2(vmerge(vmsgtu(temp_1, 47, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqtbl3q_u8(uint8x16x3_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m8_t temp_0 = vle8_v_u8m8(((const uint8_t *)((&t))), 48);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m2_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vlmul_trunc_v_u8m8_u8m2(vmerge(vmsgtu(temp_1, 47, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqtbl3_s8(int8x16x3_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m8_t temp_0 = vle8_v_i8m8(((const int8_t *)((&t))), 48);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m1_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m8_i8m1(vmerge(vmsgtu(temp_1, 47, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqtbl3_u8(uint8x16x3_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m8_t temp_0 = vle8_v_u8m8(((const uint8_t *)((&t))), 48);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m1_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m8_u8m1(vmerge(vmsgtu(temp_1, 47, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vqtbl4q_s8(int8x16x4_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m8_t temp_0 = vle8_v_i8m8(((const int8_t *)((&t))), 64);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m2_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vlmul_trunc_v_i8m8_i8m2(vmerge(vmsgtu(temp_1, 63, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vqtbl4q_u8(uint8x16x4_t t, uint8x16_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m8_t temp_0 = vle8_v_u8m8(((const uint8_t *)((&t))), 64);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m2_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m2(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vlmul_trunc_v_u8m8_u8m2(vmerge(vmsgtu(temp_1, 63, 16), vrgather(temp_0, temp_1, 16), 0, 16)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vqtbl4_s8(int8x16x4_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m8_t temp_0 = vle8_v_i8m8(((const int8_t *)((&t))), 64);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m1_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m8_i8m1(vmerge(vmsgtu(temp_1, 63, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vqtbl4_u8(uint8x16x4_t t, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m8_t temp_0 = vle8_v_u8m8(((const uint8_t *)((&t))), 64);
	vuint8m8_t temp_1 = vlmul_ext_v_u8m1_u8m8(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m8_u8m1(vmerge(vmsgtu(temp_1, 63, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx1q_s8") int8x16_t vqtbx1q_s8(int8x16_t a, int8x16_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx1q_u8") uint8x16_t vqtbx1q_u8(uint8x16_t a, uint8x16_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx1_s8") int8x8_t vqtbx1_s8(int8x8_t a, int8x16_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx1_u8") uint8x8_t vqtbx1_u8(uint8x8_t a, uint8x16_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx2q_s8") int8x16_t vqtbx2q_s8(int8x16_t a, int8x16x2_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx2q_u8") uint8x16_t vqtbx2q_u8(uint8x16_t a, uint8x16x2_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx2_s8") int8x8_t vqtbx2_s8(int8x8_t a, int8x16x2_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx2_u8") uint8x8_t vqtbx2_u8(uint8x8_t a, uint8x16x2_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx3q_s8") int8x16_t vqtbx3q_s8(int8x16_t a, int8x16x3_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx3q_u8") uint8x16_t vqtbx3q_u8(uint8x16_t a, uint8x16x3_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx3_s8") int8x8_t vqtbx3_s8(int8x8_t a, int8x16x3_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx3_u8") uint8x8_t vqtbx3_u8(uint8x8_t a, uint8x16x3_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx4q_s8") int8x16_t vqtbx4q_s8(int8x16_t a, int8x16x4_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx4q_u8") uint8x16_t vqtbx4q_u8(uint8x16_t a, uint8x16x4_t t, uint8x16_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx4_s8") int8x8_t vqtbx4_s8(int8x8_t a, int8x16x4_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vqtbx4_u8") uint8x8_t vqtbx4_u8(uint8x8_t a, uint8x16x4_t t, uint8x8_t idx);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_s16") int8x16_t vraddhn_high_s16(int8x8_t r, int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_s32") int16x8_t vraddhn_high_s32(int16x4_t r, int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_s64") int32x4_t vraddhn_high_s64(int32x2_t r, int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_u16") uint8x16_t vraddhn_high_u16(uint8x8_t r, uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_u32") uint16x8_t vraddhn_high_u32(uint16x4_t r, uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_high_u64") uint32x4_t vraddhn_high_u64(uint32x2_t r, uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_s16") int8x8_t vraddhn_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_s32") int16x4_t vraddhn_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_s64") int32x2_t vraddhn_s64(int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_u16") uint8x8_t vraddhn_u16(uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_u32") uint16x4_t vraddhn_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vraddhn_u64") uint32x2_t vraddhn_u64(uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrax1q_u64") uint64x2_t vrax1q_u64(uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrbitq_s8") int8x16_t vrbitq_s8(int8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrbitq_u8") uint8x16_t vrbitq_u8(uint8x16_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrbit_s8") int8x8_t vrbit_s8(int8x8_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrbit_u8") uint8x8_t vrbit_u8(uint8x8_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecped_f64") float64_t vrecped_f64(float64_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrecpe_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfrec7(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrecpe_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfrec7(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrecpe_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfrec7(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrecpeh_f16") float16_t vrecpeh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrecpeq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfrec7(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrecpeq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfrec7(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrecpeq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfrec7(temp_0, 2));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrecpeq_u32") uint32x4_t vrecpeq_u32(uint32x4_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecpes_f32") float32_t vrecpes_f32(float32_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrecpe_u32") uint32x2_t vrecpe_u32(uint32x2_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecpsd_f64") float64_t vrecpsd_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrecps_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vuint16m1_t temp_2 = vand(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 153, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 4), vmseq(temp_2, 17, 4), 4), vmseq(temp_2, 9, 4), 4), vfnmsac(vfmv_v_f_f16m1(2, 4), temp_0, temp_1, 4), 2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrecps_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vuint32m1_t temp_2 = vand(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 153, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 2), vmseq(temp_2, 17, 2), 2), vmseq(temp_2, 9, 2), 2), vfnmsac(vfmv_v_f_f32m1(2, 2), temp_0, temp_1, 2), 2, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrecps_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vuint64m1_t temp_2 = vand(vor(vfclass(temp_0, 1), vfclass(temp_1, 1), 1), 153, 1);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 1), vmseq(temp_2, 17, 1), 1), vmseq(temp_2, 9, 1), 1), vfnmsac(vfmv_v_f_f64m1(2, 1), temp_0, temp_1, 1), 2, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrecpsh_f16") float16_t vrecpsh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrecpsq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vuint16m2_t temp_2 = vand(vor(vfclass(temp_0, 8), vfclass(temp_1, 8), 8), 153, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 8), vmseq(temp_2, 17, 8), 8), vmseq(temp_2, 9, 8), 8), vfnmsac(vfmv_v_f_f16m2(2, 8), temp_0, temp_1, 8), 2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrecpsq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vuint32m2_t temp_2 = vand(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 153, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 4), vmseq(temp_2, 17, 4), 4), vmseq(temp_2, 9, 4), 4), vfnmsac(vfmv_v_f_f32m2(2, 4), temp_0, temp_1, 4), 2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrecpsq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vuint64m2_t temp_2 = vand(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 153, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 2), vmseq(temp_2, 17, 2), 2), vmseq(temp_2, 9, 2), 2), vfnmsac(vfmv_v_f_f64m2(2, 2), temp_0, temp_1, 2), 2, 2));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecpss_f32") float32_t vrecpss_f32(float32_t a, float32_t b);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecpxd_f64") float64_t vrecpxd_f64(float64_t a);
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrecpxh_f16") float16_t vrecpxh_f16(float16_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrecpxs_f32") float32_t vrecpxs_f32(float32_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vreinterpret_f16_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vreinterpret_f32_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vreinterpret_f64_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x1_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vreinterpretq_f16_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float16x8_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vreinterpretq_f32_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float32x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vreinterpretq_f64_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((float64x2_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vreinterpretq_s16_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vreinterpretq_s32_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vreinterpretq_s64_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x2_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vreinterpretq_s8_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x16_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vreinterpretq_u16_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vreinterpretq_u32_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vreinterpretq_u64_u8(uint8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x2_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_s16(int16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_s32(int32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_s64(int64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_s8(int8x16_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_u16(uint16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_u32(uint32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vreinterpretq_u8_u64(uint64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x16_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vreinterpret_s16_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vreinterpret_s32_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int32x2_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vreinterpret_s64_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int64x1_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vreinterpret_s8_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((int8x8_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vreinterpret_u16_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint16x4_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vreinterpret_u32_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint32x2_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vreinterpret_u64_u8(uint8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint64x1_t)(a));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_s16(int16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_s32(int32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_s64(int64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_s8(int8x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_u16(uint16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_u32(uint32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vreinterpret_u8_u64(uint64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	return ((uint8x8_t)(a));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrev16q_s8(int8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(vec);
	const uint8_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrev16q_u8(uint8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(vec);
	const uint8_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrev16_s8(int8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(vec);
	const uint8_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrev16_u8(uint8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(vec);
	const uint8_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vrev32q_s16(int16x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(vec);
	const uint16_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6};
	vuint16m2_t temp_2 = vle16_v_u16m2(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrev32q_s8(int8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(vec);
	const uint8_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vrev32q_u16(uint16x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(vec);
	const uint16_t temp_1[] = {1, 0, 3, 2, 5, 4, 7, 6};
	vuint16m2_t temp_2 = vle16_v_u16m2(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrev32q_u8(uint8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(vec);
	const uint8_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrev32_s16(int16x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(vec);
	const uint16_t temp_1[] = {1, 0, 3, 2};
	vuint16m1_t temp_2 = vle16_v_u16m1(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrev32_s8(int8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(vec);
	const uint8_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrev32_u16(uint16x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(vec);
	const uint16_t temp_1[] = {1, 0, 3, 2};
	vuint16m1_t temp_2 = vle16_v_u16m1(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrev32_u8(uint8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(vec);
	const uint8_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrev64_f16(float16x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0};
	vuint16m1_t temp_2 = vle16_v_u16m1(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vrgather(temp_0, temp_2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrev64_f32(float32x2_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(vec);
	const uint32_t temp_1[] = {1, 0};
	vuint32m1_t temp_2 = vle32_v_u32m1(temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vrgather(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrev64q_f16(float16x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4};
	vuint16m2_t temp_2 = vle16_v_u16m2(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vrgather(temp_0, temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrev64q_f32(float32x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(vec);
	const uint32_t temp_1[] = {1, 0, 3, 2};
	vuint32m2_t temp_2 = vle32_v_u32m2(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vrev64q_s16(int16x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4};
	vuint16m2_t temp_2 = vle16_v_u16m2(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vrev64q_s32(int32x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(vec);
	const uint32_t temp_1[] = {1, 0, 3, 2};
	vuint32m2_t temp_2 = vle32_v_u32m2(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrev64q_s8(int8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(vec);
	const uint8_t temp_1[] = {7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vrev64q_u16(uint16x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0, 7, 6, 5, 4};
	vuint16m2_t temp_2 = vle16_v_u16m2(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vrev64q_u32(uint32x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(vec);
	const uint32_t temp_1[] = {1, 0, 3, 2};
	vuint32m2_t temp_2 = vle32_v_u32m2(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrev64q_u8(uint8x16_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(vec);
	const uint8_t temp_1[] = {7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8};
	vuint8m2_t temp_2 = vle8_v_u8m2(temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vrgather(temp_0, temp_2, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrev64_s16(int16x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0};
	vuint16m1_t temp_2 = vle16_v_u16m1(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vrev64_s32(int32x2_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(vec);
	const uint32_t temp_1[] = {1, 0};
	vuint32m1_t temp_2 = vle32_v_u32m1(temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vrgather(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrev64_s8(int8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(vec);
	const uint8_t temp_1[] = {7, 6, 5, 4, 3, 2, 1, 0};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrev64_u16(uint16x4_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(vec);
	const uint16_t temp_1[] = {3, 2, 1, 0};
	vuint16m1_t temp_2 = vle16_v_u16m1(temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vrgather(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vrev64_u32(uint32x2_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(vec);
	const uint32_t temp_1[] = {1, 0};
	vuint32m1_t temp_2 = vle32_v_u32m1(temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vrgather(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrev64_u8(uint8x8_t vec)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(vec);
	const uint8_t temp_1[] = {7, 6, 5, 4, 3, 2, 1, 0};
	vuint8m1_t temp_2 = vle8_v_u8m1(temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vrgather(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vrhaddq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint16m2_t temp_4 = vaadd(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vrhaddq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint32m2_t temp_4 = vaadd(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrhaddq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint8m2_t temp_4 = vaadd(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vrhaddq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint16m2_t temp_4 = vaaddu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vrhaddq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint32m2_t temp_4 = vaaddu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrhaddq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint8m2_t temp_4 = vaaddu(temp_0, temp_1, 16);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrhadd_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint16m1_t temp_4 = vaadd(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vrhadd_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint32m1_t temp_4 = vaadd(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrhadd_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vint8m1_t temp_4 = vaadd(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrhadd_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint16m1_t temp_4 = vaaddu(temp_0, temp_1, 4);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vrhadd_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint32m1_t temp_4 = vaaddu(temp_0, temp_1, 2);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrhadd_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	int temp_2 = vegetxround();
	int temp_3 = vesetxround(VE_TONEARESTUP);
	assert(temp_3 == 0);
	vuint8m1_t temp_4 = vaaddu(temp_0, temp_1, 8);
	vesetxround(temp_2);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32x_f32") float32x2_t vrnd32x_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32x_f64") float64x1_t vrnd32x_f64(float64x1_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32xq_f32") float32x4_t vrnd32xq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32xq_f64") float64x2_t vrnd32xq_f64(float64x2_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32z_f32") float32x2_t vrnd32z_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32z_f64") float64x1_t vrnd32z_f64(float64x1_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32zq_f32") float32x4_t vrnd32zq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd32zq_f64") float64x2_t vrnd32zq_f64(float64x2_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64x_f32") float32x2_t vrnd64x_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64x_f64") float64x1_t vrnd64x_f64(float64x1_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64xq_f32") float32x4_t vrnd64xq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64xq_f64") float64x2_t vrnd64xq_f64(float64x2_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64z_f32") float32x2_t vrnd64z_f32(float32x2_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64z_f64") float64x1_t vrnd64z_f64(float64x1_t a);
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64zq_f32") float32x4_t vrnd64zq_f32(float32x4_t a);
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrnd64zq_f64") float64x2_t vrnd64zq_f64(float64x2_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrnda_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-32767 - 1), 4), vmsne(temp_3, 32767, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float16_t *)((&temp_4)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrnda_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-2147483647L - 1), 2), vmsne(temp_3, 2147483647L, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float32_t *)((&temp_4)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrnda_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 1), vmsne(temp_3, 9223372036854775807LL, 1), 1), 1), temp_0, temp_3, 1), temp_0, 1), (*((const float64_t *)((&temp_4)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndah_f16") float16_t vrndah_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndaq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_3, (-32767 - 1), 8), vmsne(temp_3, 32767, 8), 8), 8), temp_0, temp_3, 8), temp_0, 8), (*((const float16_t *)((&temp_4)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndaq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-2147483647L - 1), 4), vmsne(temp_3, 2147483647L, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float32_t *)((&temp_4)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndaq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEARESTFROMZERO);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 2), vmsne(temp_3, 9223372036854775807LL, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float64_t *)((&temp_4)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrnd_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vint16m1_t temp_1 = vfcvt_rtz_x(temp_0, 4);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-32767 - 1), 4), vmsne(temp_1, 32767, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrnd_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vint32m1_t temp_1 = vfcvt_rtz_x(temp_0, 2);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-2147483647L - 1), 2), vmsne(temp_1, 2147483647L, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrnd_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vint64m1_t temp_1 = vfcvt_rtz_x(temp_0, 1);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 1), vmsne(temp_1, 9223372036854775807LL, 1), 1), 1), temp_0, temp_1, 1), temp_0, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndh_f16") float16_t vrndh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrndi_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vint16m1_t temp_1 = vfcvt_x(temp_0, 4);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-32767 - 1), 4), vmsne(temp_1, 32767, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrndi_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vint32m1_t temp_1 = vfcvt_x(temp_0, 2);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-2147483647L - 1), 2), vmsne(temp_1, 2147483647L, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrndi_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vint64m1_t temp_1 = vfcvt_x(temp_0, 1);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 1), vmsne(temp_1, 9223372036854775807LL, 1), 1), 1), temp_0, temp_1, 1), temp_0, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndih_f16") float16_t vrndih_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndiq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vint16m2_t temp_1 = vfcvt_x(temp_0, 8);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_1, (-32767 - 1), 8), vmsne(temp_1, 32767, 8), 8), 8), temp_0, temp_1, 8), temp_0, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndiq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vint32m2_t temp_1 = vfcvt_x(temp_0, 4);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-2147483647L - 1), 4), vmsne(temp_1, 2147483647L, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndiq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vint64m2_t temp_1 = vfcvt_x(temp_0, 2);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 2), vmsne(temp_1, 9223372036854775807LL, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrndm_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-32767 - 1), 4), vmsne(temp_3, 32767, 4), 4), 4), temp_0, temp_3, 4), (*((const float16_t *)((&temp_4)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrndm_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-2147483647L - 1), 2), vmsne(temp_3, 2147483647L, 2), 2), 2), temp_0, temp_3, 2), (*((const float32_t *)((&temp_4)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrndm_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 1), vmsne(temp_3, 9223372036854775807LL, 1), 1), 1), temp_0, temp_3, 1), (*((const float64_t *)((&temp_4)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndmh_f16") float16_t vrndmh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndmq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_3, (-32767 - 1), 8), vmsne(temp_3, 32767, 8), 8), 8), temp_0, temp_3, 8), (*((const float16_t *)((&temp_4)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndmq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-2147483647L - 1), 4), vmsne(temp_3, 2147483647L, 4), 4), 4), temp_0, temp_3, 4), (*((const float32_t *)((&temp_4)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndmq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_DOWNWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 2), vmsne(temp_3, 9223372036854775807LL, 2), 2), 2), temp_0, temp_3, 2), (*((const float64_t *)((&temp_4)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrndn_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-32767 - 1), 4), vmsne(temp_3, 32767, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float16_t *)((&temp_4)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrndn_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-2147483647L - 1), 2), vmsne(temp_3, 2147483647L, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float32_t *)((&temp_4)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrndn_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 1), vmsne(temp_3, 9223372036854775807LL, 1), 1), 1), temp_0, temp_3, 1), temp_0, 1), (*((const float64_t *)((&temp_4)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndnh_f16") float16_t vrndnh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndnq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_3, (-32767 - 1), 8), vmsne(temp_3, 32767, 8), 8), 8), temp_0, temp_3, 8), temp_0, 8), (*((const float16_t *)((&temp_4)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndnq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-2147483647L - 1), 4), vmsne(temp_3, 2147483647L, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float32_t *)((&temp_4)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndnq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_TONEAREST);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 2), vmsne(temp_3, 9223372036854775807LL, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float64_t *)((&temp_4)))), 2));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrndns_f32") float32_t vrndns_f32(float32_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrndp_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint16m1_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-32767 - 1), 4), vmsne(temp_3, 32767, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float16_t *)((&temp_4)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrndp_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint32m1_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-2147483647L - 1), 2), vmsne(temp_3, 2147483647L, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float32_t *)((&temp_4)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrndp_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint64m1_t temp_3 = vfcvt_x(temp_0, 1);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 1), vmsne(temp_3, 9223372036854775807LL, 1), 1), 1), temp_0, temp_3, 1), temp_0, 1), (*((const float64_t *)((&temp_4)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndph_f16") float16_t vrndph_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndpq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vfcvt_x(temp_0, 8);
	fesetround(temp_1);
	uint16_t temp_4 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_3, (-32767 - 1), 8), vmsne(temp_3, 32767, 8), 8), 8), temp_0, temp_3, 8), temp_0, 8), (*((const float16_t *)((&temp_4)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndpq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vfcvt_x(temp_0, 4);
	fesetround(temp_1);
	uint32_t temp_4 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_3, (-2147483647L - 1), 4), vmsne(temp_3, 2147483647L, 4), 4), 4), temp_0, temp_3, 4), temp_0, 4), (*((const float32_t *)((&temp_4)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndpq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	int temp_1 = fegetround();
	int temp_2 = fesetround(FE_UPWARD);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vfcvt_x(temp_0, 2);
	fesetround(temp_1);
	uint64_t temp_4 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_3, (-9223372036854775807LL - 1), 2), vmsne(temp_3, 9223372036854775807LL, 2), 2), 2), temp_0, temp_3, 2), temp_0, 2), (*((const float64_t *)((&temp_4)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vint16m2_t temp_1 = vfcvt_rtz_x(temp_0, 8);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_1, (-32767 - 1), 8), vmsne(temp_1, 32767, 8), 8), 8), temp_0, temp_1, 8), temp_0, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vint32m2_t temp_1 = vfcvt_rtz_x(temp_0, 4);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-2147483647L - 1), 4), vmsne(temp_1, 2147483647L, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vint64m2_t temp_1 = vfcvt_rtz_x(temp_0, 2);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 2), vmsne(temp_1, 9223372036854775807LL, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrndx_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vint16m1_t temp_1 = vfcvt_x(temp_0, 4);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-32767 - 1), 4), vmsne(temp_1, 32767, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float16_t *)((&temp_2)))), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrndx_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vint32m1_t temp_1 = vfcvt_x(temp_0, 2);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-2147483647L - 1), 2), vmsne(temp_1, 2147483647L, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float32_t *)((&temp_2)))), 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrndx_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vint64m1_t temp_1 = vfcvt_x(temp_0, 1);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmfne(temp_0, temp_0, 1), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 1), 153, 1), 0, 1), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 1), vmsne(temp_1, 9223372036854775807LL, 1), 1), 1), temp_0, temp_1, 1), temp_0, 1), (*((const float64_t *)((&temp_2)))), 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrndxh_f16") float16_t vrndxh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrndxq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vint16m2_t temp_1 = vfcvt_x(temp_0, 8);
	uint16_t temp_2 = 32256;
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmfne(temp_0, temp_0, 8), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 8), 153, 8), 0, 8), vmand(vmsne(temp_1, (-32767 - 1), 8), vmsne(temp_1, 32767, 8), 8), 8), temp_0, temp_1, 8), temp_0, 8), (*((const float16_t *)((&temp_2)))), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrndxq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vint32m2_t temp_1 = vfcvt_x(temp_0, 4);
	uint32_t temp_2 = 2143289344L;
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmfne(temp_0, temp_0, 4), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 4), 153, 4), 0, 4), vmand(vmsne(temp_1, (-2147483647L - 1), 4), vmsne(temp_1, 2147483647L, 4), 4), 4), temp_0, temp_1, 4), temp_0, 4), (*((const float32_t *)((&temp_2)))), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrndxq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vint64m2_t temp_1 = vfcvt_x(temp_0, 2);
	uint64_t temp_2 = 9221120237041090560LL;
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmfne(temp_0, temp_0, 2), vfsgnj(vfcvt_f(vmand(vmseq(vand(vfclass(temp_0, 2), 153, 2), 0, 2), vmand(vmsne(temp_1, (-9223372036854775807LL - 1), 2), vmsne(temp_1, 9223372036854775807LL, 2), 2), 2), temp_0, temp_1, 2), temp_0, 2), (*((const float64_t *)((&temp_2)))), 2));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vrshld_s64") int64_t vrshld_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vrshld_u64") uint64_t vrshld_u64(uint64_t a, int64_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vrshlq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vuint16m2_t temp_2 = vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m4_t temp_5 = vssra(vwcvt_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vint16m2_t temp_6 = vncvt_x(temp_5, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), vsll(temp_0, vreinterpret_v_i16m2_u16m2(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), temp_6, 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vrshlq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint32m2_t temp_2 = vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint64m4_t temp_5 = vssra(vwcvt_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vint32m2_t temp_6 = vncvt_x(temp_5, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), vsll(temp_0, vreinterpret_v_i32m2_u32m2(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vrshlq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint64m2_t temp_2 = vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2);
	vbool32_t temp_3 = vmseq(temp_2, 64, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vint64m2_t temp_6 = vssra(temp_3, temp_0, temp_0, 1, 2);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vint64m2_t temp_9 = vssra(temp_6, vsub(temp_3, temp_2, temp_2, 1, 2), 2);
	vesetxround(temp_7);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), temp_9, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrshlq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m4_t temp_5 = vssra(vwcvt_x(temp_0, 16), vwcvtu_x(temp_2, 16), 16);
	vesetxround(temp_3);
	vint8m2_t temp_6 = vncvt_x(temp_5, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), vsll(temp_0, vreinterpret_v_i8m2_u8m2(temp_1), 16), 0, 16), vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), temp_6, 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vrshlq_u16(uint16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	vuint16m2_t temp_2 = vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vuint16m2_t temp_6 = vncvt_x(temp_5, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), vsll(temp_0, vreinterpret_v_i16m2_u16m2(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), temp_6, 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vrshlq_u32(uint32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint32m2_t temp_2 = vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint64m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vuint32m2_t temp_6 = vncvt_x(temp_5, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), vsll(temp_0, vreinterpret_v_i32m2_u32m2(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vrshlq_u64(uint64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint64m2_t temp_2 = vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2);
	vbool32_t temp_3 = vmseq(temp_2, 64, 2);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vuint64m2_t temp_6 = vssrl(temp_3, temp_0, temp_0, 1, 2);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vuint64m2_t temp_9 = vssrl(temp_6, vsub(temp_3, temp_2, temp_2, 1, 2), 2);
	vesetxround(temp_7);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), temp_9, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrshlq_u8(uint8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m4_t temp_5 = vssrl(vwcvtu_x(temp_0, 16), vwcvtu_x(temp_2, 16), 16);
	vesetxround(temp_3);
	vuint8m2_t temp_6 = vncvt_x(temp_5, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), vsll(temp_0, vreinterpret_v_i8m2_u8m2(temp_1), 16), 0, 16), vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), temp_6, 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrshl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint16m1_t temp_2 = vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint32m2_t temp_5 = vssra(vwcvt_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vint16m1_t temp_6 = vncvt_x(temp_5, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), vsll(temp_0, vreinterpret_v_i16m1_u16m1(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vrshl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint32m1_t temp_2 = vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint64m2_t temp_5 = vssra(vwcvt_x(temp_0, 2), vwcvtu_x(temp_2, 2), 2);
	vesetxround(temp_3);
	vint32m1_t temp_6 = vncvt_x(temp_5, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), vsll(temp_0, vreinterpret_v_i32m1_u32m1(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), temp_6, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vrshl_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vuint64m1_t temp_2 = vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1);
	vbool64_t temp_3 = vmseq(temp_2, 64, 1);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vint64m1_t temp_6 = vssra(temp_3, temp_0, temp_0, 1, 1);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vint64m1_t temp_9 = vssra(temp_6, vsub(temp_3, temp_2, temp_2, 1, 1), 1);
	vesetxround(temp_7);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1), 0, 1), vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), temp_9, 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrshl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vint16m2_t temp_5 = vssra(vwcvt_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vint8m1_t temp_6 = vncvt_x(temp_5, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), vsll(temp_0, vreinterpret_v_i8m1_u8m1(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), temp_6, 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrshl_u16(uint16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	vuint16m1_t temp_2 = vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint32m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 4), vwcvtu_x(temp_2, 4), 4);
	vesetxround(temp_3);
	vuint16m1_t temp_6 = vncvt_x(temp_5, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), vsll(temp_0, vreinterpret_v_i16m1_u16m1(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), temp_6, 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vrshl_u32(uint32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	vuint32m1_t temp_2 = vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint64m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 2), vwcvtu_x(temp_2, 2), 2);
	vesetxround(temp_3);
	vuint32m1_t temp_6 = vncvt_x(temp_5, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), vsll(temp_0, vreinterpret_v_i32m1_u32m1(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), temp_6, 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vrshl_u64(uint64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	vuint64m1_t temp_2 = vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1);
	vbool64_t temp_3 = vmseq(temp_2, 64, 1);
	int temp_4 = vegetxround();
	int temp_5 = vesetxround(VE_TOODD);
	assert(temp_5 == 0);
	vuint64m1_t temp_6 = vssrl(temp_3, temp_0, temp_0, 1, 1);
	vesetxround(temp_4);
	int temp_7 = vegetxround();
	int temp_8 = vesetxround(VE_TONEARESTUP);
	assert(temp_8 == 0);
	vuint64m1_t temp_9 = vssrl(temp_6, vsub(temp_3, temp_2, temp_2, 1, 1), 1);
	vesetxround(temp_7);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1), 0, 1), vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), temp_9, 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrshl_u8(uint8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8);
	int temp_3 = vegetxround();
	int temp_4 = vesetxround(VE_TONEARESTUP);
	assert(temp_4 == 0);
	vuint16m2_t temp_5 = vssrl(vwcvtu_x(temp_0, 8), vwcvtu_x(temp_2, 8), 8);
	vesetxround(temp_3);
	vuint8m1_t temp_6 = vncvt_x(temp_5, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), vsll(temp_0, vreinterpret_v_i8m1_u8m1(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), temp_6, 0, 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vrshrd_n_s64") int64_t vrshrd_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vrshrd_n_u64") uint64_t vrshrd_n_u64(uint64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_s16") int8x16_t vrshrn_high_n_s16(int8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_s32") int16x8_t vrshrn_high_n_s32(int16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_s64") int32x4_t vrshrn_high_n_s64(int32x2_t r, int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_u16") uint8x16_t vrshrn_high_n_u16(uint8x8_t r, uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_u32") uint16x8_t vrshrn_high_n_u32(uint16x4_t r, uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrshrn_high_n_u64") uint32x4_t vrshrn_high_n_u64(uint32x2_t r, uint64x2_t a, const int n);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrshrn_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint16m2_t temp_3 = vssra(temp_0, n, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vncvt_x(temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrshrn_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint32m2_t temp_3 = vssra(temp_0, n, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vncvt_x(temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vrshrn_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vint64m2_t temp_3 = vssra(temp_0, n, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vncvt_x(temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrshrn_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint16m2_t temp_3 = vssrl(temp_0, n, 8);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vncvt_x(temp_3, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrshrn_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint32m2_t temp_3 = vssrl(temp_0, n, 4);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vncvt_x(temp_3, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vrshrn_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	int temp_1 = vegetxround();
	int temp_2 = vesetxround(VE_TONEARESTUP);
	assert(temp_2 == 0);
	vuint64m2_t temp_3 = vssrl(temp_0, n, 2);
	vesetxround(temp_1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vncvt_x(temp_3, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vrshr_n_s16(int16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1;
	if ((n < 16))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint16m1_t temp_4 = vssra(temp_0, n, 4);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i16m1(0, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vrshr_n_s32(int32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1;
	if ((n < 32))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint32m1_t temp_4 = vssra(temp_0, n, 2);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i32m1(0, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vrshr_n_s64(int64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1;
	if ((n < 64))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint64m1_t temp_4 = vssra(temp_0, n, 1);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i64m1(0, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vrshr_n_s8(int8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1;
	if ((n < 8))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint8m1_t temp_4 = vssra(temp_0, n, 8);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i8m1(0, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vrshr_n_u16(uint16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1;
	if ((n < 16))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint16m1_t temp_4 = vssrl(temp_0, n, 4);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 32767, 4), vmv_v_x_u16m1(0, 4), 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vrshr_n_u32(uint32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1;
	if ((n < 32))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint32m1_t temp_4 = vssrl(temp_0, n, 2);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 2147483647L, 2), vmv_v_x_u32m1(0, 2), 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vrshr_n_u64(uint64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1;
	if ((n < 64))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint64m1_t temp_4 = vssrl(temp_0, n, 1);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 9223372036854775807LL, 1), vmv_v_x_u64m1(0, 1), 1, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vrshr_n_u8(uint8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1;
	if ((n < 8))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint8m1_t temp_4 = vssrl(temp_0, n, 8);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 127, 8), vmv_v_x_u8m1(0, 8), 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vrshrq_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1;
	if ((n < 16))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint16m2_t temp_4 = vssra(temp_0, n, 8);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i16m2(0, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vrshrq_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1;
	if ((n < 32))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint32m2_t temp_4 = vssra(temp_0, n, 4);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i32m2(0, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vrshrq_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1;
	if ((n < 64))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint64m2_t temp_4 = vssra(temp_0, n, 2);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i64m2(0, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vrshrq_n_s8(int8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1;
	if ((n < 8))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vint8m2_t temp_4 = vssra(temp_0, n, 16);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmv_v_x_i8m2(0, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vrshrq_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1;
	if ((n < 16))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint16m2_t temp_4 = vssrl(temp_0, n, 8);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 32767, 8), vmv_v_x_u16m2(0, 8), 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vrshrq_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1;
	if ((n < 32))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint32m2_t temp_4 = vssrl(temp_0, n, 4);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 2147483647L, 4), vmv_v_x_u32m2(0, 4), 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vrshrq_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1;
	if ((n < 64))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint64m2_t temp_4 = vssrl(temp_0, n, 2);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 9223372036854775807LL, 2), vmv_v_x_u64m2(0, 2), 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vrshrq_n_u8(uint8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1;
	if ((n < 8))
	{
		int temp_2 = vegetxround();
		int temp_3 = vesetxround(VE_TONEARESTUP);
		assert(temp_3 == 0);
		vuint8m2_t temp_4 = vssrl(temp_0, n, 16);
		vesetxround(temp_2);
		temp_1 = temp_4;
	}
	else
	{
		temp_1 = vmerge(vmsgtu(temp_0, 127, 16), vmv_v_x_u8m2(0, 16), 1, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1);
}
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrsqrted_f64") float64_t vrsqrted_f64(float64_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrsqrte_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfrsqrt7(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrsqrte_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfrsqrt7(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrsqrte_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfrsqrt7(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrsqrteh_f16") float16_t vrsqrteh_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrsqrteq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfrsqrt7(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrsqrteq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfrsqrt7(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrsqrteq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfrsqrt7(temp_0, 2));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsqrteq_u32") uint32x4_t vrsqrteq_u32(uint32x4_t a);
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrsqrtes_f32") float32_t vrsqrtes_f32(float32_t a);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsqrte_u32") uint32x2_t vrsqrte_u32(uint32x2_t a);
#endif
#if (64 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrsqrtsd_f64") float64_t vrsqrtsd_f64(float64_t a, float64_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vrsqrts_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vuint16m1_t temp_2 = vand(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 153, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 4), vmseq(temp_2, 17, 4), 4), vmseq(temp_2, 9, 4), 4), vfdiv(vfnmsac(vfmv_v_f_f16m1(3, 4), temp_0, temp_1, 4), 2, 4), 1.5, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vrsqrts_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vuint32m1_t temp_2 = vand(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 153, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 2), vmseq(temp_2, 17, 2), 2), vmseq(temp_2, 9, 2), 2), vfdiv(vfnmsac(vfmv_v_f_f32m1(3, 2), temp_0, temp_1, 2), 2, 2), 1.5, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vrsqrts_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	vuint64m1_t temp_2 = vand(vor(vfclass(temp_0, 1), vfclass(temp_1, 1), 1), 153, 1);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 1), vmseq(temp_2, 17, 1), 1), vmseq(temp_2, 9, 1), 1), vfdiv(vfnmsac(vfmv_v_f_f64m1(3, 1), temp_0, temp_1, 1), 2, 1), 1.5, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vrsqrtsh_f16") float16_t vrsqrtsh_f16(float16_t a, float16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vrsqrtsq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vuint16m2_t temp_2 = vand(vor(vfclass(temp_0, 8), vfclass(temp_1, 8), 8), 153, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 8), vmseq(temp_2, 17, 8), 8), vmseq(temp_2, 9, 8), 8), vfdiv(vfnmsac(vfmv_v_f_f16m2(3, 8), temp_0, temp_1, 8), 2, 8), 1.5, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vrsqrtsq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vuint32m2_t temp_2 = vand(vor(vfclass(temp_0, 4), vfclass(temp_1, 4), 4), 153, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 4), vmseq(temp_2, 17, 4), 4), vmseq(temp_2, 9, 4), 4), vfdiv(vfnmsac(vfmv_v_f_f32m2(3, 4), temp_0, temp_1, 4), 2, 4), 1.5, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vrsqrtsq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vuint64m2_t temp_2 = vand(vor(vfclass(temp_0, 2), vfclass(temp_1, 2), 2), 153, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfmerge(vmor(vmor(vmsgtu(temp_2, 135, 2), vmseq(temp_2, 17, 2), 2), vmseq(temp_2, 9, 2), 2), vfdiv(vfnmsac(vfmv_v_f_f64m2(3, 2), temp_0, temp_1, 2), 2, 2), 1.5, 2));
}
#endif
#if (32 <= __riscv_flen)
NEON2RVV_NOT_IMPLEMENT("vrsqrtss_f32") float32_t vrsqrtss_f32(float32_t a, float32_t b);
#endif
NEON2RVV_NOT_IMPLEMENT("vrsrad_n_s64") int64_t vrsrad_n_s64(int64_t a, int64_t b, const int n);
NEON2RVV_NOT_IMPLEMENT("vrsrad_n_u64") uint64_t vrsrad_n_u64(uint64_t a, uint64_t b, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_s16") int16x4_t vrsra_n_s16(int16x4_t a, int16x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_s32") int32x2_t vrsra_n_s32(int32x2_t a, int32x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_s64") int64x1_t vrsra_n_s64(int64x1_t a, int64x1_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_s8") int8x8_t vrsra_n_s8(int8x8_t a, int8x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_u16") uint16x4_t vrsra_n_u16(uint16x4_t a, uint16x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_u32") uint32x2_t vrsra_n_u32(uint32x2_t a, uint32x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_u64") uint64x1_t vrsra_n_u64(uint64x1_t a, uint64x1_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsra_n_u8") uint8x8_t vrsra_n_u8(uint8x8_t a, uint8x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_s16") int16x8_t vrsraq_n_s16(int16x8_t a, int16x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_s32") int32x4_t vrsraq_n_s32(int32x4_t a, int32x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_s64") int64x2_t vrsraq_n_s64(int64x2_t a, int64x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_s8") int8x16_t vrsraq_n_s8(int8x16_t a, int8x16_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_u16") uint16x8_t vrsraq_n_u16(uint16x8_t a, uint16x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_u32") uint32x4_t vrsraq_n_u32(uint32x4_t a, uint32x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_u64") uint64x2_t vrsraq_n_u64(uint64x2_t a, uint64x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsraq_n_u8") uint8x16_t vrsraq_n_u8(uint8x16_t a, uint8x16_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_s16") int8x16_t vrsubhn_high_s16(int8x8_t r, int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_s32") int16x8_t vrsubhn_high_s32(int16x4_t r, int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_s64") int32x4_t vrsubhn_high_s64(int32x2_t r, int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_u16") uint8x16_t vrsubhn_high_u16(uint8x8_t r, uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_u32") uint16x8_t vrsubhn_high_u32(uint16x4_t r, uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_high_u64") uint32x4_t vrsubhn_high_u64(uint32x2_t r, uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_s16") int8x8_t vrsubhn_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_s32") int16x4_t vrsubhn_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_s64") int32x2_t vrsubhn_s64(int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_u16") uint8x8_t vrsubhn_u16(uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_u32") uint16x4_t vrsubhn_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vrsubhn_u64") uint32x2_t vrsubhn_u64(uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vset_lane_f16(float16_t a, float16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vset_lane_f32(float32_t a, float32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vset_lane_f64(float64_t a, float64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vset_lane_s16(int16_t a, int16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vset_lane_s32(int32_t a, int32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vset_lane_s64(int64_t a, int64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vset_lane_s8(int8_t a, int8x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vset_lane_u16(uint16_t a, uint16x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vset_lane_u32(uint32_t a, uint32x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vset_lane_u64(uint64_t a, uint64x1_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vset_lane_u8(uint8_t a, uint8x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vsetq_lane_f16(float16_t a, float16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vsetq_lane_f32(float32_t a, float32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vsetq_lane_f64(float64_t a, float64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsetq_lane_s16(int16_t a, int16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsetq_lane_s32(int32_t a, int32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsetq_lane_s64(int64_t a, int64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vsetq_lane_s8(int8_t a, int8x16_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsetq_lane_u16(uint16_t a, uint16x8_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsetq_lane_u32(uint32_t a, uint32x4_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsetq_lane_u64(uint64_t a, uint64x2_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vsetq_lane_u8(uint8_t a, uint8x16_t v, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	v[lane] = a;
	return v;
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha1cq_u32") uint32x4_t vsha1cq_u32(uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk);
#endif
NEON2RVV_NOT_IMPLEMENT("vsha1h_u32") uint32_t vsha1h_u32(uint32_t hash_e);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha1mq_u32") uint32x4_t vsha1mq_u32(uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha1pq_u32") uint32x4_t vsha1pq_u32(uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha1su0q_u32") uint32x4_t vsha1su0q_u32(uint32x4_t w0_3, uint32x4_t w4_7, uint32x4_t w8_11);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha1su1q_u32") uint32x4_t vsha1su1q_u32(uint32x4_t tw0_3, uint32x4_t w12_15);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha256h2q_u32") uint32x4_t vsha256h2q_u32(uint32x4_t hash_efgh, uint32x4_t hash_abcd, uint32x4_t wk);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha256hq_u32") uint32x4_t vsha256hq_u32(uint32x4_t hash_abcd, uint32x4_t hash_efgh, uint32x4_t wk);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha256su0q_u32") uint32x4_t vsha256su0q_u32(uint32x4_t w0_3, uint32x4_t w4_7);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha256su1q_u32") uint32x4_t vsha256su1q_u32(uint32x4_t tw0_3, uint32x4_t w8_11, uint32x4_t w12_15);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha512h2q_u64") uint64x2_t vsha512h2q_u64(uint64x2_t sum_ab, uint64x2_t hash_c_, uint64x2_t hash_ab);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha512hq_u64") uint64x2_t vsha512hq_u64(uint64x2_t hash_ed, uint64x2_t hash_gf, uint64x2_t kwh_kwh2);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha512su0q_u64") uint64x2_t vsha512su0q_u64(uint64x2_t w0_1, uint64x2_t w2_);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsha512su1q_u64") uint64x2_t vsha512su1q_u64(uint64x2_t s01_s02, uint64x2_t w14_15, uint64x2_t w9_10);
#endif
NEON2RVV_NOT_IMPLEMENT("vshld_n_s64") int64_t vshld_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vshld_n_u64") uint64_t vshld_n_u64(uint64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vshld_s64") int64_t vshld_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vshld_u64") uint64_t vshld_u64(uint64_t a, int64_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_s16") int32x4_t vshll_high_n_s16(int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_s32") int64x2_t vshll_high_n_s32(int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_s8") int16x8_t vshll_high_n_s8(int8x16_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_u16") uint32x4_t vshll_high_n_u16(uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_u32") uint64x2_t vshll_high_n_u32(uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshll_high_n_u8") uint16x8_t vshll_high_n_u8(uint8x16_t a, const int n);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vshll_n_s16(int16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint32m2_t temp_1 = vwcvt_x(temp_0, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsll(temp_1, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vshll_n_s32(int32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint64m2_t temp_1 = vwcvt_x(temp_0, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsll(temp_1, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vshll_n_s8(int8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint16m2_t temp_1 = vwcvt_x(temp_0, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsll(temp_1, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vshll_n_u16(uint16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint32m2_t temp_1 = vwcvtu_x(temp_0, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsll(temp_1, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vshll_n_u32(uint32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint64m2_t temp_1 = vwcvtu_x(temp_0, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsll(temp_1, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vshll_n_u8(uint8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint16m2_t temp_1 = vwcvtu_x(temp_0, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsll(temp_1, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vshl_n_s16(int16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vsll(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vshl_n_s32(int32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vsll(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vshl_n_s64(int64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vsll(temp_0, n, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vshl_n_s8(int8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vsll(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vshl_n_u16(uint16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vsll(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vshl_n_u32(uint32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vsll(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vshl_n_u64(uint64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vsll(temp_0, n, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vshl_n_u8(uint8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vsll(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vshlq_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsll(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vshlq_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsll(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vshlq_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsll(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vshlq_n_s8(int8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vsll(temp_0, n, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vshlq_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsll(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vshlq_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsll(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vshlq_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsll(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vshlq_n_u8(uint8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vsll(temp_0, n, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vshlq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), vsll(temp_0, vreinterpret_v_i16m2_u16m2(temp_1), 8), 0, 8), vsra(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 240, 8), vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i16m2(0, 8), -1, 8), temp_0, vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8), 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vshlq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), vsll(temp_0, vreinterpret_v_i32m2_u32m2(temp_1), 4), 0, 4), vsra(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 224, 4), vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i32m2(0, 4), -1, 4), temp_0, vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4), 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vshlq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2), 0, 2), vsra(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 192, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i64m2(0, 2), -1, 2), temp_0, vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2), 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vshlq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), vsll(temp_0, vreinterpret_v_i8m2_u8m2(temp_1), 16), 0, 16), vsra(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 248, 16), vmerge(vmslt(temp_0, 0, 16), vmv_v_x_i8m2(0, 16), -1, 16), temp_0, vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16), 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vshlq_u16(uint16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	temp_1 = vand(temp_1, 255, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i16m2_u16m2(temp_1), 15, 8), vsll(temp_0, vreinterpret_v_i16m2_u16m2(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i16m2_u16m2(temp_1), 241, 8), vsrl(temp_0, vrsub(vreinterpret_v_i16m2_u16m2(temp_1), 256, 8), 8), 0, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vshlq_u32(uint32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	temp_1 = vand(temp_1, 255, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i32m2_u32m2(temp_1), 31, 4), vsll(temp_0, vreinterpret_v_i32m2_u32m2(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i32m2_u32m2(temp_1), 225, 4), vsrl(temp_0, vrsub(vreinterpret_v_i32m2_u32m2(temp_1), 256, 4), 4), 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vshlq_u64(uint64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	temp_1 = vand(temp_1, 255, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i64m2_u64m2(temp_1), 63, 2), vsll(temp_0, vreinterpret_v_i64m2_u64m2(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i64m2_u64m2(temp_1), 193, 2), vsrl(temp_0, vrsub(vreinterpret_v_i64m2_u64m2(temp_1), 256, 2), 2), 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vshlq_u8(uint8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 127, 16), vmerge(vmsgtu(vreinterpret_v_i8m2_u8m2(temp_1), 7, 16), vsll(temp_0, vreinterpret_v_i8m2_u8m2(temp_1), 16), 0, 16), vmerge(vmsltu(vreinterpret_v_i8m2_u8m2(temp_1), 249, 16), vsrl(temp_0, vmul(vreinterpret_v_i8m2_u8m2(temp_1), 255, 16), 16), 0, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vshl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), vsll(temp_0, vreinterpret_v_i16m1_u16m1(temp_1), 4), 0, 4), vsra(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 240, 4), vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i16m1(0, 4), -1, 4), temp_0, vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4), 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vshl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), vsll(temp_0, vreinterpret_v_i32m1_u32m1(temp_1), 2), 0, 2), vsra(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 224, 2), vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i32m1(0, 2), -1, 2), temp_0, vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2), 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vshl_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1), 0, 1), vsra(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 192, 1), vmerge(vmslt(temp_0, 0, 1), vmv_v_x_i64m1(0, 1), -1, 1), temp_0, vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1), 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vshl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), vsll(temp_0, vreinterpret_v_i8m1_u8m1(temp_1), 8), 0, 8), vsra(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 248, 8), vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i8m1(0, 8), -1, 8), temp_0, vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8), 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vshl_u16(uint16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	temp_1 = vand(temp_1, 255, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 127, 4), vmerge(vmsgtu(vreinterpret_v_i16m1_u16m1(temp_1), 15, 4), vsll(temp_0, vreinterpret_v_i16m1_u16m1(temp_1), 4), 0, 4), vmerge(vmsltu(vreinterpret_v_i16m1_u16m1(temp_1), 241, 4), vsrl(temp_0, vrsub(vreinterpret_v_i16m1_u16m1(temp_1), 256, 4), 4), 0, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vshl_u32(uint32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	temp_1 = vand(temp_1, 255, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 127, 2), vmerge(vmsgtu(vreinterpret_v_i32m1_u32m1(temp_1), 31, 2), vsll(temp_0, vreinterpret_v_i32m1_u32m1(temp_1), 2), 0, 2), vmerge(vmsltu(vreinterpret_v_i32m1_u32m1(temp_1), 225, 2), vsrl(temp_0, vrsub(vreinterpret_v_i32m1_u32m1(temp_1), 256, 2), 2), 0, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vshl_u64(uint64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	temp_1 = vand(temp_1, 255, 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 127, 1), vmerge(vmsgtu(vreinterpret_v_i64m1_u64m1(temp_1), 63, 1), vsll(temp_0, vreinterpret_v_i64m1_u64m1(temp_1), 1), 0, 1), vmerge(vmsltu(vreinterpret_v_i64m1_u64m1(temp_1), 193, 1), vsrl(temp_0, vrsub(vreinterpret_v_i64m1_u64m1(temp_1), 256, 1), 1), 0, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vshl_u8(uint8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 127, 8), vmerge(vmsgtu(vreinterpret_v_i8m1_u8m1(temp_1), 7, 8), vsll(temp_0, vreinterpret_v_i8m1_u8m1(temp_1), 8), 0, 8), vmerge(vmsltu(vreinterpret_v_i8m1_u8m1(temp_1), 249, 8), vsrl(temp_0, vmul(vreinterpret_v_i8m1_u8m1(temp_1), 255, 8), 8), 0, 8), 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vshrd_n_s64") int64_t vshrd_n_s64(int64_t a, const int n);
NEON2RVV_NOT_IMPLEMENT("vshrd_n_u64") uint64_t vshrd_n_u64(uint64_t a, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_s16") int8x16_t vshrn_high_n_s16(int8x8_t r, int16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_s32") int16x8_t vshrn_high_n_s32(int16x4_t r, int32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_s64") int32x4_t vshrn_high_n_s64(int32x2_t r, int64x2_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_u16") uint8x16_t vshrn_high_n_u16(uint8x8_t r, uint16x8_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_u32") uint16x8_t vshrn_high_n_u32(uint16x4_t r, uint32x4_t a, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vshrn_high_n_u64") uint32x4_t vshrn_high_n_u64(uint32x2_t r, uint64x2_t a, const int n);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vshrn_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vnsra(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vshrn_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vnsra(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vshrn_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vnsra(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vshrn_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vnsrl(temp_0, n, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vshrn_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vnsrl(temp_0, n, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vshrn_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vnsrl(temp_0, n, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vshr_n_s16(int16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1;
	if ((n < 16))
	{
		temp_1 = vsra(temp_0, n, 4);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i16m1(0, 4), -1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vshr_n_s32(int32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1;
	if ((n < 32))
	{
		temp_1 = vsra(temp_0, n, 2);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i32m1(0, 2), -1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vshr_n_s64(int64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1;
	if ((n < 64))
	{
		temp_1 = vsra(temp_0, n, 1);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 1), vmv_v_x_i64m1(0, 1), -1, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vshr_n_s8(int8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1;
	if ((n < 8))
	{
		temp_1 = vsra(temp_0, n, 8);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i8m1(0, 8), -1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vshr_n_u16(uint16x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1;
	if ((n < 16))
	{
		temp_1 = vsrl(temp_0, n, 4);
	}
	else
	{
		temp_1 = vmv_v_x_u16m1(0, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vshr_n_u32(uint32x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1;
	if ((n < 32))
	{
		temp_1 = vsrl(temp_0, n, 2);
	}
	else
	{
		temp_1 = vmv_v_x_u32m1(0, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vshr_n_u64(uint64x1_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1;
	if ((n < 64))
	{
		temp_1 = vsrl(temp_0, n, 1);
	}
	else
	{
		temp_1 = vmv_v_x_u64m1(0, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vshr_n_u8(uint8x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1;
	if ((n < 8))
	{
		temp_1 = vsrl(temp_0, n, 8);
	}
	else
	{
		temp_1 = vmv_v_x_u8m1(0, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vshrq_n_s16(int16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1;
	if ((n < 16))
	{
		temp_1 = vsra(temp_0, n, 8);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 8), vmv_v_x_i16m2(0, 8), -1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vshrq_n_s32(int32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1;
	if ((n < 32))
	{
		temp_1 = vsra(temp_0, n, 4);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 4), vmv_v_x_i32m2(0, 4), -1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vshrq_n_s64(int64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1;
	if ((n < 64))
	{
		temp_1 = vsra(temp_0, n, 2);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 2), vmv_v_x_i64m2(0, 2), -1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vshrq_n_s8(int8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1;
	if ((n < 8))
	{
		temp_1 = vsra(temp_0, n, 16);
	}
	else
	{
		temp_1 = vmerge(vmslt(temp_0, 0, 16), vmv_v_x_i8m2(0, 16), -1, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vshrq_n_u16(uint16x8_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1;
	if ((n < 16))
	{
		temp_1 = vsrl(temp_0, n, 8);
	}
	else
	{
		temp_1 = vmv_v_x_u16m2(0, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vshrq_n_u32(uint32x4_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1;
	if ((n < 32))
	{
		temp_1 = vsrl(temp_0, n, 4);
	}
	else
	{
		temp_1 = vmv_v_x_u32m2(0, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vshrq_n_u64(uint64x2_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1;
	if ((n < 64))
	{
		temp_1 = vsrl(temp_0, n, 2);
	}
	else
	{
		temp_1 = vmv_v_x_u64m2(0, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vshrq_n_u8(uint8x16_t a, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1;
	if ((n < 8))
	{
		temp_1 = vsrl(temp_0, n, 16);
	}
	else
	{
		temp_1 = vmv_v_x_u8m2(0, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_1);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vslid_n_s64") int64_t vslid_n_s64(int64_t a, int64_t b, const int n);
NEON2RVV_NOT_IMPLEMENT("vslid_n_u64") uint64_t vslid_n_u64(uint64_t a, uint64_t b, const int n);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vsli_n_s16(int16x4_t a, int16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	const uint16_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vor(vand(temp_0, ((temp_2 << n) - 1), 4), vsll(temp_1, n, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vsli_n_s32(int32x2_t a, int32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	const uint32_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vor(vand(temp_0, ((temp_2 << n) - 1), 2), vsll(temp_1, n, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vsli_n_s64(int64x1_t a, int64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	const uint64_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vor(vand(temp_0, ((temp_2 << n) - 1), 1), vsll(temp_1, n, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vsli_n_s8(int8x8_t a, int8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	const uint8_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vor(vand(temp_0, ((temp_2 << n) - 1), 8), vsll(temp_1, n, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vsli_n_u16(uint16x4_t a, uint16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	const uint16_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vor(vand(temp_0, ((temp_2 << n) - 1), 4), vsll(temp_1, n, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vsli_n_u32(uint32x2_t a, uint32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	const uint32_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vor(vand(temp_0, ((temp_2 << n) - 1), 2), vsll(temp_1, n, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vsli_n_u64(uint64x1_t a, uint64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	const uint64_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vor(vand(temp_0, ((temp_2 << n) - 1), 1), vsll(temp_1, n, 1), 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vsli_n_u8(uint8x8_t a, uint8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	const uint8_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vor(vand(temp_0, ((temp_2 << n) - 1), 8), vsll(temp_1, n, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsliq_n_s16(int16x8_t a, int16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	const uint16_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vor(vand(temp_0, ((temp_2 << n) - 1), 8), vsll(temp_1, n, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsliq_n_s32(int32x4_t a, int32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	const uint32_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vor(vand(temp_0, ((temp_2 << n) - 1), 4), vsll(temp_1, n, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsliq_n_s64(int64x2_t a, int64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	const uint64_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vor(vand(temp_0, ((temp_2 << n) - 1), 2), vsll(temp_1, n, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vsliq_n_s8(int8x16_t a, int8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	const uint8_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vor(vand(temp_0, ((temp_2 << n) - 1), 16), vsll(temp_1, n, 16), 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsliq_n_u16(uint16x8_t a, uint16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	const uint16_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vor(vand(temp_0, ((temp_2 << n) - 1), 8), vsll(temp_1, n, 8), 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsliq_n_u32(uint32x4_t a, uint32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	const uint32_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vor(vand(temp_0, ((temp_2 << n) - 1), 4), vsll(temp_1, n, 4), 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsliq_n_u64(uint64x2_t a, uint64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	const uint64_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vor(vand(temp_0, ((temp_2 << n) - 1), 2), vsll(temp_1, n, 2), 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vsliq_n_u8(uint8x16_t a, uint8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	const uint8_t temp_2 = 1;
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vor(vand(temp_0, ((temp_2 << n) - 1), 16), vsll(temp_1, n, 16), 16));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3partw1q_u32") uint32x4_t vsm3partw1q_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3partw2q_u32") uint32x4_t vsm3partw2q_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3ss1q_u32") uint32x4_t vsm3ss1q_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3tt1aq_u32") uint32x4_t vsm3tt1aq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c, const int imm2);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3tt1bq_u32") uint32x4_t vsm3tt1bq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c, const int imm2);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3tt2aq_u32") uint32x4_t vsm3tt2aq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c, const int imm2);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm3tt2bq_u32") uint32x4_t vsm3tt2bq_u32(uint32x4_t a, uint32x4_t b, uint32x4_t c, const int imm2);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm4ekeyq_u32") uint32x4_t vsm4ekeyq_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsm4eq_u32") uint32x4_t vsm4eq_u32(uint32x4_t a, uint32x4_t b);
#endif
NEON2RVV_NOT_IMPLEMENT("vsqaddb_u8") uint8_t vsqaddb_u8(uint8_t a, int8_t b);
NEON2RVV_NOT_IMPLEMENT("vsqaddd_u64") uint64_t vsqaddd_u64(uint64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vsqaddh_u16") uint16_t vsqaddh_u16(uint16_t a, int16_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqaddq_u16") uint16x8_t vsqaddq_u16(uint16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqaddq_u32") uint32x4_t vsqaddq_u32(uint32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqaddq_u64") uint64x2_t vsqaddq_u64(uint64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqaddq_u8") uint8x16_t vsqaddq_u8(uint8x16_t a, int8x16_t b);
#endif
NEON2RVV_NOT_IMPLEMENT("vsqadds_u32") uint32_t vsqadds_u32(uint32_t a, int32_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqadd_u16") uint16x4_t vsqadd_u16(uint16x4_t a, int16x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqadd_u32") uint32x2_t vsqadd_u32(uint32x2_t a, int32x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqadd_u64") uint64x1_t vsqadd_u64(uint64x1_t a, int64x1_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsqadd_u8") uint8x8_t vsqadd_u8(uint8x8_t a, int8x8_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vsqrt_f16(float16x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfsqrt(temp_0, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vsqrt_f32(float32x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfsqrt(temp_0, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vsqrt_f64(float64x1_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfsqrt(temp_0, 1));
}
#endif
#if defined(__riscv_zfh)
NEON2RVV_NOT_IMPLEMENT("vsqrth_f16") float16_t vsqrth_f16(float16_t a);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vsqrtq_f16(float16x8_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfsqrt(temp_0, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vsqrtq_f32(float32x4_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfsqrt(temp_0, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vsqrtq_f64(float64x2_t a)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfsqrt(temp_0, 2));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vsrad_n_s64") int64_t vsrad_n_s64(int64_t a, int64_t b, const int n);
NEON2RVV_NOT_IMPLEMENT("vsrad_n_u64") uint64_t vsrad_n_u64(uint64_t a, uint64_t b, const int n);
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vsra_n_s16(int16x4_t a, int16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vint16m1_t temp_2;
	if ((n < 16))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 4), 4);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 4), temp_0, temp_0, 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vsra_n_s32(int32x2_t a, int32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vint32m1_t temp_2;
	if ((n < 32))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 2), 2);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 2), temp_0, temp_0, 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vsra_n_s64(int64x1_t a, int64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	vint64m1_t temp_2;
	if ((n < 64))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 1), 1);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 1), temp_0, temp_0, 1, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vsra_n_s8(int8x8_t a, int8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vint8m1_t temp_2;
	if ((n < 8))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 8), 8);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 8), temp_0, temp_0, 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vsra_n_u16(uint16x4_t a, uint16x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2;
	if ((n < 16))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 4), 4);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 4), temp_0, temp_0, 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vsra_n_u32(uint32x2_t a, uint32x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2;
	if ((n < 32))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 2), 2);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 2), temp_0, temp_0, 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vsra_n_u64(uint64x1_t a, uint64x1_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	vuint64m1_t temp_2;
	if ((n < 64))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 1), 1);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 1), temp_0, temp_0, 1, 1);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vsra_n_u8(uint8x8_t a, uint8x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2;
	if ((n < 8))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 8), 8);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 8), temp_0, temp_0, 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsraq_n_s16(int16x8_t a, int16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m2_t temp_2;
	if ((n < 16))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 8), 8);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 8), temp_0, temp_0, 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsraq_n_s32(int32x4_t a, int32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m2_t temp_2;
	if ((n < 32))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 4), 4);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 4), temp_0, temp_0, 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsraq_n_s64(int64x2_t a, int64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	vint64m2_t temp_2;
	if ((n < 64))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 2), 2);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 2), temp_0, temp_0, 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vsraq_n_s8(int8x16_t a, int8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m2_t temp_2;
	if ((n < 8))
	{
		temp_2 = vadd(temp_0, vsra(temp_1, n, 16), 16);
	}
	else
	{
		temp_2 = vsub(vmslt(temp_1, 0, 16), temp_0, temp_0, 1, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsraq_n_u16(uint16x8_t a, uint16x8_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2;
	if ((n < 16))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 8), 8);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 8), temp_0, temp_0, 1, 8);
	}
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsraq_n_u32(uint32x4_t a, uint32x4_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2;
	if ((n < 32))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 4), 4);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 4), temp_0, temp_0, 1, 4);
	}
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsraq_n_u64(uint64x2_t a, uint64x2_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	vuint64m2_t temp_2;
	if ((n < 64))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 2), 2);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 2), temp_0, temp_0, 1, 2);
	}
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vsraq_n_u8(uint8x16_t a, uint8x16_t b, const int n)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2;
	if ((n < 8))
	{
		temp_2 = vadd(temp_0, vsrl(temp_1, n, 16), 16);
	}
	else
	{
		temp_2 = vsub(vmsltu(temp_1, 0, 16), temp_0, temp_0, 1, 16);
	}
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_2);
}
#endif
NEON2RVV_NOT_IMPLEMENT("vsrid_n_s64") int64_t vsrid_n_s64(int64_t a, int64_t b, const int n);
NEON2RVV_NOT_IMPLEMENT("vsrid_n_u64") uint64_t vsrid_n_u64(uint64_t a, uint64_t b, const int n);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_s16") int16x4_t vsri_n_s16(int16x4_t a, int16x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_s32") int32x2_t vsri_n_s32(int32x2_t a, int32x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_s64") int64x1_t vsri_n_s64(int64x1_t a, int64x1_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_s8") int8x8_t vsri_n_s8(int8x8_t a, int8x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_u16") uint16x4_t vsri_n_u16(uint16x4_t a, uint16x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_u32") uint32x2_t vsri_n_u32(uint32x2_t a, uint32x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_u64") uint64x1_t vsri_n_u64(uint64x1_t a, uint64x1_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsri_n_u8") uint8x8_t vsri_n_u8(uint8x8_t a, uint8x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_s16") int16x8_t vsriq_n_s16(int16x8_t a, int16x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_s32") int32x4_t vsriq_n_s32(int32x4_t a, int32x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_s64") int64x2_t vsriq_n_s64(int64x2_t a, int64x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_s8") int8x16_t vsriq_n_s8(int8x16_t a, int8x16_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_u16") uint16x8_t vsriq_n_u16(uint16x8_t a, uint16x8_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_u32") uint32x4_t vsriq_n_u32(uint32x4_t a, uint32x4_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_u64") uint64x2_t vsriq_n_u64(uint64x2_t a, uint64x2_t b, const int n);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsriq_n_u8") uint8x16_t vsriq_n_u8(uint8x16_t a, uint8x16_t b, const int n);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1_f16(float16_t * ptr, float16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(val);
	vse16(ptr, temp_0, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1_f16_x2(float16_t * ptr, float16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1_f16_x3(float16_t * ptr, float16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1_f16_x4(float16_t * ptr, float16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vfloat16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[3]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
	vse16((ptr + 12), temp_3, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f32(float32_t * ptr, float32x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(val);
	vse32(ptr, temp_0, 2);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f32_x2(float32_t * ptr, float32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f32_x3(float32_t * ptr, float32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f32_x4(float32_t * ptr, float32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vfloat32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[3]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
	vse32((ptr + 6), temp_3, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f64(float64_t * ptr, float64x1_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(val);
	vse64(ptr, temp_0, 1);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f64_x2(float64_t * ptr, float64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f64_x3(float64_t * ptr, float64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_f64_x4(float64_t * ptr, float64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vfloat64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[3]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
	vse64((ptr + 3), temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1_lane_f16(float16_t * ptr, float16x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(val);
	vse16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_f32(float32_t * ptr, float32x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(val);
	vse32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_f64(float64_t * ptr, float64x1_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(val);
	vse64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 1)), temp_0, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_s16(int16_t * ptr, int16x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(val);
	vse16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_s32(int32_t * ptr, int32x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(val);
	vse32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_s64(int64_t * ptr, int64x1_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(val);
	vse64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 1)), temp_0, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_s8(int8_t * ptr, int8x8_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(val);
	vse8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 1)), temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_u16(uint16_t * ptr, uint16x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(val);
	vse16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_u32(uint32_t * ptr, uint32x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(val);
	vse32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_u64(uint64_t * ptr, uint64x1_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(val);
	vse64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 1)), temp_0, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_lane_u8(uint8_t * ptr, uint8x8_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(val);
	vse8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 1)), temp_0, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1q_f16(float16_t * ptr, float16x8_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(val);
	vse16(ptr, temp_0, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1q_f16_x2(float16_t * ptr, float16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1q_f16_x3(float16_t * ptr, float16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1q_f16_x4(float16_t * ptr, float16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vfloat16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[3]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
	vse16((ptr + 24), temp_3, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f32(float32_t * ptr, float32x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(val);
	vse32(ptr, temp_0, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f32_x2(float32_t * ptr, float32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f32_x3(float32_t * ptr, float32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f32_x4(float32_t * ptr, float32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vfloat32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[3]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
	vse32((ptr + 12), temp_3, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f64(float64_t * ptr, float64x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(val);
	vse64(ptr, temp_0, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f64_x2(float64_t * ptr, float64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f64_x3(float64_t * ptr, float64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_f64_x4(float64_t * ptr, float64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vfloat64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[3]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
	vse64((ptr + 6), temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline void vst1q_lane_f16(float16_t * ptr, float16x8_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(val);
	vse16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 1)), temp_0, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_f32(float32_t * ptr, float32x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(val);
	vse32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_f64(float64_t * ptr, float64x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(val);
	vse64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_s16(int16_t * ptr, int16x8_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(val);
	vse16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 1)), temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_s32(int32_t * ptr, int32x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(val);
	vse32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_s64(int64_t * ptr, int64x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(val);
	vse64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_s8(int8_t * ptr, int8x16_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(val);
	vse8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 1)), temp_0, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_u16(uint16_t * ptr, uint16x8_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(val);
	vse16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 1)), temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_u32(uint32_t * ptr, uint32x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(val);
	vse32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 1)), temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_u64(uint64_t * ptr, uint64x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(val);
	vse64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 1)), temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_lane_u8(uint8_t * ptr, uint8x16_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(val);
	vse8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 1)), temp_0, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s16(int16_t * ptr, int16x8_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(val);
	vse16(ptr, temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s16_x2(int16_t * ptr, int16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s16_x3(int16_t * ptr, int16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s16_x4(int16_t * ptr, int16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[3]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
	vse16((ptr + 24), temp_3, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s32(int32_t * ptr, int32x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(val);
	vse32(ptr, temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s32_x2(int32_t * ptr, int32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s32_x3(int32_t * ptr, int32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s32_x4(int32_t * ptr, int32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[3]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
	vse32((ptr + 12), temp_3, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s64(int64_t * ptr, int64x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(val);
	vse64(ptr, temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s64_x2(int64_t * ptr, int64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s64_x3(int64_t * ptr, int64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s64_x4(int64_t * ptr, int64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[3]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
	vse64((ptr + 6), temp_3, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s8(int8_t * ptr, int8x16_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(val);
	vse8(ptr, temp_0, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s8_x2(int8_t * ptr, int8x16x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s8_x3(int8_t * ptr, int8x16x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
	vse8((ptr + 32), temp_2, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_s8_x4(int8_t * ptr, int8x16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[3]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
	vse8((ptr + 32), temp_2, 16);
	vse8((ptr + 48), temp_3, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u16(uint16_t * ptr, uint16x8_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(val);
	vse16(ptr, temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u16_x2(uint16_t * ptr, uint16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u16_x3(uint16_t * ptr, uint16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u16_x4(uint16_t * ptr, uint16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vuint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[3]);
	vse16((ptr + 0), temp_0, 8);
	vse16((ptr + 8), temp_1, 8);
	vse16((ptr + 16), temp_2, 8);
	vse16((ptr + 24), temp_3, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u32(uint32_t * ptr, uint32x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(val);
	vse32(ptr, temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u32_x2(uint32_t * ptr, uint32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u32_x3(uint32_t * ptr, uint32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u32_x4(uint32_t * ptr, uint32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vuint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[3]);
	vse32((ptr + 0), temp_0, 4);
	vse32((ptr + 4), temp_1, 4);
	vse32((ptr + 8), temp_2, 4);
	vse32((ptr + 12), temp_3, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u64(uint64_t * ptr, uint64x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(val);
	vse64(ptr, temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u64_x2(uint64_t * ptr, uint64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u64_x3(uint64_t * ptr, uint64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u64_x4(uint64_t * ptr, uint64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vuint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[3]);
	vse64((ptr + 0), temp_0, 2);
	vse64((ptr + 2), temp_1, 2);
	vse64((ptr + 4), temp_2, 2);
	vse64((ptr + 6), temp_3, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u8(uint8_t * ptr, uint8x16_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(val);
	vse8(ptr, temp_0, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u8_x2(uint8_t * ptr, uint8x16x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u8_x3(uint8_t * ptr, uint8x16x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
	vse8((ptr + 32), temp_2, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1q_u8_x4(uint8_t * ptr, uint8x16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vuint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[3]);
	vse8((ptr + 0), temp_0, 16);
	vse8((ptr + 16), temp_1, 16);
	vse8((ptr + 32), temp_2, 16);
	vse8((ptr + 48), temp_3, 16);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s16(int16_t * ptr, int16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(val);
	vse16(ptr, temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s16_x2(int16_t * ptr, int16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s16_x3(int16_t * ptr, int16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s16_x4(int16_t * ptr, int16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[3]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
	vse16((ptr + 12), temp_3, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s32(int32_t * ptr, int32x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(val);
	vse32(ptr, temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s32_x2(int32_t * ptr, int32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s32_x3(int32_t * ptr, int32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s32_x4(int32_t * ptr, int32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[3]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
	vse32((ptr + 6), temp_3, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s64(int64_t * ptr, int64x1_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(val);
	vse64(ptr, temp_0, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s64_x2(int64_t * ptr, int64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s64_x3(int64_t * ptr, int64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s64_x4(int64_t * ptr, int64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[3]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
	vse64((ptr + 3), temp_3, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s8(int8_t * ptr, int8x8_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(val);
	vse8(ptr, temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s8_x2(int8_t * ptr, int8x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s8_x3(int8_t * ptr, int8x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
	vse8((ptr + 16), temp_2, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_s8_x4(int8_t * ptr, int8x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[3]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
	vse8((ptr + 16), temp_2, 8);
	vse8((ptr + 24), temp_3, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u16(uint16_t * ptr, uint16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(val);
	vse16(ptr, temp_0, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u16_x2(uint16_t * ptr, uint16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u16_x3(uint16_t * ptr, uint16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u16_x4(uint16_t * ptr, uint16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vuint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[3]);
	vse16((ptr + 0), temp_0, 4);
	vse16((ptr + 4), temp_1, 4);
	vse16((ptr + 8), temp_2, 4);
	vse16((ptr + 12), temp_3, 4);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u32(uint32_t * ptr, uint32x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(val);
	vse32(ptr, temp_0, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u32_x2(uint32_t * ptr, uint32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u32_x3(uint32_t * ptr, uint32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u32_x4(uint32_t * ptr, uint32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vuint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[3]);
	vse32((ptr + 0), temp_0, 2);
	vse32((ptr + 2), temp_1, 2);
	vse32((ptr + 4), temp_2, 2);
	vse32((ptr + 6), temp_3, 2);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u64(uint64_t * ptr, uint64x1_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(val);
	vse64(ptr, temp_0, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u64_x2(uint64_t * ptr, uint64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u64_x3(uint64_t * ptr, uint64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u64_x4(uint64_t * ptr, uint64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vuint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[3]);
	vse64((ptr + 0), temp_0, 1);
	vse64((ptr + 1), temp_1, 1);
	vse64((ptr + 2), temp_2, 1);
	vse64((ptr + 3), temp_3, 1);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u8(uint8_t * ptr, uint8x8_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(val);
	vse8(ptr, temp_0, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u8_x2(uint8_t * ptr, uint8x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u8_x3(uint8_t * ptr, uint8x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
	vse8((ptr + 16), temp_2, 8);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline void vst1_u8_x4(uint8_t * ptr, uint8x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vuint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[3]);
	vse8((ptr + 0), temp_0, 8);
	vse8((ptr + 8), temp_1, 8);
	vse8((ptr + 16), temp_2, 8);
	vse8((ptr + 24), temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_f16(float16_t * ptr, float16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_f32(float32_t * ptr, float32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_f64(float64_t * ptr, float64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_f16(float16_t * ptr, float16x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_f32(float32_t * ptr, float32x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_f64(float64_t * ptr, float64x1x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 2)), temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_s16(int16_t * ptr, int16x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_s32(int32_t * ptr, int32x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_s64(int64_t * ptr, int64x1x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 2)), temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_s8(int8_t * ptr, int8x8x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vsseg2e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 2)), temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_u16(uint16_t * ptr, uint16x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_u32(uint32_t * ptr, uint32x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_u64(uint64_t * ptr, uint64x1x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 2)), temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_lane_u8(uint8_t * ptr, uint8x8x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vsseg2e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 2)), temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_f16(float16_t * ptr, float16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_f32(float32_t * ptr, float32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_f64(float64_t * ptr, float64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_f16(float16_t * ptr, float16x8x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 2)), temp_0, temp_1, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_f32(float32_t * ptr, float32x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_f64(float64_t * ptr, float64x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_s16(int16_t * ptr, int16x8x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 2)), temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_s32(int32_t * ptr, int32x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_s64(int64_t * ptr, int64x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_s8(int8_t * ptr, int8x16x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vsseg2e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 2)), temp_0, temp_1, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_u16(uint16_t * ptr, uint16x8x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vsseg2e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 2)), temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_u32(uint32_t * ptr, uint32x4x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vsseg2e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 2)), temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_u64(uint64_t * ptr, uint64x2x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vsseg2e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 2)), temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_lane_u8(uint8_t * ptr, uint8x16x2_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vsseg2e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 2)), temp_0, temp_1, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_s16(int16_t * ptr, int16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_s32(int32_t * ptr, int32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_s64(int64_t * ptr, int64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_s8(int8_t * ptr, int8x16x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vsseg2e8(ptr, temp_0, temp_1, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_u16(uint16_t * ptr, uint16x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_u32(uint32_t * ptr, uint32x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_u64(uint64_t * ptr, uint64x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2q_u8(uint8_t * ptr, uint8x16x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vsseg2e8(ptr, temp_0, temp_1, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_s16(int16_t * ptr, int16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_s32(int32_t * ptr, int32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_s64(int64_t * ptr, int64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_s8(int8_t * ptr, int8x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vsseg2e8(ptr, temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_u16(uint16_t * ptr, uint16x4x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vsseg2e16(ptr, temp_0, temp_1, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_u32(uint32_t * ptr, uint32x2x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vsseg2e32(ptr, temp_0, temp_1, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_u64(uint64_t * ptr, uint64x1x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vsseg2e64(ptr, temp_0, temp_1, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst2_u8(uint8_t * ptr, uint8x8x2_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vsseg2e8(ptr, temp_0, temp_1, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_f16(float16_t * ptr, float16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_f32(float32_t * ptr, float32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_f64(float64_t * ptr, float64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_f16(float16_t * ptr, float16x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_f32(float32_t * ptr, float32x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_f64(float64_t * ptr, float64x1x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_s16(int16_t * ptr, int16x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_s32(int32_t * ptr, int32x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_s64(int64_t * ptr, int64x1x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_s8(int8_t * ptr, int8x8x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vsseg3e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_u16(uint16_t * ptr, uint16x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_u32(uint32_t * ptr, uint32x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_u64(uint64_t * ptr, uint64x1x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_lane_u8(uint8_t * ptr, uint8x8x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vsseg3e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_f16(float16_t * ptr, float16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_f32(float32_t * ptr, float32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_f64(float64_t * ptr, float64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_f16(float16_t * ptr, float16x8x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_f32(float32_t * ptr, float32x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_f64(float64_t * ptr, float64x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_s16(int16_t * ptr, int16x8x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_s32(int32_t * ptr, int32x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_s64(int64_t * ptr, int64x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_s8(int8_t * ptr, int8x16x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vsseg3e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_u16(uint16_t * ptr, uint16x8x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vsseg3e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_u32(uint32_t * ptr, uint32x4x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vsseg3e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_u64(uint64_t * ptr, uint64x2x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vsseg3e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_lane_u8(uint8_t * ptr, uint8x16x3_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vsseg3e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 3)), temp_0, temp_1, temp_2, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_s16(int16_t * ptr, int16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_s32(int32_t * ptr, int32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_s64(int64_t * ptr, int64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_s8(int8_t * ptr, int8x16x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vsseg3e8(ptr, temp_0, temp_1, temp_2, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_u16(uint16_t * ptr, uint16x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_u32(uint32_t * ptr, uint32x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_u64(uint64_t * ptr, uint64x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3q_u8(uint8_t * ptr, uint8x16x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vsseg3e8(ptr, temp_0, temp_1, temp_2, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_s16(int16_t * ptr, int16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_s32(int32_t * ptr, int32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_s64(int64_t * ptr, int64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_s8(int8_t * ptr, int8x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vsseg3e8(ptr, temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_u16(uint16_t * ptr, uint16x4x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vsseg3e16(ptr, temp_0, temp_1, temp_2, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_u32(uint32_t * ptr, uint32x2x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vsseg3e32(ptr, temp_0, temp_1, temp_2, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_u64(uint64_t * ptr, uint64x1x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vsseg3e64(ptr, temp_0, temp_1, temp_2, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst3_u8(uint8_t * ptr, uint8x8x3_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vsseg3e8(ptr, temp_0, temp_1, temp_2, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_f16(float16_t * ptr, float16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vfloat16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_f32(float32_t * ptr, float32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vfloat32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_f64(float64_t * ptr, float64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vfloat64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_f16(float16_t * ptr, float16x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[0]);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[1]);
	vfloat16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[2]);
	vfloat16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m1((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_f32(float32_t * ptr, float32x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[0]);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[1]);
	vfloat32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[2]);
	vfloat32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m1((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_f64(float64_t * ptr, float64x1x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[0]);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[1]);
	vfloat64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[2]);
	vfloat64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m1((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_s16(int16_t * ptr, int16x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_s32(int32_t * ptr, int32x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_s64(int64_t * ptr, int64x1x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_s8(int8_t * ptr, int8x8x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[3]);
	vsseg4e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_u16(uint16_t * ptr, uint16x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vuint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m1(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_u32(uint32_t * ptr, uint32x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vuint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m1(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_u64(uint64_t * ptr, uint64x1x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vuint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m1(1), lane, 1), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_lane_u8(uint8_t * ptr, uint8x8x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vuint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[3]);
	vsseg4e8(vmseq(vid_v_u8m1(8), lane, 8), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_f16(float16_t * ptr, float16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vfloat16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_f32(float32_t * ptr, float32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vfloat32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_f64(float64_t * ptr, float64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vfloat64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_f16(float16_t * ptr, float16x8x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[0]);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[1]);
	vfloat16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[2]);
	vfloat16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f16m2((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_f32(float32_t * ptr, float32x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[0]);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[1]);
	vfloat32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[2]);
	vfloat32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f32m2((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_f64(float64_t * ptr, float64x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[0]);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[1]);
	vfloat64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[2]);
	vfloat64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_f64m2((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_s16(int16_t * ptr, int16x8x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_s32(int32_t * ptr, int32x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_s64(int64_t * ptr, int64x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_s8(int8_t * ptr, int8x16x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[3]);
	vsseg4e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_u16(uint16_t * ptr, uint16x8x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vuint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[3]);
	vsseg4e16(vmseq(vid_v_u16m2(8), lane, 8), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_u32(uint32_t * ptr, uint32x4x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vuint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[3]);
	vsseg4e32(vmseq(vid_v_u32m2(4), lane, 4), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_u64(uint64_t * ptr, uint64x2x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vuint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[3]);
	vsseg4e64(vmseq(vid_v_u64m2(2), lane, 2), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_lane_u8(uint8_t * ptr, uint8x16x4_t val, const int lane)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vuint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[3]);
	vsseg4e8(vmseq(vid_v_u8m2(16), lane, 16), (ptr - (lane * 4)), temp_0, temp_1, temp_2, temp_3, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_s16(int16_t * ptr, int16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[0]);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[1]);
	vint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[2]);
	vint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m2((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_s32(int32_t * ptr, int32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[0]);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[1]);
	vint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[2]);
	vint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m2((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_s64(int64_t * ptr, int64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[0]);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[1]);
	vint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[2]);
	vint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m2((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_s8(int8_t * ptr, int8x16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[0]);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[1]);
	vint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[2]);
	vint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m2((val.val)[3]);
	vsseg4e8(ptr, temp_0, temp_1, temp_2, temp_3, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_u16(uint16_t * ptr, uint16x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[0]);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[1]);
	vuint16m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[2]);
	vuint16m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m2((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_u32(uint32_t * ptr, uint32x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[0]);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[1]);
	vuint32m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[2]);
	vuint32m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m2((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_u64(uint64_t * ptr, uint64x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[0]);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[1]);
	vuint64m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[2]);
	vuint64m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m2((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4q_u8(uint8_t * ptr, uint8x16x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[0]);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[1]);
	vuint8m2_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[2]);
	vuint8m2_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m2((val.val)[3]);
	vsseg4e8(ptr, temp_0, temp_1, temp_2, temp_3, 16);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_s16(int16_t * ptr, int16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[0]);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[1]);
	vint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[2]);
	vint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i16m1((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_s32(int32_t * ptr, int32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[0]);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[1]);
	vint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[2]);
	vint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i32m1((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_s64(int64_t * ptr, int64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[0]);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[1]);
	vint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[2]);
	vint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i64m1((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_s8(int8_t * ptr, int8x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[0]);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[1]);
	vint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[2]);
	vint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_i8m1((val.val)[3]);
	vsseg4e8(ptr, temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_u16(uint16_t * ptr, uint16x4x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[0]);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[1]);
	vuint16m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[2]);
	vuint16m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u16m1((val.val)[3]);
	vsseg4e16(ptr, temp_0, temp_1, temp_2, temp_3, 4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_u32(uint32_t * ptr, uint32x2x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[0]);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[1]);
	vuint32m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[2]);
	vuint32m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u32m1((val.val)[3]);
	vsseg4e32(ptr, temp_0, temp_1, temp_2, temp_3, 2);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_u64(uint64_t * ptr, uint64x1x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[0]);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[1]);
	vuint64m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[2]);
	vuint64m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u64m1((val.val)[3]);
	vsseg4e64(ptr, temp_0, temp_1, temp_2, temp_3, 1);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline void vst4_u8(uint8_t * ptr, uint8x8x4_t val)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[0]);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[1]);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[2]);
	vuint8m1_t temp_3 = __builtin_rvv_vcast_from_fixed_64_u8m1((val.val)[3]);
	vsseg4e8(ptr, temp_0, temp_1, temp_2, temp_3, 8);
}
#endif
__attribute__((always_inline)) inline int64_t vsubd_s64(int64_t a, int64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (((uint64_t)(a)) - ((uint64_t)(b)));
}
__attribute__((always_inline)) inline uint64_t vsubd_u64(uint64_t a, uint64_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a - b);
}
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vsub_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vfsub(temp_0, temp_1, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vsub_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vfsub(temp_0, temp_1, 2));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x1_t vsub_f64(float64x1_t a, float64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m1(a);
	vfloat64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m1(vfsub(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_zfh)
__attribute__((always_inline)) inline float16_t vsubh_f16(float16_t a, float16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	return (a - b);
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_s16") int8x16_t vsubhn_high_s16(int8x8_t r, int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_s32") int16x8_t vsubhn_high_s32(int16x4_t r, int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_s64") int32x4_t vsubhn_high_s64(int32x2_t r, int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_u16") uint8x16_t vsubhn_high_u16(uint8x8_t r, uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_u32") uint16x8_t vsubhn_high_u32(uint16x4_t r, uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_high_u64") uint32x4_t vsubhn_high_u64(uint32x2_t r, uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_s16") int8x8_t vsubhn_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_s32") int16x4_t vsubhn_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_s64") int32x2_t vsubhn_s64(int64x2_t a, int64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_u16") uint8x8_t vsubhn_u16(uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_u32") uint16x4_t vsubhn_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubhn_u64") uint32x2_t vsubhn_u64(uint64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_s16") int32x4_t vsubl_high_s16(int16x8_t a, int16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_s32") int64x2_t vsubl_high_s32(int32x4_t a, int32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_s8") int16x8_t vsubl_high_s8(int8x16_t a, int8x16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_u16") uint32x4_t vsubl_high_u16(uint16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_u32") uint64x2_t vsubl_high_u32(uint32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsubl_high_u8") uint16x8_t vsubl_high_u8(uint8x16_t a, uint8x16_t b);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsubl_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwsub_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsubl_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwsub_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsubl_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwsub_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsubl_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwsubu_vv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsubl_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwsubu_vv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsubl_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwsubu_vv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vsubq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vfsub(temp_0, temp_1, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vsubq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vfsub(temp_0, temp_1, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vsubq_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vfsub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsubq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vsub(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsubq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vsub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsubq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vsub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vsubq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vsub(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsubq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vsub(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsubq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vsub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsubq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vsub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vsubq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vsub(temp_0, temp_1, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vsub_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vsub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vsub_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vsub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x1_t vsub_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m1(vsub(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vsub_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vsub(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vsub_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vsub(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vsub_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vsub(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vsub_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vsub(temp_0, temp_1, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vsub_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vsub(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsubw_high_s16(int32x4_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vint16m1_t temp_2 = vlmul_trunc_v_i16m2_i16m1(vslidedown(vundefined_i16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwsub_wv(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsubw_high_s32(int64x2_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vint32m1_t temp_2 = vlmul_trunc_v_i32m2_i32m1(vslidedown(vundefined_i32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwsub_wv(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsubw_high_s8(int16x8_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vint8m1_t temp_2 = vlmul_trunc_v_i8m2_i8m1(vslidedown(vundefined_i8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwsub_wv(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsubw_high_u16(uint32x4_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m1_t temp_2 = vlmul_trunc_v_u16m2_u16m1(vslidedown(vundefined_u16m2(), temp_1, 4, 4));
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwsubu_wv(temp_0, temp_2, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsubw_high_u32(uint64x2_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m1_t temp_2 = vlmul_trunc_v_u32m2_u32m1(vslidedown(vundefined_u32m2(), temp_1, 2, 2));
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwsubu_wv(temp_0, temp_2, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsubw_high_u8(uint16x8_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m1_t temp_2 = vlmul_trunc_v_u8m2_u8m1(vslidedown(vundefined_u8m2(), temp_1, 8, 8));
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwsubu_wv(temp_0, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vsubw_s16(int32x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vwsub_wv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vsubw_s32(int64x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vwsub_wv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vsubw_s8(int16x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vwsub_wv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vsubw_u16(uint32x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vwsubu_wv(temp_0, temp_1, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vsubw_u32(uint64x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vwsubu_wv(temp_0, temp_1, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vsubw_u8(uint16x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vwsubu_wv(temp_0, temp_1, 8));
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsudot_laneq_s32") int32x2_t vsudot_laneq_s32(int32x2_t r, int8x8_t a, uint8x16_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsudot_lane_s32") int32x2_t vsudot_lane_s32(int32x2_t r, int8x8_t a, uint8x8_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsudotq_laneq_s32") int32x4_t vsudotq_laneq_s32(int32x4_t r, int8x16_t a, uint8x16_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vsudotq_lane_s32") int32x4_t vsudotq_lane_s32(int32x4_t r, int8x16_t a, uint8x8_t b, const int lane);
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbl1_s8(int8x8_t a, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = vle8_v_i8m1(((const int8_t *)((&a))), 8);
	vuint8m1_t temp_1 = vreinterpret_v_i8m1_u8m1(__builtin_rvv_vcast_from_fixed_64_i8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vmerge(vmsgtu(temp_1, 7, 8), vrgather(temp_0, temp_1, 8), 0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbl1_u8(uint8x8_t a, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = vle8_v_u8m1(((const uint8_t *)((&a))), 8);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(idx);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsgtu(temp_1, 7, 8), vrgather(temp_0, temp_1, 8), 0, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbl2_s8(int8x8x2_t a, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vle8_v_i8m2(((const int8_t *)((&a))), 16);
	vuint8m2_t temp_1 = vreinterpret_v_i8m2_u8m2(vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vmerge(vmsgtu(temp_1, 15, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbl2_u8(uint8x8x2_t a, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vle8_v_u8m2(((const uint8_t *)((&a))), 16);
	vuint8m2_t temp_1 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vmerge(vmsgtu(temp_1, 15, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbl3_s8(int8x8x3_t a, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vle8_v_i8m4(((const int8_t *)((&a))), 24);
	vuint8m4_t temp_1 = vreinterpret_v_i8m4_u8m4(vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m4_i8m1(vmerge(vmsgtu(temp_1, 23, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbl3_u8(uint8x8x3_t a, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vle8_v_u8m4(((const uint8_t *)((&a))), 24);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m4_u8m1(vmerge(vmsgtu(temp_1, 23, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbl4_s8(int8x8x4_t a, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vle8_v_i8m4(((const int8_t *)((&a))), 32);
	vuint8m4_t temp_1 = vreinterpret_v_i8m4_u8m4(vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m4_i8m1(vmerge(vmsgtu(temp_1, 31, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbl4_u8(uint8x8x4_t a, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vle8_v_u8m4(((const uint8_t *)((&a))), 32);
	vuint8m4_t temp_1 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m4_u8m1(vmerge(vmsgtu(temp_1, 31, 8), vrgather(temp_0, temp_1, 8), 0, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbx1_s8(int8x8_t a, int8x8_t b, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vreinterpret_v_i8m1_u8m1(__builtin_rvv_vcast_from_fixed_64_i8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vrgather(vmsltu(temp_2, 8, 8), temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbx1_u8(uint8x8_t a, uint8x8_t b, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = __builtin_rvv_vcast_from_fixed_64_u8m1(idx);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vrgather(vmsltu(temp_2, 8, 8), temp_0, temp_1, temp_2, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbx2_s8(int8x8_t a, int8x8x2_t b, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(a));
	vint8m2_t temp_1 = vle8_v_i8m2(((const int8_t *)((&b))), 16);
	vuint8m2_t temp_2 = vreinterpret_v_i8m2_u8m2(vlmul_ext_v_i8m1_i8m2(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m2_i8m1(vrgather(vmsltu(temp_2, 16, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbx2_u8(uint8x8_t a, uint8x8x2_t b, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(a));
	vuint8m2_t temp_1 = vle8_v_u8m2(((const uint8_t *)((&b))), 16);
	vuint8m2_t temp_2 = vlmul_ext_v_u8m1_u8m2(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m2_u8m1(vrgather(vmsltu(temp_2, 16, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbx3_s8(int8x8_t a, int8x8x3_t b, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(a));
	vint8m4_t temp_1 = vle8_v_i8m4(((const int8_t *)((&b))), 24);
	vuint8m4_t temp_2 = vreinterpret_v_i8m4_u8m4(vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m4_i8m1(vrgather(vmsltu(temp_2, 24, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbx3_u8(uint8x8_t a, uint8x8x3_t b, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(a));
	vuint8m4_t temp_1 = vle8_v_u8m4(((const uint8_t *)((&b))), 24);
	vuint8m4_t temp_2 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m4_u8m1(vrgather(vmsltu(temp_2, 24, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtbx4_s8(int8x8_t a, int8x8x4_t b, int8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m4_t temp_0 = vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(a));
	vint8m4_t temp_1 = vle8_v_i8m4(((const int8_t *)((&b))), 32);
	vuint8m4_t temp_2 = vreinterpret_v_i8m4_u8m4(vlmul_ext_v_i8m1_i8m4(__builtin_rvv_vcast_from_fixed_64_i8m1(idx)));
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vlmul_trunc_v_i8m4_i8m1(vrgather(vmsltu(temp_2, 32, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtbx4_u8(uint8x8_t a, uint8x8x4_t b, uint8x8_t idx)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m4_t temp_0 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(a));
	vuint8m4_t temp_1 = vle8_v_u8m4(((const uint8_t *)((&b))), 32);
	vuint8m4_t temp_2 = vlmul_ext_v_u8m1_u8m4(__builtin_rvv_vcast_from_fixed_64_u8m1(idx));
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vlmul_trunc_v_u8m4_u8m1(vrgather(vmsltu(temp_2, 32, 8), temp_0, temp_1, temp_2, 8)));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vtrn1_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vfloat16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vtrn1_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vfloat32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vtrn1q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vfloat16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vtrn1q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vfloat32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(temp_3);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vtrn1q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vfloat64m2_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vtrn1q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vint16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vtrn1q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vint32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vtrn1q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vint64m2_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vtrn1q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vint8m2_t temp_3 = vslideup(vmseq(temp_2, 1, 16), temp_0, temp_1, 1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vtrn1q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vuint16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vtrn1q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vuint32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vtrn1q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vuint64m2_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vtrn1q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vuint8m2_t temp_3 = vslideup(vmseq(temp_2, 1, 16), temp_0, temp_1, 1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vtrn1_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vint16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vtrn1_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vint32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtrn1_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vint8m1_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vtrn1_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vuint16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vtrn1_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vuint32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtrn1_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vuint8m1_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4_t vtrn2_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vfloat16m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2_t vtrn2_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vfloat32m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8_t vtrn2q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vfloat16m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4_t vtrn2q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vfloat32m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(temp_3);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float64x2_t vtrn2q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vfloat64m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8_t vtrn2q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vint16m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4_t vtrn2q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vint32m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int64x2_t vtrn2q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vint64m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16_t vtrn2q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vint8m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 16), temp_1, temp_0, 1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vtrn2q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vuint16m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vtrn2q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vuint32m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vtrn2q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	vuint64m2_t temp_2 = vand(vid_v_u64m2(2), 1, 2);
	vuint64m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vtrn2q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vuint8m2_t temp_3 = vslidedown(vmseq(temp_2, 0, 16), temp_1, temp_0, 1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4_t vtrn2_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vint16m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2_t vtrn2_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vint32m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8_t vtrn2_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vint8m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vtrn2_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vuint16m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vtrn2_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vuint32m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtrn2_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vuint8m1_t temp_3 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x4x2_t vtrn_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vfloat16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vfloat16m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_4)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x2x2_t vtrn_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vfloat32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	vfloat32m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh)
__attribute__((always_inline)) inline float16x8x2_t vtrnq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vfloat16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	vfloat16m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_4)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v)
__attribute__((always_inline)) inline float32x4x2_t vtrnq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vfloat32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vfloat32m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x8x2_t vtrnq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vint16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	vint16m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x4x2_t vtrnq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vint32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vint32m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x16x2_t vtrnq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vint8m2_t temp_3 = vslideup(vmseq(temp_2, 1, 16), temp_0, temp_1, 1, 16);
	vint8m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 16), temp_1, temp_0, 1, 16);
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8x2_t vtrnq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	vuint16m2_t temp_2 = vand(vid_v_u16m2(8), 1, 8);
	vuint16m2_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	vuint16m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4x2_t vtrnq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	vuint32m2_t temp_2 = vand(vid_v_u32m2(4), 1, 4);
	vuint32m2_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vuint32m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16x2_t vtrnq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	vuint8m2_t temp_2 = vand(vid_v_u8m2(16), 1, 16);
	vuint8m2_t temp_3 = vslideup(vmseq(temp_2, 1, 16), temp_0, temp_1, 1, 16);
	vuint8m2_t temp_4 = vslidedown(vmseq(temp_2, 0, 16), temp_1, temp_0, 1, 16);
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int16x4x2_t vtrn_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vint16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vint16m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int32x2x2_t vtrn_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vint32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	vint32m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline int8x8x2_t vtrn_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vint8m1_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	vint8m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4x2_t vtrn_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	vuint16m1_t temp_2 = vand(vid_v_u16m1(4), 1, 4);
	vuint16m1_t temp_3 = vslideup(vmseq(temp_2, 1, 4), temp_0, temp_1, 1, 4);
	vuint16m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 4), temp_1, temp_0, 1, 4);
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2x2_t vtrn_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	vuint32m1_t temp_2 = vand(vid_v_u32m1(2), 1, 2);
	vuint32m1_t temp_3 = vslideup(vmseq(temp_2, 1, 2), temp_0, temp_1, 1, 2);
	vuint32m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 2), temp_1, temp_0, 1, 2);
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4)});
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8x2_t vtrn_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	vuint8m1_t temp_2 = vand(vid_v_u8m1(8), 1, 8);
	vuint8m1_t temp_3 = vslideup(vmseq(temp_2, 1, 8), temp_0, temp_1, 1, 8);
	vuint8m1_t temp_4 = vslidedown(vmseq(temp_2, 0, 8), temp_1, temp_0, 1, 8);
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4)});
}
#endif
NEON2RVV_NOT_IMPLEMENT("vtstd_s64") uint64_t vtstd_s64(int64_t a, int64_t b);
NEON2RVV_NOT_IMPLEMENT("vtstd_u64") uint64_t vtstd_u64(uint64_t a, uint64_t b);
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vtstq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsne(vand(temp_0, temp_1, 8), 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vtstq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsne(vand(temp_0, temp_1, 4), 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vtstq_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsne(vand(temp_0, temp_1, 2), 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vtstq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsne(vand(temp_0, temp_1, 16), 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x8_t vtstq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vmerge(vmsne(vand(temp_0, temp_1, 8), 0, 8), vmv_v_x_u16m2(0, 8), 65535U, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x4_t vtstq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vmerge(vmsne(vand(temp_0, temp_1, 4), 0, 4), vmv_v_x_u32m2(0, 4), 4294967295UL, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x2_t vtstq_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vmerge(vmsne(vand(temp_0, temp_1, 2), 0, 2), vmv_v_x_u64m2(0, 2), 18446744073709551615ULL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x16_t vtstq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vmerge(vmsne(vand(temp_0, temp_1, 16), 0, 16), vmv_v_x_u8m2(0, 16), 255, 16));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vtst_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsne(vand(temp_0, temp_1, 4), 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vtst_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsne(vand(temp_0, temp_1, 2), 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vtst_s64(int64x1_t a, int64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m1(a);
	vint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsne(vand(temp_0, temp_1, 1), 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtst_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsne(vand(temp_0, temp_1, 8), 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint16x4_t vtst_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vmerge(vmsne(vand(temp_0, temp_1, 4), 0, 4), vmv_v_x_u16m1(0, 4), 65535U, 4));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint32x2_t vtst_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vmerge(vmsne(vand(temp_0, temp_1, 2), 0, 2), vmv_v_x_u32m1(0, 2), 4294967295UL, 2));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint64x1_t vtst_u64(uint64x1_t a, uint64x1_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m1(a);
	vuint64m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u64m1(vmerge(vmsne(vand(temp_0, temp_1, 1), 0, 1), vmv_v_x_u64m1(0, 1), 18446744073709551615ULL, 1));
}
#endif
#if defined(__riscv_v)
__attribute__((always_inline)) inline uint8x8_t vtst_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vmerge(vmsne(vand(temp_0, temp_1, 8), 0, 8), vmv_v_x_u8m1(0, 8), 255, 8));
}
#endif
NEON2RVV_NOT_IMPLEMENT("vuqaddb_s8") int8_t vuqaddb_s8(int8_t a, uint8_t b);
NEON2RVV_NOT_IMPLEMENT("vuqaddd_s64") int64_t vuqaddd_s64(int64_t a, uint64_t b);
NEON2RVV_NOT_IMPLEMENT("vuqaddh_s16") int16_t vuqaddh_s16(int16_t a, uint16_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqaddq_s16") int16x8_t vuqaddq_s16(int16x8_t a, uint16x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqaddq_s32") int32x4_t vuqaddq_s32(int32x4_t a, uint32x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqaddq_s64") int64x2_t vuqaddq_s64(int64x2_t a, uint64x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqaddq_s8") int8x16_t vuqaddq_s8(int8x16_t a, uint8x16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqadd_s16") int16x4_t vuqadd_s16(int16x4_t a, uint16x4_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqadd_s32") int32x2_t vuqadd_s32(int32x2_t a, uint32x2_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqadd_s64") int64x1_t vuqadd_s64(int64x1_t a, uint64x1_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vuqadd_s8") int8x8_t vuqadd_s8(int8x8_t a, uint8x8_t b);
#endif
NEON2RVV_NOT_IMPLEMENT("vuqadds_s32") int32_t vuqadds_s32(int32_t a, uint32_t b);
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdot_laneq_s32") int32x2_t vusdot_laneq_s32(int32x2_t r, uint8x8_t a, int8x16_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdot_lane_s32") int32x2_t vusdot_lane_s32(int32x2_t r, uint8x8_t a, int8x8_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdotq_laneq_s32") int32x4_t vusdotq_laneq_s32(int32x4_t r, uint8x16_t a, int8x16_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdotq_lane_s32") int32x4_t vusdotq_lane_s32(int32x4_t r, uint8x16_t a, int8x8_t b, const int lane);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdotq_s32") int32x4_t vusdotq_s32(int32x4_t r, uint8x16_t a, int8x16_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusdot_s32") int32x2_t vusdot_s32(int32x2_t r, uint8x8_t a, int8x8_t b);
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vusmmlaq_s32") int32x4_t vusmmlaq_s32(int32x4_t r, uint8x16_t a, int8x16_t b);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vuzp1_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m1_f16m2(temp_0), vlmul_ext_v_f16m1_f16m2(temp_1), 4, 8), 8);
	vfloat16m1_t temp_3;
	vfloat16m1_t temp_4;
	vlseg2e16_v_f16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vuzp1_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m1_f32m2(temp_0), vlmul_ext_v_f32m1_f32m2(temp_1), 2, 4), 4);
	vfloat32m1_t temp_3;
	vfloat32m1_t temp_4;
	vlseg2e32_v_f32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vuzp1q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m2_f16m4(temp_0), vlmul_ext_v_f16m2_f16m4(temp_1), 8, 16), 16);
	vfloat16m2_t temp_3;
	vfloat16m2_t temp_4;
	vlseg2e16_v_f16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(temp_3);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vuzp1q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m2_f32m4(temp_0), vlmul_ext_v_f32m2_f32m4(temp_1), 4, 8), 8);
	vfloat32m2_t temp_3;
	vfloat32m2_t temp_4;
	vlseg2e32_v_f32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(temp_3);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vuzp1q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	float64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_f64m2_f64m4(temp_0), vlmul_ext_v_f64m2_f64m4(temp_1), 2, 4), 4);
	vfloat64m2_t temp_3;
	vfloat64m2_t temp_4;
	vlseg2e64_v_f64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vuzp1q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m2_i16m4(temp_0), vlmul_ext_v_i16m2_i16m4(temp_1), 8, 16), 16);
	vint16m2_t temp_3;
	vint16m2_t temp_4;
	vlseg2e16_v_i16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vuzp1q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m2_i32m4(temp_0), vlmul_ext_v_i32m2_i32m4(temp_1), 4, 8), 8);
	vint32m2_t temp_3;
	vint32m2_t temp_4;
	vlseg2e32_v_i32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vuzp1q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	int64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_i64m2_i64m4(temp_0), vlmul_ext_v_i64m2_i64m4(temp_1), 2, 4), 4);
	vint64m2_t temp_3;
	vint64m2_t temp_4;
	vlseg2e64_v_i64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vuzp1q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m2_i8m4(temp_0), vlmul_ext_v_i8m2_i8m4(temp_1), 16, 32), 32);
	vint8m2_t temp_3;
	vint8m2_t temp_4;
	vlseg2e8_v_i8m2((&temp_3), (&temp_4), temp_2, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vuzp1q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m2_u16m4(temp_0), vlmul_ext_v_u16m2_u16m4(temp_1), 8, 16), 16);
	vuint16m2_t temp_3;
	vuint16m2_t temp_4;
	vlseg2e16_v_u16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vuzp1q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m2_u32m4(temp_0), vlmul_ext_v_u32m2_u32m4(temp_1), 4, 8), 8);
	vuint32m2_t temp_3;
	vuint32m2_t temp_4;
	vlseg2e32_v_u32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vuzp1q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	uint64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_u64m2_u64m4(temp_0), vlmul_ext_v_u64m2_u64m4(temp_1), 2, 4), 4);
	vuint64m2_t temp_3;
	vuint64m2_t temp_4;
	vlseg2e64_v_u64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vuzp1q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m2_u8m4(temp_0), vlmul_ext_v_u8m2_u8m4(temp_1), 16, 32), 32);
	vuint8m2_t temp_3;
	vuint8m2_t temp_4;
	vlseg2e8_v_u8m2((&temp_3), (&temp_4), temp_2, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vuzp1_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m1_i16m2(temp_0), vlmul_ext_v_i16m1_i16m2(temp_1), 4, 8), 8);
	vint16m1_t temp_3;
	vint16m1_t temp_4;
	vlseg2e16_v_i16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vuzp1_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m1_i32m2(temp_0), vlmul_ext_v_i32m1_i32m2(temp_1), 2, 4), 4);
	vint32m1_t temp_3;
	vint32m1_t temp_4;
	vlseg2e32_v_i32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vuzp1_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m1_i8m2(temp_0), vlmul_ext_v_i8m1_i8m2(temp_1), 8, 16), 16);
	vint8m1_t temp_3;
	vint8m1_t temp_4;
	vlseg2e8_v_i8m1((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vuzp1_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m1_u16m2(temp_0), vlmul_ext_v_u16m1_u16m2(temp_1), 4, 8), 8);
	vuint16m1_t temp_3;
	vuint16m1_t temp_4;
	vlseg2e16_v_u16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vuzp1_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m1_u32m2(temp_0), vlmul_ext_v_u32m1_u32m2(temp_1), 2, 4), 4);
	vuint32m1_t temp_3;
	vuint32m1_t temp_4;
	vlseg2e32_v_u32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vuzp1_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m1_u8m2(temp_0), vlmul_ext_v_u8m1_u8m2(temp_1), 8, 16), 16);
	vuint8m1_t temp_3;
	vuint8m1_t temp_4;
	vlseg2e8_v_u8m1((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_3);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vuzp2_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m1_f16m2(temp_0), vlmul_ext_v_f16m1_f16m2(temp_1), 4, 8), 8);
	vfloat16m1_t temp_3;
	vfloat16m1_t temp_4;
	vlseg2e16_v_f16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(temp_4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vuzp2_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m1_f32m2(temp_0), vlmul_ext_v_f32m1_f32m2(temp_1), 2, 4), 4);
	vfloat32m1_t temp_3;
	vfloat32m1_t temp_4;
	vlseg2e32_v_f32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vuzp2q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m2_f16m4(temp_0), vlmul_ext_v_f16m2_f16m4(temp_1), 8, 16), 16);
	vfloat16m2_t temp_3;
	vfloat16m2_t temp_4;
	vlseg2e16_v_f16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(temp_4);
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vuzp2q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m2_f32m4(temp_0), vlmul_ext_v_f32m2_f32m4(temp_1), 4, 8), 8);
	vfloat32m2_t temp_3;
	vfloat32m2_t temp_4;
	vlseg2e32_v_f32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(temp_4);
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vuzp2q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	float64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_f64m2_f64m4(temp_0), vlmul_ext_v_f64m2_f64m4(temp_1), 2, 4), 4);
	vfloat64m2_t temp_3;
	vfloat64m2_t temp_4;
	vlseg2e64_v_f64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vuzp2q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m2_i16m4(temp_0), vlmul_ext_v_i16m2_i16m4(temp_1), 8, 16), 16);
	vint16m2_t temp_3;
	vint16m2_t temp_4;
	vlseg2e16_v_i16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vuzp2q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m2_i32m4(temp_0), vlmul_ext_v_i32m2_i32m4(temp_1), 4, 8), 8);
	vint32m2_t temp_3;
	vint32m2_t temp_4;
	vlseg2e32_v_i32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vuzp2q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	int64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_i64m2_i64m4(temp_0), vlmul_ext_v_i64m2_i64m4(temp_1), 2, 4), 4);
	vint64m2_t temp_3;
	vint64m2_t temp_4;
	vlseg2e64_v_i64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vuzp2q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m2_i8m4(temp_0), vlmul_ext_v_i8m2_i8m4(temp_1), 16, 32), 32);
	vint8m2_t temp_3;
	vint8m2_t temp_4;
	vlseg2e8_v_i8m2((&temp_3), (&temp_4), temp_2, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vuzp2q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m2_u16m4(temp_0), vlmul_ext_v_u16m2_u16m4(temp_1), 8, 16), 16);
	vuint16m2_t temp_3;
	vuint16m2_t temp_4;
	vlseg2e16_v_u16m2((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vuzp2q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m2_u32m4(temp_0), vlmul_ext_v_u32m2_u32m4(temp_1), 4, 8), 8);
	vuint32m2_t temp_3;
	vuint32m2_t temp_4;
	vlseg2e32_v_u32m2((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vuzp2q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	uint64_t temp_2[4];
	vse64(temp_2, vslideup(vlmul_ext_v_u64m2_u64m4(temp_0), vlmul_ext_v_u64m2_u64m4(temp_1), 2, 4), 4);
	vuint64m2_t temp_3;
	vuint64m2_t temp_4;
	vlseg2e64_v_u64m2((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vuzp2q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m2_u8m4(temp_0), vlmul_ext_v_u8m2_u8m4(temp_1), 16, 32), 32);
	vuint8m2_t temp_3;
	vuint8m2_t temp_4;
	vlseg2e8_v_u8m2((&temp_3), (&temp_4), temp_2, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vuzp2_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m1_i16m2(temp_0), vlmul_ext_v_i16m1_i16m2(temp_1), 4, 8), 8);
	vint16m1_t temp_3;
	vint16m1_t temp_4;
	vlseg2e16_v_i16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vuzp2_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m1_i32m2(temp_0), vlmul_ext_v_i32m1_i32m2(temp_1), 2, 4), 4);
	vint32m1_t temp_3;
	vint32m1_t temp_4;
	vlseg2e32_v_i32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vuzp2_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m1_i8m2(temp_0), vlmul_ext_v_i8m1_i8m2(temp_1), 8, 16), 16);
	vint8m1_t temp_3;
	vint8m1_t temp_4;
	vlseg2e8_v_i8m1((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vuzp2_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m1_u16m2(temp_0), vlmul_ext_v_u16m1_u16m2(temp_1), 4, 8), 8);
	vuint16m1_t temp_3;
	vuint16m1_t temp_4;
	vlseg2e16_v_u16m1((&temp_3), (&temp_4), temp_2, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vuzp2_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m1_u32m2(temp_0), vlmul_ext_v_u32m1_u32m2(temp_1), 2, 4), 4);
	vuint32m1_t temp_3;
	vuint32m1_t temp_4;
	vlseg2e32_v_u32m1((&temp_3), (&temp_4), temp_2, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vuzp2_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m1_u8m2(temp_0), vlmul_ext_v_u8m1_u8m2(temp_1), 8, 16), 16);
	vuint8m1_t temp_3;
	vuint8m1_t temp_4;
	vlseg2e8_v_u8m1((&temp_3), (&temp_4), temp_2, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4);
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4x2_t vuzp_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m1_f16m2(temp_0), vlmul_ext_v_f16m1_f16m2(temp_1), 4, 8), 8);
	vfloat16m1_t temp_3;
	vfloat16m1_t temp_4;
	vlseg2e16_v_f16m1((&temp_3), (&temp_4), temp_2, 4);
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_f16m1(temp_4)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2x2_t vuzp_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m1_f32m2(temp_0), vlmul_ext_v_f32m1_f32m2(temp_1), 2, 4), 4);
	vfloat32m1_t temp_3;
	vfloat32m1_t temp_4;
	vlseg2e32_v_f32m1((&temp_3), (&temp_4), temp_2, 2);
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_f32m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8x2_t vuzpq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_f16m2_f16m4(temp_0), vlmul_ext_v_f16m2_f16m4(temp_1), 8, 16), 16);
	vfloat16m2_t temp_3;
	vfloat16m2_t temp_4;
	vlseg2e16_v_f16m2((&temp_3), (&temp_4), temp_2, 8);
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_f16m2(temp_4)});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4x2_t vuzpq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_f32m2_f32m4(temp_0), vlmul_ext_v_f32m2_f32m4(temp_1), 4, 8), 8);
	vfloat32m2_t temp_3;
	vfloat32m2_t temp_4;
	vlseg2e32_v_f32m2((&temp_3), (&temp_4), temp_2, 4);
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_f32m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8x2_t vuzpq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m2_i16m4(temp_0), vlmul_ext_v_i16m2_i16m4(temp_1), 8, 16), 16);
	vint16m2_t temp_3;
	vint16m2_t temp_4;
	vlseg2e16_v_i16m2((&temp_3), (&temp_4), temp_2, 8);
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i16m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4x2_t vuzpq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m2_i32m4(temp_0), vlmul_ext_v_i32m2_i32m4(temp_1), 4, 8), 8);
	vint32m2_t temp_3;
	vint32m2_t temp_4;
	vlseg2e32_v_i32m2((&temp_3), (&temp_4), temp_2, 4);
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i32m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16x2_t vuzpq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m2_i8m4(temp_0), vlmul_ext_v_i8m2_i8m4(temp_1), 16, 32), 32);
	vint8m2_t temp_3;
	vint8m2_t temp_4;
	vlseg2e8_v_i8m2((&temp_3), (&temp_4), temp_2, 16);
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(temp_3), __builtin_rvv_vcast_to_fixed_64_i8m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8x2_t vuzpq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[16];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m2_u16m4(temp_0), vlmul_ext_v_u16m2_u16m4(temp_1), 8, 16), 16);
	vuint16m2_t temp_3;
	vuint16m2_t temp_4;
	vlseg2e16_v_u16m2((&temp_3), (&temp_4), temp_2, 8);
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u16m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4x2_t vuzpq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[8];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m2_u32m4(temp_0), vlmul_ext_v_u32m2_u32m4(temp_1), 4, 8), 8);
	vuint32m2_t temp_3;
	vuint32m2_t temp_4;
	vlseg2e32_v_u32m2((&temp_3), (&temp_4), temp_2, 4);
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u32m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16x2_t vuzpq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[32];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m2_u8m4(temp_0), vlmul_ext_v_u8m2_u8m4(temp_1), 16, 32), 32);
	vuint8m2_t temp_3;
	vuint8m2_t temp_4;
	vlseg2e8_v_u8m2((&temp_3), (&temp_4), temp_2, 16);
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(temp_3), __builtin_rvv_vcast_to_fixed_64_u8m2(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4x2_t vuzp_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_i16m1_i16m2(temp_0), vlmul_ext_v_i16m1_i16m2(temp_1), 4, 8), 8);
	vint16m1_t temp_3;
	vint16m1_t temp_4;
	vlseg2e16_v_i16m1((&temp_3), (&temp_4), temp_2, 4);
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i16m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2x2_t vuzp_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_i32m1_i32m2(temp_0), vlmul_ext_v_i32m1_i32m2(temp_1), 2, 4), 4);
	vint32m1_t temp_3;
	vint32m1_t temp_4;
	vlseg2e32_v_i32m1((&temp_3), (&temp_4), temp_2, 2);
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i32m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8x2_t vuzp_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_i8m1_i8m2(temp_0), vlmul_ext_v_i8m1_i8m2(temp_1), 8, 16), 16);
	vint8m1_t temp_3;
	vint8m1_t temp_4;
	vlseg2e8_v_i8m1((&temp_3), (&temp_4), temp_2, 8);
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(temp_3), __builtin_rvv_vcast_to_fixed_64_i8m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4x2_t vuzp_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[8];
	vse16(temp_2, vslideup(vlmul_ext_v_u16m1_u16m2(temp_0), vlmul_ext_v_u16m1_u16m2(temp_1), 4, 8), 8);
	vuint16m1_t temp_3;
	vuint16m1_t temp_4;
	vlseg2e16_v_u16m1((&temp_3), (&temp_4), temp_2, 4);
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u16m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2x2_t vuzp_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[4];
	vse32(temp_2, vslideup(vlmul_ext_v_u32m1_u32m2(temp_0), vlmul_ext_v_u32m1_u32m2(temp_1), 2, 4), 4);
	vuint32m1_t temp_3;
	vuint32m1_t temp_4;
	vlseg2e32_v_u32m1((&temp_3), (&temp_4), temp_2, 2);
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u32m1(temp_4)});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8x2_t vuzp_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[16];
	vse8(temp_2, vslideup(vlmul_ext_v_u8m1_u8m2(temp_0), vlmul_ext_v_u8m1_u8m2(temp_1), 8, 16), 16);
	vuint8m1_t temp_3;
	vuint8m1_t temp_4;
	vlseg2e8_v_u8m1((&temp_3), (&temp_4), temp_2, 8);
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(temp_3), __builtin_rvv_vcast_to_fixed_64_u8m1(temp_4)});
}
#endif
#if defined(__riscv_v)
NEON2RVV_NOT_IMPLEMENT("vxarq_u64") uint64x2_t vxarq_u64(uint64x2_t a, uint64x2_t b, const int imm6);
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vzip1_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[4];
	vsseg2e16(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1(temp_2, 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vzip1_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[2];
	vsseg2e32(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vzip1q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2(temp_2, 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vzip1q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2(temp_2, 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vzip1q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	float64_t temp_2[2];
	vsseg2e64(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vzip1q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2(temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vzip1q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2(temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vzip1q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	int64_t temp_2[2];
	vsseg2e64(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vzip1q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2(temp_2, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vzip1q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2(temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vzip1q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2(temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vzip1q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	uint64_t temp_2[2];
	vsseg2e64(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vzip1q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2(temp_2, 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vzip1_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[4];
	vsseg2e16(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1(temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vzip1_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[2];
	vsseg2e32(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vzip1_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[8];
	vsseg2e8(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1(temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vzip1_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[4];
	vsseg2e16(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1(temp_2, 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vzip1_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[2];
	vsseg2e32(temp_2, temp_0, temp_1, 1);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1(temp_2, 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vzip1_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[8];
	vsseg2e8(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1(temp_2, 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4_t vzip2_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((temp_2 + 4), 4));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2_t vzip2_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8_t vzip2q_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((temp_2 + 8), 8));
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4_t vzip2q_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((temp_2 + 4), 4));
}
#endif
#if (64 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float64x2_t vzip2q_f64(float64x2_t a, float64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f64m2(a);
	vfloat64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f64m2(b);
	float64_t temp_2[4];
	vsseg2e64(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_f64m2(vle64_v_f64m2((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8_t vzip2q_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((temp_2 + 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4_t vzip2q_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((temp_2 + 4), 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int64x2_t vzip2q_s64(int64x2_t a, int64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i64m2(a);
	vint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i64m2(b);
	int64_t temp_2[4];
	vsseg2e64(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i64m2(vle64_v_i64m2((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16_t vzip2q_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[32];
	vsseg2e8(temp_2, temp_0, temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((temp_2 + 16), 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8_t vzip2q_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((temp_2 + 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4_t vzip2q_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((temp_2 + 4), 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint64x2_t vzip2q_u64(uint64x2_t a, uint64x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint64m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u64m2(a);
	vuint64m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u64m2(b);
	uint64_t temp_2[4];
	vsseg2e64(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u64m2(vle64_v_u64m2((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16_t vzip2q_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[32];
	vsseg2e8(temp_2, temp_0, temp_1, 16);
	return __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((temp_2 + 16), 16));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4_t vzip2_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((temp_2 + 4), 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2_t vzip2_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8_t vzip2_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((temp_2 + 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4_t vzip2_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((temp_2 + 4), 4));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2_t vzip2_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((temp_2 + 2), 2));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8_t vzip2_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((temp_2 + 8), 8));
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x4x2_t vzip_f16(float16x4_t a, float16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m1(a);
	vfloat16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m1(b);
	float16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return ((float16x4x2_t){__builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_f16m1(vle16_v_f16m1((temp_2 + 4), 4))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x2x2_t vzip_f32(float32x2_t a, float32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m1(a);
	vfloat32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m1(b);
	float32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return ((float32x2x2_t){__builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1(temp_2, 2)), __builtin_rvv_vcast_to_fixed_64_f32m1(vle32_v_f32m1((temp_2 + 2), 2))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zfh) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float16x8x2_t vzipq_f16(float16x8_t a, float16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f16m2(a);
	vfloat16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f16m2(b);
	float16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return ((float16x8x2_t){__builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2(temp_2, 8)), __builtin_rvv_vcast_to_fixed_64_f16m2(vle16_v_f16m2((temp_2 + 8), 8))});
}
#endif
#if (32 <= __riscv_flen) && defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline float32x4x2_t vzipq_f32(float32x4_t a, float32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vfloat32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_f32m2(a);
	vfloat32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_f32m2(b);
	float32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return ((float32x4x2_t){__builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_f32m2(vle32_v_f32m2((temp_2 + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x8x2_t vzipq_s16(int16x8_t a, int16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m2(a);
	vint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m2(b);
	int16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return ((int16x8x2_t){__builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2(temp_2, 8)), __builtin_rvv_vcast_to_fixed_64_i16m2(vle16_v_i16m2((temp_2 + 8), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x4x2_t vzipq_s32(int32x4_t a, int32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m2(a);
	vint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m2(b);
	int32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return ((int32x4x2_t){__builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_i32m2(vle32_v_i32m2((temp_2 + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x16x2_t vzipq_s8(int8x16_t a, int8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m2(a);
	vint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m2(b);
	int8_t temp_2[32];
	vsseg2e8(temp_2, temp_0, temp_1, 16);
	return ((int8x16x2_t){__builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2(temp_2, 16)), __builtin_rvv_vcast_to_fixed_64_i8m2(vle8_v_i8m2((temp_2 + 16), 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x8x2_t vzipq_u16(uint16x8_t a, uint16x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m2(a);
	vuint16m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m2(b);
	uint16_t temp_2[16];
	vsseg2e16(temp_2, temp_0, temp_1, 8);
	return ((uint16x8x2_t){__builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2(temp_2, 8)), __builtin_rvv_vcast_to_fixed_64_u16m2(vle16_v_u16m2((temp_2 + 8), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x4x2_t vzipq_u32(uint32x4_t a, uint32x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m2(a);
	vuint32m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m2(b);
	uint32_t temp_2[8];
	vsseg2e32(temp_2, temp_0, temp_1, 4);
	return ((uint32x4x2_t){__builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_u32m2(vle32_v_u32m2((temp_2 + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x16x2_t vzipq_u8(uint8x16_t a, uint8x16_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m2_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m2(a);
	vuint8m2_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m2(b);
	uint8_t temp_2[32];
	vsseg2e8(temp_2, temp_0, temp_1, 16);
	return ((uint8x16x2_t){__builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2(temp_2, 16)), __builtin_rvv_vcast_to_fixed_64_u8m2(vle8_v_u8m2((temp_2 + 16), 16))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int16x4x2_t vzip_s16(int16x4_t a, int16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i16m1(a);
	vint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i16m1(b);
	int16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return ((int16x4x2_t){__builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_i16m1(vle16_v_i16m1((temp_2 + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int32x2x2_t vzip_s32(int32x2_t a, int32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i32m1(a);
	vint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i32m1(b);
	int32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return ((int32x2x2_t){__builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1(temp_2, 2)), __builtin_rvv_vcast_to_fixed_64_i32m1(vle32_v_i32m1((temp_2 + 2), 2))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline int8x8x2_t vzip_s8(int8x8_t a, int8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_i8m1(a);
	vint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_i8m1(b);
	int8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return ((int8x8x2_t){__builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1(temp_2, 8)), __builtin_rvv_vcast_to_fixed_64_i8m1(vle8_v_i8m1((temp_2 + 8), 8))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint16x4x2_t vzip_u16(uint16x4_t a, uint16x4_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint16m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u16m1(a);
	vuint16m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u16m1(b);
	uint16_t temp_2[8];
	vsseg2e16(temp_2, temp_0, temp_1, 4);
	return ((uint16x4x2_t){__builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1(temp_2, 4)), __builtin_rvv_vcast_to_fixed_64_u16m1(vle16_v_u16m1((temp_2 + 4), 4))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint32x2x2_t vzip_u32(uint32x2_t a, uint32x2_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint32m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u32m1(a);
	vuint32m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u32m1(b);
	uint32_t temp_2[4];
	vsseg2e32(temp_2, temp_0, temp_1, 2);
	return ((uint32x2x2_t){__builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1(temp_2, 2)), __builtin_rvv_vcast_to_fixed_64_u32m1(vle32_v_u32m1((temp_2 + 2), 2))});
}
#endif
#if defined(__riscv_v) && defined(__riscv_zvlsseg)
__attribute__((always_inline)) inline uint8x8x2_t vzip_u8(uint8x8_t a, uint8x8_t b)
{
	NEON2RVV_DEBUG_FUNCTION;
	vuint8m1_t temp_0 = __builtin_rvv_vcast_from_fixed_64_u8m1(a);
	vuint8m1_t temp_1 = __builtin_rvv_vcast_from_fixed_64_u8m1(b);
	uint8_t temp_2[16];
	vsseg2e8(temp_2, temp_0, temp_1, 8);
	return ((uint8x8x2_t){__builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1(temp_2, 8)), __builtin_rvv_vcast_to_fixed_64_u8m1(vle8_v_u8m1((temp_2 + 8), 8))});
}
#endif
#endif
